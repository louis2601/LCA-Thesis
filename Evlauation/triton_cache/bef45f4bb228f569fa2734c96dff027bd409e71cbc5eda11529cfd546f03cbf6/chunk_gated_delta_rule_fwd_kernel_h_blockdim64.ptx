//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_86
.address_size 64

	// .globl	chunk_gated_delta_rule_fwd_kernel_h_blockdim64
.extern .shared .align 16 .b8 global_smem[];

.visible .entry chunk_gated_delta_rule_fwd_kernel_h_blockdim64(
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_0,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_1,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_2,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_3,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_4,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_5,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_6,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_7,
	.param .u32 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<139>;
	.reg .b16 	%rs<129>;
	.reg .b32 	%r<1853>;
	.reg .f32 	%f<1411>;
	.reg .b64 	%rd<241>;
	.loc	1 37 0
$L__func_begin0:
	.loc	1 37 0

	ld.param.u64 	%rd42, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_5];
	ld.param.u64 	%rd87, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_0];
$L__tmp0:
	.loc	1 62 30
	// begin inline asm
	mov.u32 %r107, %ctaid.x;
	// end inline asm
	ld.param.u64 	%rd88, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_1];
	.loc	1 62 48
	// begin inline asm
	mov.u32 %r108, %ctaid.y;
	// end inline asm
	ld.param.u64 	%rd89, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_2];
	.loc	1 63 23
	shr.s32 	%r191, %r108, 31;
	shr.u32 	%r192, %r191, 28;
	add.s32 	%r193, %r108, %r192;
	shr.s32 	%r194, %r193, 4;
	and.b32  	%r195, %r193, -16;
	sub.s32 	%r196, %r108, %r195;
	ld.param.u64 	%rd92, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_6];
	ld.param.u64 	%rd93, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_7];
	.loc	1 65 40
	mul.wide.s32 	%rd94, %r194, 4;
	add.s64 	%rd43, %rd92, %rd94;
	mov.pred 	%p63, -1;
	.loc	1 65 27
	// begin inline asm
	mov.u32 %r109, 0x0;
	@%p63 ld.global.b32 { %r109 }, [ %rd43 + 0 ];
	// end inline asm
	.loc	1 65 86
	add.s64 	%rd44, %rd43, 4;
	.loc	1 65 67
	// begin inline asm
	mov.u32 %r110, 0x0;
	@%p63 ld.global.b32 { %r110 }, [ %rd44 + 0 ];
	// end inline asm
	.loc	1 66 18
	sub.s32 	%r197, %r110, %r109;
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r198, %r197, 63;
$L__tmp2:
	.loc	1 68 38
	mul.wide.s32 	%rd95, %r194, 8;
	add.s64 	%rd46, %rd93, %rd95;
	.loc	1 68 22
	// begin inline asm
	mov.u64 %rd45, 0x0;
	@%p63 ld.global.b64 { %rd45 }, [ %rd46 + 0 ];
	// end inline asm
	.loc	1 85 28
	shl.b32 	%r206, %r109, 11;
	shl.b32 	%r207, %r196, 7;
	add.s32 	%r208, %r206, %r207;
	.loc	1 85 9
	mul.wide.s32 	%rd97, %r208, 2;
	add.s64 	%rd2, %rd88, %rd97;
	.loc	1 86 9
	add.s64 	%rd3, %rd87, %rd97;
	.loc	1 87 9
	add.s64 	%rd4, %rd89, %rd97;
	.loc	1 114 79
	shl.b32 	%r209, %r107, 6;
	.loc	1 114 94
	cvt.s64.s32 	%rd6, %r209;
	.loc	1 115 23
	mov.u32 	%r3, %tid.x;
	and.b32  	%r4, %r3, 31;
	bfe.u32 	%r210, %r3, 3, 4;
	or.b32  	%r211, %r210, 16;
	or.b32  	%r212, %r210, 32;
	or.b32  	%r213, %r210, 48;
	shl.b32 	%r214, %r3, 3;
	and.b32  	%r215, %r214, 56;
	cvt.u64.u32 	%rd7, %r210;
	cvt.u64.u32 	%rd8, %r211;
	cvt.u64.u32 	%rd9, %r212;
	cvt.u64.u32 	%rd10, %r213;
	cvt.u64.u32 	%rd11, %r215;
	or.b64  	%rd16, %rd6, %rd11;
	setp.lt.u64 	%p64, %rd16, 128;
	.loc	1 118 27
	or.b64  	%rd98, %rd7, 64;
	or.b64  	%rd99, %rd8, 64;
	or.b64  	%rd100, %rd9, 64;
	or.b64  	%rd101, %rd10, 64;
	.loc	1 126 83
	cvt.s64.s32 	%rd21, %r197;
	.loc	1 113 21
	setp.gt.s32 	%p65, %r198, 63;
	.loc	1 127 22
	mul.wide.u32 	%rd102, %r210, 2048;
	mul.wide.u32 	%rd103, %r211, 2048;
	mul.wide.u32 	%rd104, %r212, 2048;
	mul.wide.u32 	%rd105, %r213, 2048;
	or.b64  	%rd106, %rd102, %rd11;
	or.b64  	%rd107, %rd103, %rd11;
	or.b64  	%rd108, %rd104, %rd11;
	or.b64  	%rd109, %rd105, %rd11;
	shl.b64 	%rd110, %rd106, 1;
	add.s64 	%rd47, %rd4, %rd110;
	shl.b64 	%rd111, %rd107, 1;
	add.s64 	%rd48, %rd4, %rd111;
	shl.b64 	%rd112, %rd108, 1;
	add.s64 	%rd49, %rd4, %rd112;
	shl.b64 	%rd113, %rd109, 1;
	add.s64 	%rd50, %rd4, %rd113;
	setp.lt.s32 	%p66, %r210, %r197;
	setp.lt.s32 	%p67, %r211, %r197;
	setp.lt.s32 	%p68, %r212, %r197;
	setp.lt.s32 	%p69, %r213, %r197;
	shl.b32 	%r216, %r210, 6;
	xor.b32  	%r217, %r214, %r3;
	and.b32  	%r218, %r217, 56;
	or.b32  	%r5, %r216, %r218;
	shl.b32 	%r219, %r5, 1;
	mov.u32 	%r220, global_smem;
	add.s32 	%r111, %r220, %r219;
	shl.b32 	%r221, %r211, 6;
	or.b32  	%r6, %r221, %r218;
	shl.b32 	%r222, %r6, 1;
	add.s32 	%r113, %r220, %r222;
	shl.b32 	%r223, %r212, 6;
	or.b32  	%r7, %r223, %r218;
	shl.b32 	%r224, %r7, 1;
	add.s32 	%r115, %r220, %r224;
	shl.b32 	%r225, %r213, 6;
	or.b32  	%r8, %r225, %r218;
	shl.b32 	%r226, %r8, 1;
	add.s32 	%r117, %r220, %r226;
	selp.b32 	%r227, 16, 0, %p65;
	selp.b32 	%r120, %r227, 0, %p66;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r111 + 0 ], [ %rd47 + 0 ], 0x10, %r120;
	// end inline asm
	selp.b32 	%r122, %r227, 0, %p67;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r113 + 0 ], [ %rd48 + 0 ], 0x10, %r122;
	// end inline asm
	selp.b32 	%r124, %r227, 0, %p68;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r115 + 0 ], [ %rd49 + 0 ], 0x10, %r124;
	// end inline asm
	selp.b32 	%r126, %r227, 0, %p69;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r117 + 0 ], [ %rd50 + 0 ], 0x10, %r126;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 131 26
	add.s64 	%rd51, %rd47, 128;
	add.s64 	%rd52, %rd48, 128;
	add.s64 	%rd53, %rd49, 128;
	add.s64 	%rd54, %rd50, 128;
	add.s32 	%r228, %r220, 16384;
	add.s32 	%r119, %r228, %r219;
	add.s32 	%r121, %r228, %r222;
	add.s32 	%r123, %r228, %r224;
	add.s32 	%r125, %r228, %r226;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r119 + 0 ], [ %rd51 + 0 ], 0x10, %r120;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r121 + 0 ], [ %rd52 + 0 ], 0x10, %r122;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r123 + 0 ], [ %rd53 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r125 + 0 ], [ %rd54 + 0 ], 0x10, %r126;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 142 22
	add.s64 	%rd240, %rd16, %rd102;
	add.s64 	%rd239, %rd16, %rd103;
	add.s64 	%rd238, %rd16, %rd104;
	add.s64 	%rd237, %rd16, %rd105;
	shl.b64 	%rd114, %rd240, 1;
	add.s64 	%rd55, %rd2, %rd114;
	shl.b64 	%rd115, %rd239, 1;
	add.s64 	%rd56, %rd2, %rd115;
	shl.b64 	%rd116, %rd238, 1;
	add.s64 	%rd57, %rd2, %rd116;
	shl.b64 	%rd117, %rd237, 1;
	add.s64 	%rd58, %rd2, %rd117;
	and.pred  	%p138, %p64, %p66;
	and.pred  	%p137, %p64, %p67;
	and.pred  	%p136, %p64, %p68;
	and.pred  	%p135, %p64, %p69;
	or.b32  	%r9, %r216, %r215;
	shl.b32 	%r229, %r9, 1;
	add.s32 	%r230, %r220, 49152;
	add.s32 	%r127, %r230, %r229;
	or.b32  	%r10, %r221, %r215;
	shl.b32 	%r231, %r10, 1;
	add.s32 	%r129, %r230, %r231;
	or.b32  	%r11, %r223, %r215;
	shl.b32 	%r232, %r11, 1;
	add.s32 	%r131, %r230, %r232;
	or.b32  	%r12, %r225, %r215;
	shl.b32 	%r233, %r12, 1;
	add.s32 	%r133, %r230, %r233;
	selp.b32 	%r234, 16, 0, %p138;
	selp.b32 	%r128, %r234, 0, %p65;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r127 + 0 ], [ %rd55 + 0 ], 0x10, %r128;
	// end inline asm
	selp.b32 	%r235, 16, 0, %p137;
	selp.b32 	%r130, %r235, 0, %p65;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r129 + 0 ], [ %rd56 + 0 ], 0x10, %r130;
	// end inline asm
	selp.b32 	%r236, 16, 0, %p136;
	selp.b32 	%r132, %r236, 0, %p65;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r131 + 0 ], [ %rd57 + 0 ], 0x10, %r132;
	// end inline asm
	selp.b32 	%r237, 16, 0, %p135;
	selp.b32 	%r134, %r237, 0, %p65;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r133 + 0 ], [ %rd58 + 0 ], 0x10, %r134;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 183 22
	add.s64 	%rd59, %rd3, %rd110;
	add.s64 	%rd60, %rd3, %rd111;
	add.s64 	%rd61, %rd3, %rd112;
	add.s64 	%rd62, %rd3, %rd113;
	add.s32 	%r238, %r220, 32768;
	add.s32 	%r135, %r238, %r219;
	add.s32 	%r137, %r238, %r222;
	add.s32 	%r139, %r238, %r224;
	add.s32 	%r141, %r238, %r226;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r135 + 0 ], [ %rd59 + 0 ], 0x10, %r120;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r137 + 0 ], [ %rd60 + 0 ], 0x10, %r122;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r139 + 0 ], [ %rd61 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r141 + 0 ], [ %rd62 + 0 ], 0x10, %r126;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 187 26
	add.s64 	%rd63, %rd59, 128;
	add.s64 	%rd64, %rd60, 128;
	add.s64 	%rd65, %rd61, 128;
	add.s64 	%rd66, %rd62, 128;
	add.s32 	%r239, %r220, 65536;
	add.s32 	%r143, %r239, %r219;
	add.s32 	%r145, %r239, %r222;
	add.s32 	%r147, %r239, %r224;
	add.s32 	%r149, %r239, %r226;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r143 + 0 ], [ %rd63 + 0 ], 0x10, %r120;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r145 + 0 ], [ %rd64 + 0 ], 0x10, %r122;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r147 + 0 ], [ %rd65 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r149 + 0 ], [ %rd66 + 0 ], 0x10, %r126;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 113 21
	setp.gt.s32 	%p70, %r198, 127;
	.loc	1 127 22
	shl.b64 	%rd118, %rd98, 11;
	shl.b64 	%rd119, %rd99, 11;
	shl.b64 	%rd120, %rd100, 11;
	shl.b64 	%rd121, %rd101, 11;
	or.b64  	%rd122, %rd118, %rd11;
	or.b64  	%rd123, %rd119, %rd11;
	or.b64  	%rd124, %rd120, %rd11;
	or.b64  	%rd125, %rd121, %rd11;
	shl.b64 	%rd126, %rd122, 1;
	add.s64 	%rd67, %rd4, %rd126;
	shl.b64 	%rd127, %rd123, 1;
	add.s64 	%rd68, %rd4, %rd127;
	shl.b64 	%rd128, %rd124, 1;
	add.s64 	%rd69, %rd4, %rd128;
	shl.b64 	%rd129, %rd125, 1;
	add.s64 	%rd70, %rd4, %rd129;
	setp.lt.s64 	%p71, %rd98, %rd21;
	setp.lt.s64 	%p72, %rd99, %rd21;
	setp.lt.s64 	%p73, %rd100, %rd21;
	setp.lt.s64 	%p74, %rd101, %rd21;
	bar.sync 	0;
	add.s32 	%r240, %r220, 8192;
	add.s32 	%r151, %r240, %r219;
	add.s32 	%r153, %r240, %r222;
	add.s32 	%r155, %r240, %r224;
	add.s32 	%r157, %r240, %r226;
	selp.b32 	%r241, 16, 0, %p71;
	selp.b32 	%r160, %r241, 0, %p70;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r151 + 0 ], [ %rd67 + 0 ], 0x10, %r160;
	// end inline asm
	selp.b32 	%r242, 16, 0, %p72;
	selp.b32 	%r162, %r242, 0, %p70;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r153 + 0 ], [ %rd68 + 0 ], 0x10, %r162;
	// end inline asm
	selp.b32 	%r243, 16, 0, %p73;
	selp.b32 	%r164, %r243, 0, %p70;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r155 + 0 ], [ %rd69 + 0 ], 0x10, %r164;
	// end inline asm
	selp.b32 	%r244, 16, 0, %p74;
	selp.b32 	%r166, %r244, 0, %p70;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r157 + 0 ], [ %rd70 + 0 ], 0x10, %r166;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 131 26
	add.s64 	%rd71, %rd67, 128;
	add.s64 	%rd72, %rd68, 128;
	add.s64 	%rd73, %rd69, 128;
	add.s64 	%rd74, %rd70, 128;
	add.s32 	%r245, %r220, 24576;
	add.s32 	%r159, %r245, %r219;
	add.s32 	%r161, %r245, %r222;
	add.s32 	%r163, %r245, %r224;
	add.s32 	%r165, %r245, %r226;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r159 + 0 ], [ %rd71 + 0 ], 0x10, %r160;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r161 + 0 ], [ %rd72 + 0 ], 0x10, %r162;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r163 + 0 ], [ %rd73 + 0 ], 0x10, %r164;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r165 + 0 ], [ %rd74 + 0 ], 0x10, %r166;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 142 22
	add.s64 	%rd236, %rd16, %rd118;
	add.s64 	%rd235, %rd16, %rd119;
	add.s64 	%rd234, %rd16, %rd120;
	add.s64 	%rd233, %rd16, %rd121;
	shl.b64 	%rd130, %rd236, 1;
	add.s64 	%rd75, %rd2, %rd130;
	shl.b64 	%rd131, %rd235, 1;
	add.s64 	%rd76, %rd2, %rd131;
	shl.b64 	%rd132, %rd234, 1;
	add.s64 	%rd77, %rd2, %rd132;
	shl.b64 	%rd133, %rd233, 1;
	add.s64 	%rd78, %rd2, %rd133;
	and.pred  	%p134, %p64, %p71;
	and.pred  	%p133, %p64, %p72;
	and.pred  	%p132, %p64, %p73;
	and.pred  	%p131, %p64, %p74;
	add.s32 	%r246, %r220, 57344;
	add.s32 	%r167, %r246, %r229;
	add.s32 	%r169, %r246, %r231;
	add.s32 	%r171, %r246, %r232;
	add.s32 	%r173, %r246, %r233;
	selp.b32 	%r247, 16, 0, %p134;
	selp.b32 	%r168, %r247, 0, %p70;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r167 + 0 ], [ %rd75 + 0 ], 0x10, %r168;
	// end inline asm
	selp.b32 	%r248, 16, 0, %p133;
	selp.b32 	%r170, %r248, 0, %p70;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r169 + 0 ], [ %rd76 + 0 ], 0x10, %r170;
	// end inline asm
	selp.b32 	%r249, 16, 0, %p132;
	selp.b32 	%r172, %r249, 0, %p70;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r171 + 0 ], [ %rd77 + 0 ], 0x10, %r172;
	// end inline asm
	selp.b32 	%r250, 16, 0, %p131;
	selp.b32 	%r174, %r250, 0, %p70;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r173 + 0 ], [ %rd78 + 0 ], 0x10, %r174;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 183 22
	add.s64 	%rd79, %rd3, %rd126;
	add.s64 	%rd80, %rd3, %rd127;
	add.s64 	%rd81, %rd3, %rd128;
	add.s64 	%rd82, %rd3, %rd129;
	add.s32 	%r251, %r220, 40960;
	add.s32 	%r175, %r251, %r219;
	add.s32 	%r177, %r251, %r222;
	add.s32 	%r179, %r251, %r224;
	add.s32 	%r181, %r251, %r226;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r175 + 0 ], [ %rd79 + 0 ], 0x10, %r160;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r177 + 0 ], [ %rd80 + 0 ], 0x10, %r162;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r179 + 0 ], [ %rd81 + 0 ], 0x10, %r164;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r181 + 0 ], [ %rd82 + 0 ], 0x10, %r166;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 187 26
	add.s64 	%rd83, %rd79, 128;
	add.s64 	%rd84, %rd80, 128;
	add.s64 	%rd85, %rd81, 128;
	add.s64 	%rd86, %rd82, 128;
	add.s32 	%r252, %r220, 73728;
	add.s32 	%r183, %r252, %r219;
	add.s32 	%r185, %r252, %r222;
	add.s32 	%r187, %r252, %r224;
	add.s32 	%r189, %r252, %r226;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r183 + 0 ], [ %rd83 + 0 ], 0x10, %r160;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r185 + 0 ], [ %rd84 + 0 ], 0x10, %r162;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r187 + 0 ], [ %rd85 + 0 ], 0x10, %r164;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r189 + 0 ], [ %rd86 + 0 ], 0x10, %r166;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 127 22
	// begin inline asm
	cp.async.wait_group 0x5;
	// end inline asm
	bar.sync 	0;
	bfe.u32 	%r13, %r3, 5, 2;
	bfe.u32 	%r253, %r3, 2, 3;
	shl.b32 	%r254, %r3, 1;
	and.b32  	%r14, %r254, 6;
	shl.b32 	%r255, %r13, 4;
	or.b32  	%r15, %r255, %r253;
	.loc	1 113 21
	@%p65 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	.loc	1 0 21
	ld.param.u64 	%rd90, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_3];
	ld.param.u64 	%rd91, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_4];
	shr.s32 	%r199, %r198, 31;
	shr.u32 	%r200, %r199, 26;
	add.s32 	%r201, %r198, %r200;
	shr.s32 	%r2, %r201, 6;
	cvt.u32.u64 	%r202, %rd45;
	shl.b32 	%r203, %r202, 18;
	shl.b32 	%r204, %r196, 14;
	add.s32 	%r205, %r203, %r204;
	mul.wide.s32 	%rd96, %r205, 2;
	add.s64 	%rd1, %rd91, %rd96;
	add.s64 	%rd5, %rd90, %rd97;
	mul.wide.u32 	%rd12, %r210, 128;
	mul.wide.u32 	%rd13, %r211, 128;
	mul.wide.u32 	%rd14, %r212, 128;
	mul.wide.u32 	%rd15, %r213, 128;
	shl.b64 	%rd17, %rd98, 7;
	shl.b64 	%rd18, %rd99, 7;
	shl.b64 	%rd19, %rd100, 7;
	shl.b64 	%rd20, %rd101, 7;
	add.s32 	%r26, %r2, -2;
	mul.lo.s32 	%r264, %r15, 72;
	or.b32  	%r265, %r264, %r14;
	shl.b32 	%r266, %r265, 1;
	add.s32 	%r267, %r220, 81920;
	add.s32 	%r27, %r267, %r266;
	add.s32 	%r31, %r27, 1152;
	or.b32  	%r1851, %r14, 8;
	or.b32  	%r1850, %r14, 16;
	or.b32  	%r1849, %r14, 24;
	or.b32  	%r1848, %r14, 32;
	or.b32  	%r1847, %r14, 40;
	or.b32  	%r1846, %r14, 48;
	or.b32  	%r1845, %r14, 56;
	and.b32  	%r268, %r3, 7;
	shr.u32 	%r269, %r4, 3;
	shl.b32 	%r270, %r13, 2;
	or.b32  	%r271, %r270, %r269;
	shl.b32 	%r272, %r268, 3;
	mad.lo.s32 	%r273, %r271, 72, %r272;
	shl.b32 	%r274, %r273, 1;
	add.s32 	%r38, %r267, %r274;
	bfe.u32 	%r275, %r4, 3, 1;
	shr.u32 	%r1844, %r4, 4;
	shl.b32 	%r1843, %r13, 1;
	or.b32  	%r276, %r1843, %r275;
	or.b32  	%r277, %r1844, 2;
	or.b32  	%r278, %r1844, 4;
	or.b32  	%r279, %r1844, 6;
	mad.lo.s32 	%r280, %r15, 66, %r14;
	shl.b32 	%r281, %r280, 1;
	add.s32 	%r41, %r267, %r281;
	add.s32 	%r42, %r41, 1056;
	shl.b32 	%r282, %r4, 1;
	mad.lo.s32 	%r283, %r13, 132, %r282;
	shl.b32 	%r284, %r283, 1;
	add.s32 	%r43, %r267, %r284;
	shr.u32 	%r1852, %r3, 4;
	and.b32  	%r285, %r1852, 6;
	or.b32  	%r286, %r285, 1;
	or.b32  	%r287, %r285, 9;
	or.b32  	%r288, %r285, 17;
	or.b32  	%r289, %r285, 25;
	or.b32  	%r290, %r285, 33;
	or.b32  	%r291, %r285, 41;
	or.b32  	%r292, %r285, 49;
	or.b32  	%r293, %r285, 57;
	shl.b32 	%r294, %r285, 6;
	shr.u32 	%r295, %r3, 2;
	xor.b32  	%r297, %r253, %r285;
	shl.b32 	%r298, %r297, 3;
	or.b32  	%r299, %r298, %r294;
	or.b32  	%r300, %r299, %r14;
	shl.b32 	%r301, %r300, 1;
	add.s32 	%r45, %r267, %r301;
	shl.b32 	%r302, %r286, 6;
	xor.b32  	%r303, %r286, %r253;
	shl.b32 	%r304, %r303, 3;
	or.b32  	%r305, %r304, %r302;
	or.b32  	%r306, %r305, %r14;
	shl.b32 	%r307, %r306, 1;
	add.s32 	%r46, %r267, %r307;
	shl.b32 	%r308, %r287, 6;
	xor.b32  	%r309, %r287, %r295;
	shl.b32 	%r310, %r309, 3;
	and.b32  	%r311, %r310, 56;
	or.b32  	%r312, %r311, %r308;
	or.b32  	%r313, %r312, %r14;
	shl.b32 	%r314, %r313, 1;
	add.s32 	%r47, %r267, %r314;
	shl.b32 	%r315, %r288, 6;
	xor.b32  	%r316, %r288, %r295;
	shl.b32 	%r317, %r316, 3;
	and.b32  	%r318, %r317, 56;
	or.b32  	%r319, %r318, %r315;
	or.b32  	%r320, %r319, %r14;
	shl.b32 	%r321, %r320, 1;
	add.s32 	%r48, %r267, %r321;
	shl.b32 	%r322, %r289, 6;
	xor.b32  	%r323, %r289, %r295;
	shl.b32 	%r324, %r323, 3;
	and.b32  	%r325, %r324, 56;
	or.b32  	%r326, %r325, %r322;
	or.b32  	%r327, %r326, %r14;
	shl.b32 	%r328, %r327, 1;
	add.s32 	%r49, %r267, %r328;
	shl.b32 	%r329, %r290, 6;
	xor.b32  	%r330, %r290, %r295;
	shl.b32 	%r331, %r330, 3;
	and.b32  	%r332, %r331, 56;
	or.b32  	%r333, %r332, %r329;
	or.b32  	%r334, %r333, %r14;
	shl.b32 	%r335, %r334, 1;
	add.s32 	%r50, %r267, %r335;
	shl.b32 	%r336, %r291, 6;
	xor.b32  	%r337, %r291, %r295;
	shl.b32 	%r338, %r337, 3;
	and.b32  	%r339, %r338, 56;
	or.b32  	%r340, %r339, %r336;
	or.b32  	%r341, %r340, %r14;
	shl.b32 	%r342, %r341, 1;
	add.s32 	%r51, %r267, %r342;
	shl.b32 	%r343, %r292, 6;
	xor.b32  	%r344, %r292, %r295;
	shl.b32 	%r345, %r344, 3;
	and.b32  	%r346, %r345, 56;
	or.b32  	%r347, %r346, %r343;
	or.b32  	%r348, %r347, %r14;
	shl.b32 	%r349, %r348, 1;
	add.s32 	%r52, %r267, %r349;
	shl.b32 	%r350, %r293, 6;
	xor.b32  	%r351, %r293, %r295;
	shl.b32 	%r352, %r351, 3;
	and.b32  	%r353, %r352, 56;
	or.b32  	%r354, %r353, %r350;
	or.b32  	%r355, %r354, %r14;
	shl.b32 	%r356, %r355, 1;
	add.s32 	%r53, %r267, %r356;
	xor.b32  	%r357, %r1844, %r268;
	shl.b32 	%r358, %r275, 9;
	shl.b32 	%r359, %r268, 6;
	or.b32  	%r360, %r358, %r359;
	shl.b32 	%r361, %r357, 3;
	or.b32  	%r362, %r360, %r361;
	shl.b32 	%r363, %r362, 1;
	add.s32 	%r441, %r267, %r363;
	add.s32 	%r446, %r441, 2048;
	add.s32 	%r451, %r441, 4096;
	add.s32 	%r456, %r441, 6144;
	xor.b32  	%r364, %r277, %r268;
	shl.b32 	%r365, %r364, 3;
	or.b32  	%r366, %r365, %r360;
	shl.b32 	%r367, %r366, 1;
	add.s32 	%r461, %r267, %r367;
	add.s32 	%r466, %r461, 2048;
	add.s32 	%r471, %r461, 4096;
	add.s32 	%r476, %r461, 6144;
	xor.b32  	%r368, %r278, %r268;
	shl.b32 	%r369, %r368, 3;
	or.b32  	%r370, %r369, %r360;
	shl.b32 	%r371, %r370, 1;
	add.s32 	%r481, %r267, %r371;
	add.s32 	%r486, %r481, 2048;
	add.s32 	%r491, %r481, 4096;
	add.s32 	%r496, %r481, 6144;
	xor.b32  	%r372, %r279, %r268;
	shl.b32 	%r373, %r372, 3;
	or.b32  	%r374, %r373, %r360;
	shl.b32 	%r375, %r374, 1;
	add.s32 	%r501, %r267, %r375;
	add.s32 	%r506, %r501, 2048;
	add.s32 	%r511, %r501, 4096;
	add.s32 	%r516, %r501, 6144;
	shr.u32 	%r376, %r3, 1;
	and.b32  	%r377, %r376, 48;
	or.b32  	%r378, %r253, %r377;
	shl.b32 	%r379, %r276, 9;
	or.b32  	%r380, %r379, %r359;
	or.b32  	%r70, %r361, %r380;
	or.b32  	%r71, %r365, %r380;
	or.b32  	%r72, %r369, %r380;
	or.b32  	%r73, %r373, %r380;
	shl.b32 	%r74, %r378, 6;
	or.b32  	%r75, %r74, %r14;
	xor.b32  	%r381, %r276, %r268;
	shl.b32 	%r382, %r1844, 9;
	or.b32  	%r383, %r382, %r359;
	shl.b32 	%r384, %r381, 3;
	or.b32  	%r76, %r384, %r383;
	mov.f32 	%f199, 0f00000000;
	mov.b32 	%r1841, 1;
	mov.b32 	%r1834, 128;
	mov.b32 	%r1833, 0;
	shl.b64 	%rd168, %rd12, 1;
	shl.b64 	%rd171, %rd13, 1;
	shl.b64 	%rd173, %rd14, 1;
	shl.b64 	%rd175, %rd15, 1;
	shl.b64 	%rd177, %rd17, 1;
	shl.b64 	%rd179, %rd18, 1;
	shl.b64 	%rd181, %rd19, 1;
	shl.b64 	%rd183, %rd20, 1;
	shl.b32 	%r1593, %r70, 1;
	shl.b32 	%r1594, %r71, 1;
	shl.b32 	%r1595, %r72, 1;
	shl.b32 	%r1596, %r73, 1;
	shl.b32 	%r1629, %r75, 1;
	shl.b32 	%r1687, %r76, 1;
	mov.u32 	%r1835, %r239;
	mov.u32 	%r1836, %r238;
	mov.u32 	%r1837, %r230;
	mov.u32 	%r1838, %r228;
	mov.u32 	%r1839, %r220;
	mov.u32 	%r1840, %r1833;
	mov.f32 	%f1283, %f199;
	mov.f32 	%f1284, %f199;
	mov.f32 	%f1285, %f199;
	mov.f32 	%f1286, %f199;
	mov.f32 	%f1287, %f199;
	mov.f32 	%f1288, %f199;
	mov.f32 	%f1289, %f199;
	mov.f32 	%f1290, %f199;
	mov.f32 	%f1291, %f199;
	mov.f32 	%f1292, %f199;
	mov.f32 	%f1293, %f199;
	mov.f32 	%f1294, %f199;
	mov.f32 	%f1295, %f199;
	mov.f32 	%f1296, %f199;
	mov.f32 	%f1297, %f199;
	mov.f32 	%f1298, %f199;
	mov.f32 	%f1299, %f199;
	mov.f32 	%f1300, %f199;
	mov.f32 	%f1301, %f199;
	mov.f32 	%f1302, %f199;
	mov.f32 	%f1303, %f199;
	mov.f32 	%f1304, %f199;
	mov.f32 	%f1305, %f199;
	mov.f32 	%f1306, %f199;
	mov.f32 	%f1307, %f199;
	mov.f32 	%f1308, %f199;
	mov.f32 	%f1309, %f199;
	mov.f32 	%f1310, %f199;
	mov.f32 	%f1311, %f199;
	mov.f32 	%f1312, %f199;
	mov.f32 	%f1313, %f199;
	mov.f32 	%f1314, %f199;
	mov.f32 	%f1315, %f199;
	mov.f32 	%f1316, %f199;
	mov.f32 	%f1317, %f199;
	mov.f32 	%f1318, %f199;
	mov.f32 	%f1319, %f199;
	mov.f32 	%f1320, %f199;
	mov.f32 	%f1321, %f199;
	mov.f32 	%f1322, %f199;
	mov.f32 	%f1323, %f199;
	mov.f32 	%f1324, %f199;
	mov.f32 	%f1325, %f199;
	mov.f32 	%f1326, %f199;
	mov.f32 	%f1327, %f199;
	mov.f32 	%f1328, %f199;
	mov.f32 	%f1329, %f199;
	mov.f32 	%f1330, %f199;
	mov.f32 	%f1331, %f199;
	mov.f32 	%f1332, %f199;
	mov.f32 	%f1333, %f199;
	mov.f32 	%f1334, %f199;
	mov.f32 	%f1335, %f199;
	mov.f32 	%f1336, %f199;
	mov.f32 	%f1337, %f199;
	mov.f32 	%f1338, %f199;
	mov.f32 	%f1339, %f199;
	mov.f32 	%f1340, %f199;
	mov.f32 	%f1341, %f199;
	mov.f32 	%f1342, %f199;
	mov.f32 	%f1343, %f199;
	mov.f32 	%f1344, %f199;
	mov.f32 	%f1345, %f199;
	mov.f32 	%f1346, %f199;
	mov.u32 	%r1842, %r1833;
$L__BB0_3:
	mov.u64 	%rd33, %rd236;
	mov.u64 	%rd32, %rd235;
	mov.u64 	%rd31, %rd234;
	mov.u64 	%rd30, %rd233;
	mov.pred 	%p12, %p134;
	mov.pred 	%p11, %p133;
	mov.pred 	%p10, %p132;
	mov.pred 	%p9, %p131;
	.loc	1 113 21
	setp.lt.s32 	%p107, %r1842, %r26;
	.loc	1 114 37
	mul.wide.s32 	%rd166, %r1833, 2;
	add.s64 	%rd167, %rd1, %rd166;
	.loc	1 115 31
	cvt.rn.f16.f32 	%rs1, %f1315;
	cvt.rn.f16.f32 	%rs2, %f1316;
	cvt.rn.f16.f32 	%rs3, %f1317;
	cvt.rn.f16.f32 	%rs4, %f1318;
	cvt.rn.f16.f32 	%rs5, %f1319;
	cvt.rn.f16.f32 	%rs6, %f1320;
	cvt.rn.f16.f32 	%rs7, %f1321;
	cvt.rn.f16.f32 	%rs8, %f1322;
	cvt.rn.f16.f32 	%rs9, %f1323;
	cvt.rn.f16.f32 	%rs10, %f1324;
	cvt.rn.f16.f32 	%rs11, %f1325;
	cvt.rn.f16.f32 	%rs12, %f1326;
	cvt.rn.f16.f32 	%rs13, %f1327;
	cvt.rn.f16.f32 	%rs14, %f1328;
	cvt.rn.f16.f32 	%rs15, %f1329;
	cvt.rn.f16.f32 	%rs16, %f1330;
	cvt.rn.f16.f32 	%rs17, %f1331;
	cvt.rn.f16.f32 	%rs18, %f1332;
	cvt.rn.f16.f32 	%rs19, %f1333;
	cvt.rn.f16.f32 	%rs20, %f1334;
	cvt.rn.f16.f32 	%rs21, %f1335;
	cvt.rn.f16.f32 	%rs22, %f1336;
	cvt.rn.f16.f32 	%rs23, %f1337;
	cvt.rn.f16.f32 	%rs24, %f1338;
	cvt.rn.f16.f32 	%rs25, %f1339;
	cvt.rn.f16.f32 	%rs26, %f1340;
	cvt.rn.f16.f32 	%rs27, %f1341;
	cvt.rn.f16.f32 	%rs28, %f1342;
	cvt.rn.f16.f32 	%rs29, %f1343;
	cvt.rn.f16.f32 	%rs30, %f1344;
	cvt.rn.f16.f32 	%rs31, %f1345;
	cvt.rn.f16.f32 	%rs32, %f1346;
	.loc	1 115 23
	add.s64 	%rd169, %rd167, %rd168;
	shl.b64 	%rd170, %rd16, 1;
	add.s64 	%rd134, %rd169, %rd170;
	add.s64 	%rd172, %rd167, %rd171;
	add.s64 	%rd135, %rd172, %rd170;
	add.s64 	%rd174, %rd167, %rd173;
	add.s64 	%rd136, %rd174, %rd170;
	add.s64 	%rd176, %rd167, %rd175;
	add.s64 	%rd137, %rd176, %rd170;
	mov.b32 	%r1561, {%rs1, %rs2};
	st.shared.b32 	[%r27], %r1561;
	mov.b32 	%r1562, {%rs3, %rs4};
	st.shared.b32 	[%r31], %r1562;
	mov.b32 	%r1563, {%rs5, %rs6};
	st.shared.b32 	[%r27+16], %r1563;
	mov.b32 	%r1564, {%rs7, %rs8};
	st.shared.b32 	[%r31+16], %r1564;
	mov.b32 	%r1565, {%rs9, %rs10};
	st.shared.b32 	[%r27+32], %r1565;
	mov.b32 	%r1566, {%rs11, %rs12};
	st.shared.b32 	[%r31+32], %r1566;
	mov.b32 	%r1567, {%rs13, %rs14};
	st.shared.b32 	[%r27+48], %r1567;
	mov.b32 	%r1568, {%rs15, %rs16};
	st.shared.b32 	[%r31+48], %r1568;
	mov.b32 	%r1569, {%rs17, %rs18};
	st.shared.b32 	[%r27+64], %r1569;
	mov.b32 	%r1570, {%rs19, %rs20};
	st.shared.b32 	[%r31+64], %r1570;
	mov.b32 	%r1571, {%rs21, %rs22};
	st.shared.b32 	[%r27+80], %r1571;
	mov.b32 	%r1572, {%rs23, %rs24};
	st.shared.b32 	[%r31+80], %r1572;
	mov.b32 	%r1573, {%rs25, %rs26};
	st.shared.b32 	[%r27+96], %r1573;
	mov.b32 	%r1574, {%rs27, %rs28};
	st.shared.b32 	[%r31+96], %r1574;
	mov.b32 	%r1575, {%rs29, %rs30};
	st.shared.b32 	[%r27+112], %r1575;
	mov.b32 	%r1576, {%rs31, %rs32};
	st.shared.b32 	[%r31+112], %r1576;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r389, %r390, %r391, %r392}, [%r38+2304];
	ld.shared.v4.u32 	{%r393, %r394, %r395, %r396}, [%r38+4608];
	ld.shared.v4.u32 	{%r397, %r398, %r399, %r400}, [%r38+6912];
	ld.shared.v4.u32 	{%r385, %r386, %r387, %r388}, [%r38];
	// begin inline asm
	@%p64 st.global.v4.b32 [ %rd134 + 0 ], { %r385, %r386, %r387, %r388 };
	// end inline asm
	// begin inline asm
	@%p64 st.global.v4.b32 [ %rd135 + 0 ], { %r389, %r390, %r391, %r392 };
	// end inline asm
	// begin inline asm
	@%p64 st.global.v4.b32 [ %rd136 + 0 ], { %r393, %r394, %r395, %r396 };
	// end inline asm
	// begin inline asm
	@%p64 st.global.v4.b32 [ %rd137 + 0 ], { %r397, %r398, %r399, %r400 };
	// end inline asm
	.loc	1 118 35
	cvt.rn.f16.f32 	%rs33, %f1283;
	cvt.rn.f16.f32 	%rs34, %f1284;
	cvt.rn.f16.f32 	%rs35, %f1285;
	cvt.rn.f16.f32 	%rs36, %f1286;
	cvt.rn.f16.f32 	%rs37, %f1287;
	cvt.rn.f16.f32 	%rs38, %f1288;
	cvt.rn.f16.f32 	%rs39, %f1289;
	cvt.rn.f16.f32 	%rs40, %f1290;
	cvt.rn.f16.f32 	%rs41, %f1291;
	cvt.rn.f16.f32 	%rs42, %f1292;
	cvt.rn.f16.f32 	%rs43, %f1293;
	cvt.rn.f16.f32 	%rs44, %f1294;
	cvt.rn.f16.f32 	%rs45, %f1295;
	cvt.rn.f16.f32 	%rs46, %f1296;
	cvt.rn.f16.f32 	%rs47, %f1297;
	cvt.rn.f16.f32 	%rs48, %f1298;
	cvt.rn.f16.f32 	%rs49, %f1299;
	cvt.rn.f16.f32 	%rs50, %f1300;
	cvt.rn.f16.f32 	%rs51, %f1301;
	cvt.rn.f16.f32 	%rs52, %f1302;
	cvt.rn.f16.f32 	%rs53, %f1303;
	cvt.rn.f16.f32 	%rs54, %f1304;
	cvt.rn.f16.f32 	%rs55, %f1305;
	cvt.rn.f16.f32 	%rs56, %f1306;
	cvt.rn.f16.f32 	%rs57, %f1307;
	cvt.rn.f16.f32 	%rs58, %f1308;
	cvt.rn.f16.f32 	%rs59, %f1309;
	cvt.rn.f16.f32 	%rs60, %f1310;
	cvt.rn.f16.f32 	%rs61, %f1311;
	cvt.rn.f16.f32 	%rs62, %f1312;
	cvt.rn.f16.f32 	%rs63, %f1313;
	cvt.rn.f16.f32 	%rs64, %f1314;
	.loc	1 118 27
	add.s64 	%rd178, %rd167, %rd177;
	add.s64 	%rd138, %rd178, %rd170;
	add.s64 	%rd180, %rd167, %rd179;
	add.s64 	%rd139, %rd180, %rd170;
	add.s64 	%rd182, %rd167, %rd181;
	add.s64 	%rd140, %rd182, %rd170;
	add.s64 	%rd184, %rd167, %rd183;
	add.s64 	%rd141, %rd184, %rd170;
	bar.sync 	0;
	mov.b32 	%r1577, {%rs33, %rs34};
	st.shared.b32 	[%r27], %r1577;
	mov.b32 	%r1578, {%rs35, %rs36};
	st.shared.b32 	[%r31], %r1578;
	mov.b32 	%r1579, {%rs37, %rs38};
	st.shared.b32 	[%r27+16], %r1579;
	mov.b32 	%r1580, {%rs39, %rs40};
	st.shared.b32 	[%r31+16], %r1580;
	mov.b32 	%r1581, {%rs41, %rs42};
	st.shared.b32 	[%r27+32], %r1581;
	mov.b32 	%r1582, {%rs43, %rs44};
	st.shared.b32 	[%r31+32], %r1582;
	mov.b32 	%r1583, {%rs45, %rs46};
	st.shared.b32 	[%r27+48], %r1583;
	mov.b32 	%r1584, {%rs47, %rs48};
	st.shared.b32 	[%r31+48], %r1584;
	mov.b32 	%r1585, {%rs49, %rs50};
	st.shared.b32 	[%r27+64], %r1585;
	mov.b32 	%r1586, {%rs51, %rs52};
	st.shared.b32 	[%r31+64], %r1586;
	mov.b32 	%r1587, {%rs53, %rs54};
	st.shared.b32 	[%r27+80], %r1587;
	mov.b32 	%r1588, {%rs55, %rs56};
	st.shared.b32 	[%r31+80], %r1588;
	mov.b32 	%r1589, {%rs57, %rs58};
	st.shared.b32 	[%r27+96], %r1589;
	mov.b32 	%r1590, {%rs59, %rs60};
	st.shared.b32 	[%r31+96], %r1590;
	mov.b32 	%r1591, {%rs61, %rs62};
	st.shared.b32 	[%r27+112], %r1591;
	mov.b32 	%r1592, {%rs63, %rs64};
	st.shared.b32 	[%r31+112], %r1592;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r405, %r406, %r407, %r408}, [%r38+2304];
	ld.shared.v4.u32 	{%r409, %r410, %r411, %r412}, [%r38+4608];
	ld.shared.v4.u32 	{%r413, %r414, %r415, %r416}, [%r38+6912];
	ld.shared.v4.u32 	{%r401, %r402, %r403, %r404}, [%r38];
	// begin inline asm
	@%p64 st.global.v4.b32 [ %rd138 + 0 ], { %r401, %r402, %r403, %r404 };
	// end inline asm
	// begin inline asm
	@%p64 st.global.v4.b32 [ %rd139 + 0 ], { %r405, %r406, %r407, %r408 };
	// end inline asm
	// begin inline asm
	@%p64 st.global.v4.b32 [ %rd140 + 0 ], { %r409, %r410, %r411, %r412 };
	// end inline asm
	// begin inline asm
	@%p64 st.global.v4.b32 [ %rd141 + 0 ], { %r413, %r414, %r415, %r416 };
	// end inline asm
	.loc	1 127 22
	add.s32 	%r421, %r1839, %r1593;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r517, %r518, %r519, %r520 }, [ %r421 + 0 ];
	// end inline asm
	add.s32 	%r426, %r1839, %r1594;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r565, %r566, %r567, %r568 }, [ %r426 + 0 ];
	// end inline asm
	add.s32 	%r431, %r1839, %r1595;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r613, %r614, %r615, %r616 }, [ %r431 + 0 ];
	// end inline asm
	add.s32 	%r436, %r1839, %r1596;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r661, %r662, %r663, %r664 }, [ %r436 + 0 ];
	// end inline asm
	.loc	1 115 31
	bar.sync 	0;
	st.shared.b32 	[%r41], %r1561;
	st.shared.b32 	[%r42], %r1562;
	st.shared.b32 	[%r41+16], %r1563;
	st.shared.b32 	[%r42+16], %r1564;
	st.shared.b32 	[%r41+32], %r1565;
	st.shared.b32 	[%r42+32], %r1566;
	st.shared.b32 	[%r41+48], %r1567;
	st.shared.b32 	[%r42+48], %r1568;
	st.shared.b32 	[%r41+64], %r1569;
	st.shared.b32 	[%r42+64], %r1570;
	st.shared.b32 	[%r41+80], %r1571;
	st.shared.b32 	[%r42+80], %r1572;
	st.shared.b32 	[%r41+96], %r1573;
	st.shared.b32 	[%r42+96], %r1574;
	st.shared.b32 	[%r41+112], %r1575;
	st.shared.b32 	[%r42+112], %r1576;
	bar.sync 	0;
	ld.shared.b32 	%r1597, [%r43];
	ld.shared.b32 	%r1598, [%r43+132];
	ld.shared.b32 	%r1599, [%r43+1056];
	ld.shared.b32 	%r1600, [%r43+1188];
	ld.shared.b32 	%r1601, [%r43+2112];
	ld.shared.b32 	%r1602, [%r43+2244];
	ld.shared.b32 	%r1603, [%r43+3168];
	ld.shared.b32 	%r1604, [%r43+3300];
	ld.shared.b32 	%r1605, [%r43+4224];
	ld.shared.b32 	%r1606, [%r43+4356];
	ld.shared.b32 	%r1607, [%r43+5280];
	ld.shared.b32 	%r1608, [%r43+5412];
	ld.shared.b32 	%r1609, [%r43+6336];
	ld.shared.b32 	%r1610, [%r43+6468];
	ld.shared.b32 	%r1611, [%r43+7392];
	ld.shared.b32 	%r1612, [%r43+7524];
	bar.sync 	0;
	st.shared.b32 	[%r45], %r1597;
	st.shared.b32 	[%r46], %r1598;
	st.shared.b32 	[%r45+1024], %r1599;
	st.shared.b32 	[%r47], %r1600;
	st.shared.b32 	[%r45+2048], %r1601;
	st.shared.b32 	[%r48], %r1602;
	st.shared.b32 	[%r45+3072], %r1603;
	st.shared.b32 	[%r49], %r1604;
	st.shared.b32 	[%r45+4096], %r1605;
	st.shared.b32 	[%r50], %r1606;
	st.shared.b32 	[%r45+5120], %r1607;
	st.shared.b32 	[%r51], %r1608;
	st.shared.b32 	[%r45+6144], %r1609;
	st.shared.b32 	[%r52], %r1610;
	st.shared.b32 	[%r45+7168], %r1611;
	st.shared.b32 	[%r53], %r1612;
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r437, %r438, %r439, %r440 }, [ %r441 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r442, %r443, %r444, %r445 }, [ %r446 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r447, %r448, %r449, %r450 }, [ %r451 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r452, %r453, %r454, %r455 }, [ %r456 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r457, %r458, %r459, %r460 }, [ %r461 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r462, %r463, %r464, %r465 }, [ %r466 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r467, %r468, %r469, %r470 }, [ %r471 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r472, %r473, %r474, %r475 }, [ %r476 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r477, %r478, %r479, %r480 }, [ %r481 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r482, %r483, %r484, %r485 }, [ %r486 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r487, %r488, %r489, %r490 }, [ %r491 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r492, %r493, %r494, %r495 }, [ %r496 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r497, %r498, %r499, %r500 }, [ %r501 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r502, %r503, %r504, %r505 }, [ %r506 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r507, %r508, %r509, %r510 }, [ %r511 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r512, %r513, %r514, %r515 }, [ %r516 + 0 ];
	// end inline asm
	.loc	1 128 26
	mov.f32 	%f259, %f199;
	mov.f32 	%f260, %f199;
	mov.f32 	%f261, %f199;
	mov.f32 	%f262, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r517, %r518, %r519, %r520 }, { %r437, %r438 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	mov.f32 	%f267, %f199;
	mov.f32 	%f268, %f199;
	mov.f32 	%f269, %f199;
	mov.f32 	%f270, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r517, %r518, %r519, %r520 }, { %r439, %r440 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	mov.f32 	%f275, %f199;
	mov.f32 	%f276, %f199;
	mov.f32 	%f277, %f199;
	mov.f32 	%f278, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r517, %r518, %r519, %r520 }, { %r457, %r458 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	mov.f32 	%f283, %f199;
	mov.f32 	%f284, %f199;
	mov.f32 	%f285, %f199;
	mov.f32 	%f286, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r517, %r518, %r519, %r520 }, { %r459, %r460 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	mov.f32 	%f291, %f199;
	mov.f32 	%f292, %f199;
	mov.f32 	%f293, %f199;
	mov.f32 	%f294, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r517, %r518, %r519, %r520 }, { %r477, %r478 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	mov.f32 	%f299, %f199;
	mov.f32 	%f300, %f199;
	mov.f32 	%f301, %f199;
	mov.f32 	%f302, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r517, %r518, %r519, %r520 }, { %r479, %r480 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	mov.f32 	%f307, %f199;
	mov.f32 	%f308, %f199;
	mov.f32 	%f309, %f199;
	mov.f32 	%f310, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r517, %r518, %r519, %r520 }, { %r497, %r498 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	mov.f32 	%f315, %f199;
	mov.f32 	%f316, %f199;
	mov.f32 	%f317, %f199;
	mov.f32 	%f318, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r517, %r518, %r519, %r520 }, { %r499, %r500 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r565, %r566, %r567, %r568 }, { %r442, %r443 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r565, %r566, %r567, %r568 }, { %r444, %r445 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r565, %r566, %r567, %r568 }, { %r462, %r463 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r565, %r566, %r567, %r568 }, { %r464, %r465 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r565, %r566, %r567, %r568 }, { %r482, %r483 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r565, %r566, %r567, %r568 }, { %r484, %r485 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r565, %r566, %r567, %r568 }, { %r502, %r503 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r565, %r566, %r567, %r568 }, { %r504, %r505 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r613, %r614, %r615, %r616 }, { %r447, %r448 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r613, %r614, %r615, %r616 }, { %r449, %r450 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r613, %r614, %r615, %r616 }, { %r467, %r468 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r613, %r614, %r615, %r616 }, { %r469, %r470 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r613, %r614, %r615, %r616 }, { %r487, %r488 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r613, %r614, %r615, %r616 }, { %r489, %r490 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r613, %r614, %r615, %r616 }, { %r507, %r508 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r613, %r614, %r615, %r616 }, { %r509, %r510 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r661, %r662, %r663, %r664 }, { %r452, %r453 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r661, %r662, %r663, %r664 }, { %r454, %r455 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r661, %r662, %r663, %r664 }, { %r472, %r473 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r661, %r662, %r663, %r664 }, { %r474, %r475 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r661, %r662, %r663, %r664 }, { %r492, %r493 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r661, %r662, %r663, %r664 }, { %r494, %r495 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r661, %r662, %r663, %r664 }, { %r512, %r513 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r661, %r662, %r663, %r664 }, { %r514, %r515 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	.loc	1 131 26
	add.s32 	%r713, %r1838, %r1593;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r809, %r810, %r811, %r812 }, [ %r713 + 0 ];
	// end inline asm
	add.s32 	%r718, %r1838, %r1594;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r857, %r858, %r859, %r860 }, [ %r718 + 0 ];
	// end inline asm
	add.s32 	%r723, %r1838, %r1595;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r905, %r906, %r907, %r908 }, [ %r723 + 0 ];
	// end inline asm
	add.s32 	%r728, %r1838, %r1596;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r953, %r954, %r955, %r956 }, [ %r728 + 0 ];
	// end inline asm
	.loc	1 118 35
	bar.sync 	0;
	st.shared.b32 	[%r41], %r1577;
	st.shared.b32 	[%r42], %r1578;
	st.shared.b32 	[%r41+16], %r1579;
	st.shared.b32 	[%r42+16], %r1580;
	st.shared.b32 	[%r41+32], %r1581;
	st.shared.b32 	[%r42+32], %r1582;
	st.shared.b32 	[%r41+48], %r1583;
	st.shared.b32 	[%r42+48], %r1584;
	st.shared.b32 	[%r41+64], %r1585;
	st.shared.b32 	[%r42+64], %r1586;
	st.shared.b32 	[%r41+80], %r1587;
	st.shared.b32 	[%r42+80], %r1588;
	st.shared.b32 	[%r41+96], %r1589;
	st.shared.b32 	[%r42+96], %r1590;
	st.shared.b32 	[%r41+112], %r1591;
	st.shared.b32 	[%r42+112], %r1592;
	bar.sync 	0;
	ld.shared.b32 	%r1613, [%r43];
	ld.shared.b32 	%r1614, [%r43+132];
	ld.shared.b32 	%r1615, [%r43+1056];
	ld.shared.b32 	%r1616, [%r43+1188];
	ld.shared.b32 	%r1617, [%r43+2112];
	ld.shared.b32 	%r1618, [%r43+2244];
	ld.shared.b32 	%r1619, [%r43+3168];
	ld.shared.b32 	%r1620, [%r43+3300];
	ld.shared.b32 	%r1621, [%r43+4224];
	ld.shared.b32 	%r1622, [%r43+4356];
	ld.shared.b32 	%r1623, [%r43+5280];
	ld.shared.b32 	%r1624, [%r43+5412];
	ld.shared.b32 	%r1625, [%r43+6336];
	ld.shared.b32 	%r1626, [%r43+6468];
	ld.shared.b32 	%r1627, [%r43+7392];
	ld.shared.b32 	%r1628, [%r43+7524];
	bar.sync 	0;
	st.shared.b32 	[%r45], %r1613;
	st.shared.b32 	[%r46], %r1614;
	st.shared.b32 	[%r45+1024], %r1615;
	st.shared.b32 	[%r47], %r1616;
	st.shared.b32 	[%r45+2048], %r1617;
	st.shared.b32 	[%r48], %r1618;
	st.shared.b32 	[%r45+3072], %r1619;
	st.shared.b32 	[%r49], %r1620;
	st.shared.b32 	[%r45+4096], %r1621;
	st.shared.b32 	[%r50], %r1622;
	st.shared.b32 	[%r45+5120], %r1623;
	st.shared.b32 	[%r51], %r1624;
	st.shared.b32 	[%r45+6144], %r1625;
	st.shared.b32 	[%r52], %r1626;
	st.shared.b32 	[%r45+7168], %r1627;
	st.shared.b32 	[%r53], %r1628;
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r729, %r730, %r731, %r732 }, [ %r441 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r734, %r735, %r736, %r737 }, [ %r446 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r739, %r740, %r741, %r742 }, [ %r451 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r744, %r745, %r746, %r747 }, [ %r456 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r749, %r750, %r751, %r752 }, [ %r461 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r754, %r755, %r756, %r757 }, [ %r466 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r759, %r760, %r761, %r762 }, [ %r471 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r764, %r765, %r766, %r767 }, [ %r476 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r769, %r770, %r771, %r772 }, [ %r481 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r774, %r775, %r776, %r777 }, [ %r486 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r779, %r780, %r781, %r782 }, [ %r491 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r784, %r785, %r786, %r787 }, [ %r496 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r789, %r790, %r791, %r792 }, [ %r501 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r794, %r795, %r796, %r797 }, [ %r506 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r799, %r800, %r801, %r802 }, [ %r511 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r804, %r805, %r806, %r807 }, [ %r516 + 0 ];
	// end inline asm
	.loc	1 132 31
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r809, %r810, %r811, %r812 }, { %r729, %r730 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r809, %r810, %r811, %r812 }, { %r731, %r732 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r809, %r810, %r811, %r812 }, { %r749, %r750 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r809, %r810, %r811, %r812 }, { %r751, %r752 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r809, %r810, %r811, %r812 }, { %r769, %r770 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r809, %r810, %r811, %r812 }, { %r771, %r772 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r809, %r810, %r811, %r812 }, { %r789, %r790 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r809, %r810, %r811, %r812 }, { %r791, %r792 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r857, %r858, %r859, %r860 }, { %r734, %r735 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r857, %r858, %r859, %r860 }, { %r736, %r737 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r857, %r858, %r859, %r860 }, { %r754, %r755 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r857, %r858, %r859, %r860 }, { %r756, %r757 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r857, %r858, %r859, %r860 }, { %r774, %r775 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r857, %r858, %r859, %r860 }, { %r776, %r777 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r857, %r858, %r859, %r860 }, { %r794, %r795 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r857, %r858, %r859, %r860 }, { %r796, %r797 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r905, %r906, %r907, %r908 }, { %r739, %r740 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r905, %r906, %r907, %r908 }, { %r741, %r742 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r905, %r906, %r907, %r908 }, { %r759, %r760 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r905, %r906, %r907, %r908 }, { %r761, %r762 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r905, %r906, %r907, %r908 }, { %r779, %r780 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r905, %r906, %r907, %r908 }, { %r781, %r782 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r905, %r906, %r907, %r908 }, { %r799, %r800 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r905, %r906, %r907, %r908 }, { %r801, %r802 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r953, %r954, %r955, %r956 }, { %r744, %r745 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r953, %r954, %r955, %r956 }, { %r746, %r747 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r953, %r954, %r955, %r956 }, { %r764, %r765 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r953, %r954, %r955, %r956 }, { %r766, %r767 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r953, %r954, %r955, %r956 }, { %r784, %r785 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r953, %r954, %r955, %r956 }, { %r786, %r787 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r953, %r954, %r955, %r956 }, { %r804, %r805 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r953, %r954, %r955, %r956 }, { %r806, %r807 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	.loc	1 142 52
	add.s32 	%r1630, %r1837, %r1629;
	add.s32 	%r1631, %r74, %r14;
	shl.b32 	%r1632, %r1631, 1;
	add.s32 	%r1633, %r1837, %r1632;
	add.s32 	%r1634, %r74, %r1851;
	shl.b32 	%r1635, %r1634, 1;
	add.s32 	%r1636, %r1837, %r1635;
	add.s32 	%r1637, %r74, %r1850;
	shl.b32 	%r1638, %r1637, 1;
	add.s32 	%r1639, %r1837, %r1638;
	add.s32 	%r1640, %r74, %r1849;
	shl.b32 	%r1641, %r1640, 1;
	add.s32 	%r1642, %r1837, %r1641;
	add.s32 	%r1643, %r74, %r1848;
	shl.b32 	%r1644, %r1643, 1;
	add.s32 	%r1645, %r1837, %r1644;
	add.s32 	%r1646, %r74, %r1847;
	shl.b32 	%r1647, %r1646, 1;
	add.s32 	%r1648, %r1837, %r1647;
	add.s32 	%r1649, %r74, %r1846;
	shl.b32 	%r1650, %r1649, 1;
	add.s32 	%r1651, %r1837, %r1650;
	add.s32 	%r1652, %r74, %r1845;
	shl.b32 	%r1653, %r1652, 1;
	add.s32 	%r1654, %r1837, %r1653;
	ld.shared.b32 	%r1655, [%r1630];
	mov.b32 	{%rs65, %rs66}, %r1655;
	cvt.f32.f16 	%f1219, %rs66;
	cvt.f32.f16 	%f1220, %rs65;
	sub.f32 	%f1221, %f1220, %f259;
	sub.f32 	%f1222, %f1219, %f260;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs67, %f1222;
	cvt.rn.f16.f32 	%rs68, %f1221;
	mov.b32 	%r1656, {%rs68, %rs67};
	.loc	1 142 52
	ld.shared.b32 	%r1657, [%r1633+1024];
	mov.b32 	{%rs69, %rs70}, %r1657;
	cvt.f32.f16 	%f1223, %rs70;
	cvt.f32.f16 	%f1224, %rs69;
	sub.f32 	%f1225, %f1224, %f261;
	sub.f32 	%f1226, %f1223, %f262;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs71, %f1226;
	cvt.rn.f16.f32 	%rs72, %f1225;
	mov.b32 	%r1658, {%rs72, %rs71};
	.loc	1 142 52
	ld.shared.b32 	%r1659, [%r1633+16];
	mov.b32 	{%rs73, %rs74}, %r1659;
	cvt.f32.f16 	%f1227, %rs74;
	cvt.f32.f16 	%f1228, %rs73;
	sub.f32 	%f1229, %f1228, %f267;
	sub.f32 	%f1230, %f1227, %f268;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs75, %f1230;
	cvt.rn.f16.f32 	%rs76, %f1229;
	mov.b32 	%r1660, {%rs76, %rs75};
	.loc	1 142 52
	ld.shared.b32 	%r1661, [%r1636+1024];
	mov.b32 	{%rs77, %rs78}, %r1661;
	cvt.f32.f16 	%f1231, %rs78;
	cvt.f32.f16 	%f1232, %rs77;
	sub.f32 	%f1233, %f1232, %f269;
	sub.f32 	%f1234, %f1231, %f270;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs79, %f1234;
	cvt.rn.f16.f32 	%rs80, %f1233;
	mov.b32 	%r1662, {%rs80, %rs79};
	.loc	1 142 52
	ld.shared.b32 	%r1663, [%r1633+32];
	mov.b32 	{%rs81, %rs82}, %r1663;
	cvt.f32.f16 	%f1235, %rs82;
	cvt.f32.f16 	%f1236, %rs81;
	sub.f32 	%f1237, %f1236, %f275;
	sub.f32 	%f1238, %f1235, %f276;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs83, %f1238;
	cvt.rn.f16.f32 	%rs84, %f1237;
	mov.b32 	%r1664, {%rs84, %rs83};
	.loc	1 142 52
	ld.shared.b32 	%r1665, [%r1639+1024];
	mov.b32 	{%rs85, %rs86}, %r1665;
	cvt.f32.f16 	%f1239, %rs86;
	cvt.f32.f16 	%f1240, %rs85;
	sub.f32 	%f1241, %f1240, %f277;
	sub.f32 	%f1242, %f1239, %f278;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs87, %f1242;
	cvt.rn.f16.f32 	%rs88, %f1241;
	mov.b32 	%r1666, {%rs88, %rs87};
	.loc	1 142 52
	ld.shared.b32 	%r1667, [%r1633+48];
	mov.b32 	{%rs89, %rs90}, %r1667;
	cvt.f32.f16 	%f1243, %rs90;
	cvt.f32.f16 	%f1244, %rs89;
	sub.f32 	%f1245, %f1244, %f283;
	sub.f32 	%f1246, %f1243, %f284;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs91, %f1246;
	cvt.rn.f16.f32 	%rs92, %f1245;
	mov.b32 	%r1668, {%rs92, %rs91};
	.loc	1 142 52
	ld.shared.b32 	%r1669, [%r1642+1024];
	mov.b32 	{%rs93, %rs94}, %r1669;
	cvt.f32.f16 	%f1247, %rs94;
	cvt.f32.f16 	%f1248, %rs93;
	sub.f32 	%f1249, %f1248, %f285;
	sub.f32 	%f1250, %f1247, %f286;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs95, %f1250;
	cvt.rn.f16.f32 	%rs96, %f1249;
	mov.b32 	%r1670, {%rs96, %rs95};
	.loc	1 142 52
	ld.shared.b32 	%r1671, [%r1633+64];
	mov.b32 	{%rs97, %rs98}, %r1671;
	cvt.f32.f16 	%f1251, %rs98;
	cvt.f32.f16 	%f1252, %rs97;
	sub.f32 	%f1253, %f1252, %f291;
	sub.f32 	%f1254, %f1251, %f292;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs99, %f1254;
	cvt.rn.f16.f32 	%rs100, %f1253;
	mov.b32 	%r1672, {%rs100, %rs99};
	.loc	1 142 52
	ld.shared.b32 	%r1673, [%r1645+1024];
	mov.b32 	{%rs101, %rs102}, %r1673;
	cvt.f32.f16 	%f1255, %rs102;
	cvt.f32.f16 	%f1256, %rs101;
	sub.f32 	%f1257, %f1256, %f293;
	sub.f32 	%f1258, %f1255, %f294;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs103, %f1258;
	cvt.rn.f16.f32 	%rs104, %f1257;
	mov.b32 	%r1674, {%rs104, %rs103};
	.loc	1 142 52
	ld.shared.b32 	%r1675, [%r1633+80];
	mov.b32 	{%rs105, %rs106}, %r1675;
	cvt.f32.f16 	%f1259, %rs106;
	cvt.f32.f16 	%f1260, %rs105;
	sub.f32 	%f1261, %f1260, %f299;
	sub.f32 	%f1262, %f1259, %f300;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs107, %f1262;
	cvt.rn.f16.f32 	%rs108, %f1261;
	mov.b32 	%r1676, {%rs108, %rs107};
	.loc	1 142 52
	ld.shared.b32 	%r1677, [%r1648+1024];
	mov.b32 	{%rs109, %rs110}, %r1677;
	cvt.f32.f16 	%f1263, %rs110;
	cvt.f32.f16 	%f1264, %rs109;
	sub.f32 	%f1265, %f1264, %f301;
	sub.f32 	%f1266, %f1263, %f302;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs111, %f1266;
	cvt.rn.f16.f32 	%rs112, %f1265;
	mov.b32 	%r1678, {%rs112, %rs111};
	.loc	1 142 52
	ld.shared.b32 	%r1679, [%r1633+96];
	mov.b32 	{%rs113, %rs114}, %r1679;
	cvt.f32.f16 	%f1267, %rs114;
	cvt.f32.f16 	%f1268, %rs113;
	sub.f32 	%f1269, %f1268, %f307;
	sub.f32 	%f1270, %f1267, %f308;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs115, %f1270;
	cvt.rn.f16.f32 	%rs116, %f1269;
	mov.b32 	%r1680, {%rs116, %rs115};
	.loc	1 142 52
	ld.shared.b32 	%r1681, [%r1651+1024];
	mov.b32 	{%rs117, %rs118}, %r1681;
	cvt.f32.f16 	%f1271, %rs118;
	cvt.f32.f16 	%f1272, %rs117;
	sub.f32 	%f1273, %f1272, %f309;
	sub.f32 	%f1274, %f1271, %f310;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs119, %f1274;
	cvt.rn.f16.f32 	%rs120, %f1273;
	mov.b32 	%r1682, {%rs120, %rs119};
	.loc	1 142 52
	ld.shared.b32 	%r1683, [%r1633+112];
	mov.b32 	{%rs121, %rs122}, %r1683;
	cvt.f32.f16 	%f1275, %rs122;
	cvt.f32.f16 	%f1276, %rs121;
	sub.f32 	%f1277, %f1276, %f315;
	sub.f32 	%f1278, %f1275, %f316;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs123, %f1278;
	cvt.rn.f16.f32 	%rs124, %f1277;
	mov.b32 	%r1684, {%rs124, %rs123};
	.loc	1 142 52
	ld.shared.b32 	%r1685, [%r1654+1024];
	mov.b32 	{%rs125, %rs126}, %r1685;
	cvt.f32.f16 	%f1279, %rs126;
	cvt.f32.f16 	%f1280, %rs125;
	sub.f32 	%f1281, %f1280, %f317;
	sub.f32 	%f1282, %f1279, %f318;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs127, %f1282;
	cvt.rn.f16.f32 	%rs128, %f1281;
	mov.b32 	%r1686, {%rs128, %rs127};
	bar.sync 	0;
	st.shared.b32 	[%r27], %r1656;
	st.shared.b32 	[%r31], %r1658;
	st.shared.b32 	[%r27+16], %r1660;
	st.shared.b32 	[%r31+16], %r1662;
	st.shared.b32 	[%r27+32], %r1664;
	st.shared.b32 	[%r31+32], %r1666;
	st.shared.b32 	[%r27+48], %r1668;
	st.shared.b32 	[%r31+48], %r1670;
	st.shared.b32 	[%r27+64], %r1672;
	st.shared.b32 	[%r31+64], %r1674;
	st.shared.b32 	[%r27+80], %r1676;
	st.shared.b32 	[%r31+80], %r1678;
	st.shared.b32 	[%r27+96], %r1680;
	st.shared.b32 	[%r31+96], %r1682;
	st.shared.b32 	[%r27+112], %r1684;
	st.shared.b32 	[%r31+112], %r1686;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1001, %r1002, %r1003, %r1004}, [%r38];
	ld.shared.v4.u32 	{%r1005, %r1006, %r1007, %r1008}, [%r38+2304];
	ld.shared.v4.u32 	{%r1009, %r1010, %r1011, %r1012}, [%r38+4608];
	ld.shared.v4.u32 	{%r1013, %r1014, %r1015, %r1016}, [%r38+6912];
	.loc	1 146 26
	shl.b64 	%rd185, %rd240, 1;
	add.s64 	%rd142, %rd5, %rd185;
	shl.b64 	%rd186, %rd239, 1;
	add.s64 	%rd143, %rd5, %rd186;
	shl.b64 	%rd187, %rd238, 1;
	add.s64 	%rd144, %rd5, %rd187;
	shl.b64 	%rd188, %rd237, 1;
	add.s64 	%rd145, %rd5, %rd188;
	// begin inline asm
	@%p138 st.global.v4.b32 [ %rd142 + 0 ], { %r1001, %r1002, %r1003, %r1004 };
	// end inline asm
	// begin inline asm
	@%p137 st.global.v4.b32 [ %rd143 + 0 ], { %r1005, %r1006, %r1007, %r1008 };
	// end inline asm
	// begin inline asm
	@%p136 st.global.v4.b32 [ %rd144 + 0 ], { %r1009, %r1010, %r1011, %r1012 };
	// end inline asm
	// begin inline asm
	@%p135 st.global.v4.b32 [ %rd145 + 0 ], { %r1013, %r1014, %r1015, %r1016 };
	// end inline asm
	.loc	1 183 22
	add.s32 	%r1021, %r1836, %r1687;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1117, %r1118, %r1119, %r1120 }, [ %r1021 + 0 ];
	// end inline asm
	add.s32 	%r1026, %r1021, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1165, %r1166, %r1167, %r1168 }, [ %r1026 + 0 ];
	// end inline asm
	add.s32 	%r1031, %r1021, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1213, %r1214, %r1215, %r1216 }, [ %r1031 + 0 ];
	// end inline asm
	add.s32 	%r1036, %r1021, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1261, %r1262, %r1263, %r1264 }, [ %r1036 + 0 ];
	// end inline asm
	.loc	1 146 33
	bar.sync 	0;
	st.shared.b32 	[%r41], %r1656;
	st.shared.b32 	[%r42], %r1658;
	st.shared.b32 	[%r41+16], %r1660;
	st.shared.b32 	[%r42+16], %r1662;
	st.shared.b32 	[%r41+32], %r1664;
	st.shared.b32 	[%r42+32], %r1666;
	st.shared.b32 	[%r41+48], %r1668;
	st.shared.b32 	[%r42+48], %r1670;
	st.shared.b32 	[%r41+64], %r1672;
	st.shared.b32 	[%r42+64], %r1674;
	st.shared.b32 	[%r41+80], %r1676;
	st.shared.b32 	[%r42+80], %r1678;
	st.shared.b32 	[%r41+96], %r1680;
	st.shared.b32 	[%r42+96], %r1682;
	st.shared.b32 	[%r41+112], %r1684;
	st.shared.b32 	[%r42+112], %r1686;
	bar.sync 	0;
	ld.shared.b32 	%r1688, [%r43];
	ld.shared.b32 	%r1689, [%r43+132];
	ld.shared.b32 	%r1690, [%r43+1056];
	ld.shared.b32 	%r1691, [%r43+1188];
	ld.shared.b32 	%r1692, [%r43+2112];
	ld.shared.b32 	%r1693, [%r43+2244];
	ld.shared.b32 	%r1694, [%r43+3168];
	ld.shared.b32 	%r1695, [%r43+3300];
	ld.shared.b32 	%r1696, [%r43+4224];
	ld.shared.b32 	%r1697, [%r43+4356];
	ld.shared.b32 	%r1698, [%r43+5280];
	ld.shared.b32 	%r1699, [%r43+5412];
	ld.shared.b32 	%r1700, [%r43+6336];
	ld.shared.b32 	%r1701, [%r43+6468];
	ld.shared.b32 	%r1702, [%r43+7392];
	ld.shared.b32 	%r1703, [%r43+7524];
	bar.sync 	0;
	st.shared.b32 	[%r45], %r1688;
	st.shared.b32 	[%r46], %r1689;
	st.shared.b32 	[%r45+1024], %r1690;
	st.shared.b32 	[%r47], %r1691;
	st.shared.b32 	[%r45+2048], %r1692;
	st.shared.b32 	[%r48], %r1693;
	st.shared.b32 	[%r45+3072], %r1694;
	st.shared.b32 	[%r49], %r1695;
	st.shared.b32 	[%r45+4096], %r1696;
	st.shared.b32 	[%r50], %r1697;
	st.shared.b32 	[%r45+5120], %r1698;
	st.shared.b32 	[%r51], %r1699;
	st.shared.b32 	[%r45+6144], %r1700;
	st.shared.b32 	[%r52], %r1701;
	st.shared.b32 	[%r45+7168], %r1702;
	st.shared.b32 	[%r53], %r1703;
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1121, %r1122, %r1127, %r1128 }, [ %r441 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1169, %r1170, %r1175, %r1176 }, [ %r446 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1217, %r1218, %r1223, %r1224 }, [ %r451 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1265, %r1266, %r1271, %r1272 }, [ %r456 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1133, %r1134, %r1139, %r1140 }, [ %r461 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1181, %r1182, %r1187, %r1188 }, [ %r466 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1229, %r1230, %r1235, %r1236 }, [ %r471 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1277, %r1278, %r1283, %r1284 }, [ %r476 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1145, %r1146, %r1151, %r1152 }, [ %r481 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1193, %r1194, %r1199, %r1200 }, [ %r486 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1241, %r1242, %r1247, %r1248 }, [ %r491 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1289, %r1290, %r1295, %r1296 }, [ %r496 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1157, %r1158, %r1163, %r1164 }, [ %r501 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1205, %r1206, %r1211, %r1212 }, [ %r506 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1253, %r1254, %r1259, %r1260 }, [ %r511 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1301, %r1302, %r1307, %r1308 }, [ %r516 + 0 ];
	// end inline asm
	.loc	1 184 28
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1315, %f1316, %f1317, %f1318 }, { %r1117, %r1118, %r1119, %r1120 }, { %r1121, %r1122 }, { %f1315, %f1316, %f1317, %f1318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1319, %f1320, %f1321, %f1322 }, { %r1117, %r1118, %r1119, %r1120 }, { %r1127, %r1128 }, { %f1319, %f1320, %f1321, %f1322 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1323, %f1324, %f1325, %f1326 }, { %r1117, %r1118, %r1119, %r1120 }, { %r1133, %r1134 }, { %f1323, %f1324, %f1325, %f1326 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1327, %f1328, %f1329, %f1330 }, { %r1117, %r1118, %r1119, %r1120 }, { %r1139, %r1140 }, { %f1327, %f1328, %f1329, %f1330 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1331, %f1332, %f1333, %f1334 }, { %r1117, %r1118, %r1119, %r1120 }, { %r1145, %r1146 }, { %f1331, %f1332, %f1333, %f1334 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1335, %f1336, %f1337, %f1338 }, { %r1117, %r1118, %r1119, %r1120 }, { %r1151, %r1152 }, { %f1335, %f1336, %f1337, %f1338 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1339, %f1340, %f1341, %f1342 }, { %r1117, %r1118, %r1119, %r1120 }, { %r1157, %r1158 }, { %f1339, %f1340, %f1341, %f1342 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1343, %f1344, %f1345, %f1346 }, { %r1117, %r1118, %r1119, %r1120 }, { %r1163, %r1164 }, { %f1343, %f1344, %f1345, %f1346 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1315, %f1316, %f1317, %f1318 }, { %r1165, %r1166, %r1167, %r1168 }, { %r1169, %r1170 }, { %f1315, %f1316, %f1317, %f1318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1319, %f1320, %f1321, %f1322 }, { %r1165, %r1166, %r1167, %r1168 }, { %r1175, %r1176 }, { %f1319, %f1320, %f1321, %f1322 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1323, %f1324, %f1325, %f1326 }, { %r1165, %r1166, %r1167, %r1168 }, { %r1181, %r1182 }, { %f1323, %f1324, %f1325, %f1326 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1327, %f1328, %f1329, %f1330 }, { %r1165, %r1166, %r1167, %r1168 }, { %r1187, %r1188 }, { %f1327, %f1328, %f1329, %f1330 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1331, %f1332, %f1333, %f1334 }, { %r1165, %r1166, %r1167, %r1168 }, { %r1193, %r1194 }, { %f1331, %f1332, %f1333, %f1334 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1335, %f1336, %f1337, %f1338 }, { %r1165, %r1166, %r1167, %r1168 }, { %r1199, %r1200 }, { %f1335, %f1336, %f1337, %f1338 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1339, %f1340, %f1341, %f1342 }, { %r1165, %r1166, %r1167, %r1168 }, { %r1205, %r1206 }, { %f1339, %f1340, %f1341, %f1342 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1343, %f1344, %f1345, %f1346 }, { %r1165, %r1166, %r1167, %r1168 }, { %r1211, %r1212 }, { %f1343, %f1344, %f1345, %f1346 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1315, %f1316, %f1317, %f1318 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1217, %r1218 }, { %f1315, %f1316, %f1317, %f1318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1319, %f1320, %f1321, %f1322 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1223, %r1224 }, { %f1319, %f1320, %f1321, %f1322 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1323, %f1324, %f1325, %f1326 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1229, %r1230 }, { %f1323, %f1324, %f1325, %f1326 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1327, %f1328, %f1329, %f1330 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1235, %r1236 }, { %f1327, %f1328, %f1329, %f1330 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1331, %f1332, %f1333, %f1334 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1241, %r1242 }, { %f1331, %f1332, %f1333, %f1334 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1335, %f1336, %f1337, %f1338 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1247, %r1248 }, { %f1335, %f1336, %f1337, %f1338 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1339, %f1340, %f1341, %f1342 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1253, %r1254 }, { %f1339, %f1340, %f1341, %f1342 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1343, %f1344, %f1345, %f1346 }, { %r1213, %r1214, %r1215, %r1216 }, { %r1259, %r1260 }, { %f1343, %f1344, %f1345, %f1346 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1315, %f1316, %f1317, %f1318 }, { %r1261, %r1262, %r1263, %r1264 }, { %r1265, %r1266 }, { %f1315, %f1316, %f1317, %f1318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1319, %f1320, %f1321, %f1322 }, { %r1261, %r1262, %r1263, %r1264 }, { %r1271, %r1272 }, { %f1319, %f1320, %f1321, %f1322 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1323, %f1324, %f1325, %f1326 }, { %r1261, %r1262, %r1263, %r1264 }, { %r1277, %r1278 }, { %f1323, %f1324, %f1325, %f1326 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1327, %f1328, %f1329, %f1330 }, { %r1261, %r1262, %r1263, %r1264 }, { %r1283, %r1284 }, { %f1327, %f1328, %f1329, %f1330 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1331, %f1332, %f1333, %f1334 }, { %r1261, %r1262, %r1263, %r1264 }, { %r1289, %r1290 }, { %f1331, %f1332, %f1333, %f1334 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1335, %f1336, %f1337, %f1338 }, { %r1261, %r1262, %r1263, %r1264 }, { %r1295, %r1296 }, { %f1335, %f1336, %f1337, %f1338 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1339, %f1340, %f1341, %f1342 }, { %r1261, %r1262, %r1263, %r1264 }, { %r1301, %r1302 }, { %f1339, %f1340, %f1341, %f1342 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1343, %f1344, %f1345, %f1346 }, { %r1261, %r1262, %r1263, %r1264 }, { %r1307, %r1308 }, { %f1343, %f1344, %f1345, %f1346 };
	// end inline asm
	.loc	1 187 26
	add.s32 	%r1313, %r1835, %r1687;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1329, %r1330, %r1331, %r1332 }, [ %r1313 + 0 ];
	// end inline asm
	add.s32 	%r1318, %r1313, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1377, %r1378, %r1379, %r1380 }, [ %r1318 + 0 ];
	// end inline asm
	add.s32 	%r1323, %r1313, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1425, %r1426, %r1427, %r1428 }, [ %r1323 + 0 ];
	// end inline asm
	add.s32 	%r1328, %r1313, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1473, %r1474, %r1475, %r1476 }, [ %r1328 + 0 ];
	// end inline asm
	.loc	1 188 32
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1283, %f1284, %f1285, %f1286 }, { %r1329, %r1330, %r1331, %r1332 }, { %r1121, %r1122 }, { %f1283, %f1284, %f1285, %f1286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1287, %f1288, %f1289, %f1290 }, { %r1329, %r1330, %r1331, %r1332 }, { %r1127, %r1128 }, { %f1287, %f1288, %f1289, %f1290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1291, %f1292, %f1293, %f1294 }, { %r1329, %r1330, %r1331, %r1332 }, { %r1133, %r1134 }, { %f1291, %f1292, %f1293, %f1294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1295, %f1296, %f1297, %f1298 }, { %r1329, %r1330, %r1331, %r1332 }, { %r1139, %r1140 }, { %f1295, %f1296, %f1297, %f1298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1299, %f1300, %f1301, %f1302 }, { %r1329, %r1330, %r1331, %r1332 }, { %r1145, %r1146 }, { %f1299, %f1300, %f1301, %f1302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1303, %f1304, %f1305, %f1306 }, { %r1329, %r1330, %r1331, %r1332 }, { %r1151, %r1152 }, { %f1303, %f1304, %f1305, %f1306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1307, %f1308, %f1309, %f1310 }, { %r1329, %r1330, %r1331, %r1332 }, { %r1157, %r1158 }, { %f1307, %f1308, %f1309, %f1310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1311, %f1312, %f1313, %f1314 }, { %r1329, %r1330, %r1331, %r1332 }, { %r1163, %r1164 }, { %f1311, %f1312, %f1313, %f1314 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1283, %f1284, %f1285, %f1286 }, { %r1377, %r1378, %r1379, %r1380 }, { %r1169, %r1170 }, { %f1283, %f1284, %f1285, %f1286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1287, %f1288, %f1289, %f1290 }, { %r1377, %r1378, %r1379, %r1380 }, { %r1175, %r1176 }, { %f1287, %f1288, %f1289, %f1290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1291, %f1292, %f1293, %f1294 }, { %r1377, %r1378, %r1379, %r1380 }, { %r1181, %r1182 }, { %f1291, %f1292, %f1293, %f1294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1295, %f1296, %f1297, %f1298 }, { %r1377, %r1378, %r1379, %r1380 }, { %r1187, %r1188 }, { %f1295, %f1296, %f1297, %f1298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1299, %f1300, %f1301, %f1302 }, { %r1377, %r1378, %r1379, %r1380 }, { %r1193, %r1194 }, { %f1299, %f1300, %f1301, %f1302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1303, %f1304, %f1305, %f1306 }, { %r1377, %r1378, %r1379, %r1380 }, { %r1199, %r1200 }, { %f1303, %f1304, %f1305, %f1306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1307, %f1308, %f1309, %f1310 }, { %r1377, %r1378, %r1379, %r1380 }, { %r1205, %r1206 }, { %f1307, %f1308, %f1309, %f1310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1311, %f1312, %f1313, %f1314 }, { %r1377, %r1378, %r1379, %r1380 }, { %r1211, %r1212 }, { %f1311, %f1312, %f1313, %f1314 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1283, %f1284, %f1285, %f1286 }, { %r1425, %r1426, %r1427, %r1428 }, { %r1217, %r1218 }, { %f1283, %f1284, %f1285, %f1286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1287, %f1288, %f1289, %f1290 }, { %r1425, %r1426, %r1427, %r1428 }, { %r1223, %r1224 }, { %f1287, %f1288, %f1289, %f1290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1291, %f1292, %f1293, %f1294 }, { %r1425, %r1426, %r1427, %r1428 }, { %r1229, %r1230 }, { %f1291, %f1292, %f1293, %f1294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1295, %f1296, %f1297, %f1298 }, { %r1425, %r1426, %r1427, %r1428 }, { %r1235, %r1236 }, { %f1295, %f1296, %f1297, %f1298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1299, %f1300, %f1301, %f1302 }, { %r1425, %r1426, %r1427, %r1428 }, { %r1241, %r1242 }, { %f1299, %f1300, %f1301, %f1302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1303, %f1304, %f1305, %f1306 }, { %r1425, %r1426, %r1427, %r1428 }, { %r1247, %r1248 }, { %f1303, %f1304, %f1305, %f1306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1307, %f1308, %f1309, %f1310 }, { %r1425, %r1426, %r1427, %r1428 }, { %r1253, %r1254 }, { %f1307, %f1308, %f1309, %f1310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1311, %f1312, %f1313, %f1314 }, { %r1425, %r1426, %r1427, %r1428 }, { %r1259, %r1260 }, { %f1311, %f1312, %f1313, %f1314 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1283, %f1284, %f1285, %f1286 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1265, %r1266 }, { %f1283, %f1284, %f1285, %f1286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1287, %f1288, %f1289, %f1290 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1271, %r1272 }, { %f1287, %f1288, %f1289, %f1290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1291, %f1292, %f1293, %f1294 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1277, %r1278 }, { %f1291, %f1292, %f1293, %f1294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1295, %f1296, %f1297, %f1298 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1283, %r1284 }, { %f1295, %f1296, %f1297, %f1298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1299, %f1300, %f1301, %f1302 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1289, %r1290 }, { %f1299, %f1300, %f1301, %f1302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1303, %f1304, %f1305, %f1306 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1295, %r1296 }, { %f1303, %f1304, %f1305, %f1306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1307, %f1308, %f1309, %f1310 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1301, %r1302 }, { %f1307, %f1308, %f1309, %f1310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1311, %f1312, %f1313, %f1314 }, { %r1473, %r1474, %r1475, %r1476 }, { %r1307, %r1308 }, { %f1311, %f1312, %f1313, %f1314 };
	// end inline asm
	.loc	1 113 21
	add.s32 	%r1704, %r1841, 1;
	setp.lt.s32 	%p108, %r1704, 2;
	selp.b32 	%r1841, %r1704, 0, %p108;
	.loc	1 126 83
	cvt.s64.s32 	%rd189, %r1834;
	.loc	1 127 22
	or.b64  	%rd190, %rd189, %rd7;
	or.b64  	%rd191, %rd189, %rd8;
	or.b64  	%rd192, %rd189, %rd9;
	or.b64  	%rd193, %rd189, %rd10;
	shl.b64 	%rd194, %rd190, 11;
	shl.b64 	%rd195, %rd191, 11;
	shl.b64 	%rd196, %rd192, 11;
	shl.b64 	%rd197, %rd193, 11;
	or.b64  	%rd198, %rd194, %rd11;
	or.b64  	%rd199, %rd195, %rd11;
	or.b64  	%rd200, %rd196, %rd11;
	or.b64  	%rd201, %rd197, %rd11;
	shl.b64 	%rd202, %rd198, 1;
	add.s64 	%rd146, %rd4, %rd202;
	shl.b64 	%rd203, %rd199, 1;
	add.s64 	%rd147, %rd4, %rd203;
	shl.b64 	%rd204, %rd200, 1;
	add.s64 	%rd148, %rd4, %rd204;
	shl.b64 	%rd205, %rd201, 1;
	add.s64 	%rd149, %rd4, %rd205;
	setp.lt.u64 	%p109, %rd190, %rd21;
	setp.lt.u64 	%p110, %rd191, %rd21;
	setp.lt.u64 	%p111, %rd192, %rd21;
	setp.lt.u64 	%p112, %rd193, %rd21;
	shl.b32 	%r1705, %r1841, 13;
	add.s32 	%r1707, %r220, %r1705;
	add.s32 	%r1521, %r1707, %r219;
	add.s32 	%r1523, %r1707, %r222;
	add.s32 	%r1525, %r1707, %r224;
	add.s32 	%r1527, %r1707, %r226;
	selp.b32 	%r1712, 16, 0, %p109;
	selp.b32 	%r1530, %r1712, 0, %p107;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1521 + 0 ], [ %rd146 + 0 ], 0x10, %r1530;
	// end inline asm
	selp.b32 	%r1713, 16, 0, %p110;
	selp.b32 	%r1532, %r1713, 0, %p107;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1523 + 0 ], [ %rd147 + 0 ], 0x10, %r1532;
	// end inline asm
	selp.b32 	%r1714, 16, 0, %p111;
	selp.b32 	%r1534, %r1714, 0, %p107;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1525 + 0 ], [ %rd148 + 0 ], 0x10, %r1534;
	// end inline asm
	selp.b32 	%r1715, 16, 0, %p112;
	selp.b32 	%r1536, %r1715, 0, %p107;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1527 + 0 ], [ %rd149 + 0 ], 0x10, %r1536;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 131 26
	add.s64 	%rd150, %rd146, 128;
	add.s64 	%rd151, %rd147, 128;
	add.s64 	%rd152, %rd148, 128;
	add.s64 	%rd153, %rd149, 128;
	add.s32 	%r1717, %r228, %r1705;
	add.s32 	%r1529, %r1717, %r219;
	add.s32 	%r1531, %r1717, %r222;
	add.s32 	%r1533, %r1717, %r224;
	add.s32 	%r1535, %r1717, %r226;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1529 + 0 ], [ %rd150 + 0 ], 0x10, %r1530;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1531 + 0 ], [ %rd151 + 0 ], 0x10, %r1532;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1533 + 0 ], [ %rd152 + 0 ], 0x10, %r1534;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1535 + 0 ], [ %rd153 + 0 ], 0x10, %r1536;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 142 22
	add.s64 	%rd236, %rd194, %rd16;
	add.s64 	%rd235, %rd195, %rd16;
	add.s64 	%rd234, %rd196, %rd16;
	add.s64 	%rd233, %rd197, %rd16;
	shl.b64 	%rd206, %rd236, 1;
	add.s64 	%rd154, %rd2, %rd206;
	shl.b64 	%rd207, %rd235, 1;
	add.s64 	%rd155, %rd2, %rd207;
	shl.b64 	%rd208, %rd234, 1;
	add.s64 	%rd156, %rd2, %rd208;
	shl.b64 	%rd209, %rd233, 1;
	add.s64 	%rd157, %rd2, %rd209;
	and.pred  	%p134, %p64, %p109;
	and.pred  	%p133, %p64, %p110;
	and.pred  	%p132, %p64, %p111;
	and.pred  	%p131, %p64, %p112;
	add.s32 	%r1719, %r230, %r1705;
	add.s32 	%r1537, %r1719, %r229;
	add.s32 	%r1539, %r1719, %r231;
	add.s32 	%r1541, %r1719, %r232;
	add.s32 	%r1543, %r1719, %r233;
	selp.b32 	%r1724, 16, 0, %p134;
	selp.b32 	%r1538, %r1724, 0, %p107;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1537 + 0 ], [ %rd154 + 0 ], 0x10, %r1538;
	// end inline asm
	selp.b32 	%r1725, 16, 0, %p133;
	selp.b32 	%r1540, %r1725, 0, %p107;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1539 + 0 ], [ %rd155 + 0 ], 0x10, %r1540;
	// end inline asm
	selp.b32 	%r1726, 16, 0, %p132;
	selp.b32 	%r1542, %r1726, 0, %p107;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1541 + 0 ], [ %rd156 + 0 ], 0x10, %r1542;
	// end inline asm
	selp.b32 	%r1727, 16, 0, %p131;
	selp.b32 	%r1544, %r1727, 0, %p107;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1543 + 0 ], [ %rd157 + 0 ], 0x10, %r1544;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 183 22
	add.s64 	%rd158, %rd3, %rd202;
	add.s64 	%rd159, %rd3, %rd203;
	add.s64 	%rd160, %rd3, %rd204;
	add.s64 	%rd161, %rd3, %rd205;
	add.s32 	%r1729, %r238, %r1705;
	add.s32 	%r1545, %r1729, %r219;
	add.s32 	%r1547, %r1729, %r222;
	add.s32 	%r1549, %r1729, %r224;
	add.s32 	%r1551, %r1729, %r226;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1545 + 0 ], [ %rd158 + 0 ], 0x10, %r1530;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1547 + 0 ], [ %rd159 + 0 ], 0x10, %r1532;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1549 + 0 ], [ %rd160 + 0 ], 0x10, %r1534;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1551 + 0 ], [ %rd161 + 0 ], 0x10, %r1536;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 187 26
	add.s64 	%rd162, %rd158, 128;
	add.s64 	%rd163, %rd159, 128;
	add.s64 	%rd164, %rd160, 128;
	add.s64 	%rd165, %rd161, 128;
	add.s32 	%r1731, %r239, %r1705;
	bar.sync 	0;
	add.s32 	%r1553, %r1731, %r219;
	add.s32 	%r1555, %r1731, %r222;
	add.s32 	%r1557, %r1731, %r224;
	add.s32 	%r1559, %r1731, %r226;
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1553 + 0 ], [ %rd162 + 0 ], 0x10, %r1530;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1555 + 0 ], [ %rd163 + 0 ], 0x10, %r1532;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1557 + 0 ], [ %rd164 + 0 ], 0x10, %r1534;
	// end inline asm
	// begin inline asm
	@%p63 cp.async.cg.shared.global [ %r1559 + 0 ], [ %rd165 + 0 ], 0x10, %r1536;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 113 21
	add.s32 	%r1732, %r1840, 1;
	setp.lt.s32 	%p113, %r1732, 2;
	selp.b32 	%r1840, %r1732, 0, %p113;
	.loc	1 127 22
	shl.b32 	%r1733, %r1840, 13;
	add.s32 	%r1839, %r220, %r1733;
	.loc	1 131 26
	add.s32 	%r1838, %r228, %r1733;
	.loc	1 142 22
	add.s32 	%r1837, %r230, %r1733;
	.loc	1 183 22
	add.s32 	%r1836, %r238, %r1733;
	.loc	1 127 22
	// begin inline asm
	cp.async.wait_group 0x5;
	// end inline asm
	bar.sync 	0;
	.loc	1 187 26
	add.s32 	%r1835, %r239, %r1733;
	.loc	1 113 21
	add.s32 	%r1842, %r1842, 1;
	add.s32 	%r1834, %r1834, 64;
	add.s32 	%r1833, %r1833, 262144;
	setp.lt.s32 	%p114, %r1842, %r2;
	mov.pred 	%p135, %p9;
	mov.pred 	%p136, %p10;
	mov.pred 	%p137, %p11;
	mov.pred 	%p138, %p12;
	mov.u64 	%rd237, %rd30;
	mov.u64 	%rd238, %rd31;
	mov.u64 	%rd239, %rd32;
	mov.u64 	%rd240, %rd33;
	@%p114 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_4;
$L__BB0_1:
	.loc	1 200 23
	shr.u32 	%r1852, %r3, 4;
	or.b32  	%r1851, %r14, 8;
	or.b32  	%r1850, %r14, 16;
	or.b32  	%r1849, %r14, 24;
	or.b32  	%r1848, %r14, 32;
	or.b32  	%r1847, %r14, 40;
	or.b32  	%r1846, %r14, 48;
	or.b32  	%r1845, %r14, 56;
	shr.u32 	%r1844, %r4, 4;
	shl.b32 	%r1843, %r13, 1;
	mov.f32 	%f1315, 0f00000000;
	mov.f32 	%f1316, %f1315;
	mov.f32 	%f1317, %f1315;
	mov.f32 	%f1318, %f1315;
	mov.f32 	%f1319, %f1315;
	mov.f32 	%f1320, %f1315;
	mov.f32 	%f1321, %f1315;
	mov.f32 	%f1322, %f1315;
	mov.f32 	%f1323, %f1315;
	mov.f32 	%f1324, %f1315;
	mov.f32 	%f1325, %f1315;
	mov.f32 	%f1326, %f1315;
	mov.f32 	%f1327, %f1315;
	mov.f32 	%f1328, %f1315;
	mov.f32 	%f1329, %f1315;
	mov.f32 	%f1330, %f1315;
	mov.f32 	%f1331, %f1315;
	mov.f32 	%f1332, %f1315;
	mov.f32 	%f1333, %f1315;
	mov.f32 	%f1334, %f1315;
	mov.f32 	%f1335, %f1315;
	mov.f32 	%f1336, %f1315;
	mov.f32 	%f1337, %f1315;
	mov.f32 	%f1338, %f1315;
	mov.f32 	%f1339, %f1315;
	mov.f32 	%f1340, %f1315;
	mov.f32 	%f1341, %f1315;
	mov.f32 	%f1342, %f1315;
	mov.f32 	%f1343, %f1315;
	mov.f32 	%f1344, %f1315;
	mov.f32 	%f1345, %f1315;
	mov.f32 	%f1346, %f1315;
	mov.f32 	%f1283, %f1315;
	mov.f32 	%f1284, %f1315;
	mov.f32 	%f1285, %f1315;
	mov.f32 	%f1286, %f1315;
	mov.f32 	%f1287, %f1315;
	mov.f32 	%f1288, %f1315;
	mov.f32 	%f1289, %f1315;
	mov.f32 	%f1290, %f1315;
	mov.f32 	%f1291, %f1315;
	mov.f32 	%f1292, %f1315;
	mov.f32 	%f1293, %f1315;
	mov.f32 	%f1294, %f1315;
	mov.f32 	%f1295, %f1315;
	mov.f32 	%f1296, %f1315;
	mov.f32 	%f1297, %f1315;
	mov.f32 	%f1298, %f1315;
	mov.f32 	%f1299, %f1315;
	mov.f32 	%f1300, %f1315;
	mov.f32 	%f1301, %f1315;
	mov.f32 	%f1302, %f1315;
	mov.f32 	%f1303, %f1315;
	mov.f32 	%f1304, %f1315;
	mov.f32 	%f1305, %f1315;
	mov.f32 	%f1306, %f1315;
	mov.f32 	%f1307, %f1315;
	mov.f32 	%f1308, %f1315;
	mov.f32 	%f1309, %f1315;
	mov.f32 	%f1310, %f1315;
	mov.f32 	%f1311, %f1315;
	mov.f32 	%f1312, %f1315;
	mov.f32 	%f1313, %f1315;
	mov.f32 	%f1314, %f1315;
$L__BB0_4:
	.loc	1 96 27
	shl.b32 	%r1798, %r108, 14;
	.loc	1 96 18
	mul.wide.s32 	%rd226, %r1798, 4;
	add.s64 	%rd227, %rd42, %rd226;
	.loc	1 113 21
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 200 23
	shl.b32 	%r1799, %r3, 2;
	and.b32  	%r1800, %r1799, 60;
	cvt.u64.u32 	%rd228, %r1800;
	shl.b32 	%r1801, %r1852, 7;
	and.b32  	%r1802, %r1801, 896;
	or.b64  	%rd229, %rd6, %rd228;
	mul.wide.u32 	%rd230, %r1802, 4;
	add.s64 	%rd231, %rd227, %rd230;
	shl.b64 	%rd232, %rd229, 2;
	add.s64 	%rd210, %rd231, %rd232;
	add.s64 	%rd211, %rd210, 4096;
	add.s64 	%rd212, %rd210, 8192;
	add.s64 	%rd213, %rd210, 12288;
	add.s64 	%rd214, %rd210, 16384;
	add.s64 	%rd215, %rd210, 20480;
	add.s64 	%rd216, %rd210, 24576;
	add.s64 	%rd217, %rd210, 28672;
	setp.lt.u64 	%p115, %rd229, 128;
	mul.lo.s32 	%r1803, %r15, 68;
	add.s32 	%r1804, %r1803, %r14;
	shl.b32 	%r1805, %r1804, 2;
	add.s32 	%r1807, %r220, %r1805;
	st.shared.v2.f32 	[%r1807], {%f1315, %f1316};
	st.shared.v2.f32 	[%r1807+2176], {%f1317, %f1318};
	add.s32 	%r1808, %r1803, %r1851;
	shl.b32 	%r1809, %r1808, 2;
	add.s32 	%r1810, %r220, %r1809;
	st.shared.v2.f32 	[%r1810], {%f1319, %f1320};
	st.shared.v2.f32 	[%r1810+2176], {%f1321, %f1322};
	add.s32 	%r1811, %r1803, %r1850;
	shl.b32 	%r1812, %r1811, 2;
	add.s32 	%r1813, %r220, %r1812;
	st.shared.v2.f32 	[%r1813], {%f1323, %f1324};
	st.shared.v2.f32 	[%r1813+2176], {%f1325, %f1326};
	add.s32 	%r1814, %r1803, %r1849;
	shl.b32 	%r1815, %r1814, 2;
	add.s32 	%r1816, %r220, %r1815;
	st.shared.v2.f32 	[%r1816], {%f1327, %f1328};
	st.shared.v2.f32 	[%r1816+2176], {%f1329, %f1330};
	add.s32 	%r1817, %r1803, %r1848;
	shl.b32 	%r1818, %r1817, 2;
	add.s32 	%r1819, %r220, %r1818;
	st.shared.v2.f32 	[%r1819], {%f1331, %f1332};
	st.shared.v2.f32 	[%r1819+2176], {%f1333, %f1334};
	add.s32 	%r1820, %r1803, %r1847;
	shl.b32 	%r1821, %r1820, 2;
	add.s32 	%r1822, %r220, %r1821;
	st.shared.v2.f32 	[%r1822], {%f1335, %f1336};
	st.shared.v2.f32 	[%r1822+2176], {%f1337, %f1338};
	add.s32 	%r1823, %r1803, %r1846;
	shl.b32 	%r1824, %r1823, 2;
	add.s32 	%r1825, %r220, %r1824;
	st.shared.v2.f32 	[%r1825], {%f1339, %f1340};
	st.shared.v2.f32 	[%r1825+2176], {%f1341, %f1342};
	add.s32 	%r1826, %r1803, %r1845;
	shl.b32 	%r1827, %r1826, 2;
	add.s32 	%r1828, %r220, %r1827;
	st.shared.v2.f32 	[%r1828], {%f1343, %f1344};
	st.shared.v2.f32 	[%r1828+2176], {%f1345, %f1346};
	bar.sync 	0;
	or.b32  	%r1829, %r1843, %r1844;
	mad.lo.s32 	%r1830, %r1829, 68, %r1800;
	shl.b32 	%r1831, %r1830, 2;
	add.s32 	%r1832, %r220, %r1831;
	ld.shared.v4.u32 	{%r1734, %r1735, %r1736, %r1737}, [%r1832];
	ld.shared.v4.u32 	{%r1738, %r1739, %r1740, %r1741}, [%r1832+2176];
	ld.shared.v4.u32 	{%r1742, %r1743, %r1744, %r1745}, [%r1832+4352];
	ld.shared.v4.u32 	{%r1746, %r1747, %r1748, %r1749}, [%r1832+6528];
	ld.shared.v4.u32 	{%r1750, %r1751, %r1752, %r1753}, [%r1832+8704];
	ld.shared.v4.u32 	{%r1754, %r1755, %r1756, %r1757}, [%r1832+10880];
	ld.shared.v4.u32 	{%r1758, %r1759, %r1760, %r1761}, [%r1832+13056];
	ld.shared.v4.u32 	{%r1762, %r1763, %r1764, %r1765}, [%r1832+15232];
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd210 + 0 ], { %r1734, %r1735, %r1736, %r1737 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd211 + 0 ], { %r1738, %r1739, %r1740, %r1741 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd212 + 0 ], { %r1742, %r1743, %r1744, %r1745 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd213 + 0 ], { %r1746, %r1747, %r1748, %r1749 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd214 + 0 ], { %r1750, %r1751, %r1752, %r1753 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd215 + 0 ], { %r1754, %r1755, %r1756, %r1757 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd216 + 0 ], { %r1758, %r1759, %r1760, %r1761 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd217 + 0 ], { %r1762, %r1763, %r1764, %r1765 };
	// end inline asm
	.loc	1 203 27
	add.s64 	%rd218, %rd210, 32768;
	add.s64 	%rd219, %rd210, 36864;
	add.s64 	%rd220, %rd210, 40960;
	add.s64 	%rd221, %rd210, 45056;
	add.s64 	%rd222, %rd210, 49152;
	add.s64 	%rd223, %rd210, 53248;
	add.s64 	%rd224, %rd210, 57344;
	add.s64 	%rd225, %rd210, 61440;
	bar.sync 	0;
	st.shared.v2.f32 	[%r1807], {%f1283, %f1284};
	st.shared.v2.f32 	[%r1807+2176], {%f1285, %f1286};
	st.shared.v2.f32 	[%r1810], {%f1287, %f1288};
	st.shared.v2.f32 	[%r1810+2176], {%f1289, %f1290};
	st.shared.v2.f32 	[%r1813], {%f1291, %f1292};
	st.shared.v2.f32 	[%r1813+2176], {%f1293, %f1294};
	st.shared.v2.f32 	[%r1816], {%f1295, %f1296};
	st.shared.v2.f32 	[%r1816+2176], {%f1297, %f1298};
	st.shared.v2.f32 	[%r1819], {%f1299, %f1300};
	st.shared.v2.f32 	[%r1819+2176], {%f1301, %f1302};
	st.shared.v2.f32 	[%r1822], {%f1303, %f1304};
	st.shared.v2.f32 	[%r1822+2176], {%f1305, %f1306};
	st.shared.v2.f32 	[%r1825], {%f1307, %f1308};
	st.shared.v2.f32 	[%r1825+2176], {%f1309, %f1310};
	st.shared.v2.f32 	[%r1828], {%f1311, %f1312};
	st.shared.v2.f32 	[%r1828+2176], {%f1313, %f1314};
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1766, %r1767, %r1768, %r1769}, [%r1832];
	ld.shared.v4.u32 	{%r1770, %r1771, %r1772, %r1773}, [%r1832+2176];
	ld.shared.v4.u32 	{%r1774, %r1775, %r1776, %r1777}, [%r1832+4352];
	ld.shared.v4.u32 	{%r1778, %r1779, %r1780, %r1781}, [%r1832+6528];
	ld.shared.v4.u32 	{%r1782, %r1783, %r1784, %r1785}, [%r1832+8704];
	ld.shared.v4.u32 	{%r1786, %r1787, %r1788, %r1789}, [%r1832+10880];
	ld.shared.v4.u32 	{%r1790, %r1791, %r1792, %r1793}, [%r1832+13056];
	ld.shared.v4.u32 	{%r1794, %r1795, %r1796, %r1797}, [%r1832+15232];
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd218 + 0 ], { %r1766, %r1767, %r1768, %r1769 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd219 + 0 ], { %r1770, %r1771, %r1772, %r1773 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd220 + 0 ], { %r1774, %r1775, %r1776, %r1777 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd221 + 0 ], { %r1778, %r1779, %r1780, %r1781 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd222 + 0 ], { %r1782, %r1783, %r1784, %r1785 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd223 + 0 ], { %r1786, %r1787, %r1788, %r1789 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd224 + 0 ], { %r1790, %r1791, %r1792, %r1793 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd225 + 0 ], { %r1794, %r1795, %r1796, %r1797 };
	// end inline asm
	.loc	1 198 4
	ret;
$L__tmp3:
$L__func_end0:

}
	.file	1 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\fla\\ops\\common\\chunk_delta_h.py"
	.file	2 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\triton\\language\\standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 253
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 104
.b8 117
.b8 110
.b8 107
.b8 95
.b8 100
.b8 101
.b8 108
.b8 116
.b8 97
.b8 95
.b8 104
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 68
.b8 58
.b8 92
.b8 85
.b8 115
.b8 101
.b8 114
.b8 115
.b8 92
.b8 76
.b8 111
.b8 117
.b8 105
.b8 115
.b8 92
.b8 80
.b8 121
.b8 99
.b8 104
.b8 97
.b8 114
.b8 109
.b8 80
.b8 114
.b8 111
.b8 106
.b8 101
.b8 99
.b8 116
.b8 115
.b8 92
.b8 77
.b8 97
.b8 115
.b8 116
.b8 101
.b8 114
.b8 95
.b8 116
.b8 104
.b8 101
.b8 115
.b8 105
.b8 115
.b8 92
.b8 66
.b8 97
.b8 98
.b8 105
.b8 108
.b8 111
.b8 110
.b8 103
.b8 95
.b8 66
.b8 101
.b8 110
.b8 99
.b8 104
.b8 109
.b8 97
.b8 114
.b8 107
.b8 92
.b8 46
.b8 118
.b8 101
.b8 110
.b8 118
.b8 92
.b8 76
.b8 105
.b8 98
.b8 92
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 92
.b8 102
.b8 108
.b8 97
.b8 92
.b8 111
.b8 112
.b8 115
.b8 92
.b8 99
.b8 111
.b8 109
.b8 109
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 99
.b8 104
.b8 117
.b8 110
.b8 107
.b8 95
.b8 103
.b8 97
.b8 116
.b8 101
.b8 100
.b8 95
.b8 100
.b8 101
.b8 108
.b8 116
.b8 97
.b8 95
.b8 114
.b8 117
.b8 108
.b8 101
.b8 95
.b8 102
.b8 119
.b8 100
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 95
.b8 104
.b8 95
.b8 98
.b8 108
.b8 111
.b8 99
.b8 107
.b8 100
.b8 105
.b8 109
.b8 54
.b8 52
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 161
.b8 4
.b32 161
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 67
.b8 24
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
