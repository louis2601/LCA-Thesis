//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_86
.address_size 64

	// .globl	chunk_scaled_dot_kkt_fwd_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry chunk_scaled_dot_kkt_fwd_kernel(
	.param .u64 chunk_scaled_dot_kkt_fwd_kernel_param_0,
	.param .u64 chunk_scaled_dot_kkt_fwd_kernel_param_1,
	.param .u64 chunk_scaled_dot_kkt_fwd_kernel_param_2,
	.param .u64 chunk_scaled_dot_kkt_fwd_kernel_param_3,
	.param .u64 chunk_scaled_dot_kkt_fwd_kernel_param_4,
	.param .u32 chunk_scaled_dot_kkt_fwd_kernel_param_5
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<13>;
	.reg .b32 	%r<316>;
	.reg .f32 	%f<319>;
	.reg .b64 	%rd<107>;
	.loc	1 30 0
$L__func_begin0:
	.loc	1 30 0

	ld.param.u64 	%rd9, [chunk_scaled_dot_kkt_fwd_kernel_param_2];
	ld.param.u64 	%rd20, [chunk_scaled_dot_kkt_fwd_kernel_param_0];
	ld.param.u64 	%rd21, [chunk_scaled_dot_kkt_fwd_kernel_param_1];
$L__tmp0:
	.loc	1 45 30
	// begin inline asm
	mov.u32 %r28, %ctaid.x;
	// end inline asm
	.loc	1 45 48
	// begin inline asm
	mov.u32 %r29, %ctaid.y;
	// end inline asm
	.loc	1 46 33
	shr.s32 	%r39, %r29, 31;
	shr.u32 	%r40, %r39, 28;
	add.s32 	%r41, %r29, %r40;
	and.b32  	%r42, %r41, -16;
	sub.s32 	%r43, %r29, %r42;
	ld.param.u64 	%rd22, [chunk_scaled_dot_kkt_fwd_kernel_param_3];
	.loc	1 48 49
	shl.b32 	%r44, %r28, 1;
	ld.param.u64 	%rd23, [chunk_scaled_dot_kkt_fwd_kernel_param_4];
	.loc	1 48 43
	mul.wide.s32 	%rd24, %r44, 4;
	add.s64 	%rd10, %rd23, %rd24;
	mov.pred 	%p3, -1;
	.loc	1 48 27
	// begin inline asm
	mov.u32 %r30, 0x0;
	@%p3 ld.global.b32 { %r30 }, [ %rd10 + 0 ];
	// end inline asm
	.loc	1 48 100
	add.s64 	%rd11, %rd10, 4;
	.loc	1 48 74
	// begin inline asm
	mov.u32 %r31, 0x0;
	@%p3 ld.global.b32 { %r31 }, [ %rd11 + 0 ];
	// end inline asm
	.loc	1 49 40
	mul.wide.s32 	%rd25, %r30, 4;
	add.s64 	%rd12, %rd22, %rd25;
	.loc	1 49 27
	// begin inline asm
	mov.u32 %r32, 0x0;
	@%p3 ld.global.b32 { %r32 }, [ %rd12 + 0 ];
	// end inline asm
	.loc	1 49 86
	add.s64 	%rd13, %rd12, 4;
	.loc	1 49 67
	// begin inline asm
	mov.u32 %r33, 0x0;
	@%p3 ld.global.b32 { %r33 }, [ %rd13 + 0 ];
	// end inline asm
	.loc	1 50 18
	sub.s32 	%r45, %r33, %r32;
	.loc	1 53 16
	shl.b32 	%r46, %r31, 6;
	.loc	1 53 34
	mov.u32 	%r1, %tid.x;
	and.b32  	%r2, %r1, 31;
	shr.u32 	%r3, %r1, 5;
	shr.u32 	%r4, %r1, 2;
	bfe.u32 	%r47, %r1, 2, 3;
	and.b32  	%r5, %r4, 16;
	and.b32  	%r48, %r4, 24;
	bfe.u32 	%r49, %r1, 2, 5;
	or.b32  	%r50, %r47, 32;
	or.b32  	%r51, %r48, %r50;
	and.b32  	%r6, %r4, 23;
	or.b32  	%r7, %r6, 8;
	or.b32  	%r8, %r50, %r5;
	or.b32  	%r9, %r6, 40;
	.loc	1 56 39
	shl.b32 	%r52, %r32, 4;
	.loc	1 56 35
	mul.wide.s32 	%rd26, %r52, 2;
	add.s64 	%rd27, %rd21, %rd26;
	.loc	1 56 43
	mul.wide.s32 	%rd28, %r43, 2;
	add.s64 	%rd29, %rd27, %rd28;
	.loc	1 56 80
	cvt.s64.s32 	%rd1, %r45;
	cvt.s64.s32 	%rd2, %r46;
	cvt.u64.u32 	%rd30, %r49;
	cvt.u64.u32 	%rd31, %r51;
	cvt.u64.u32 	%rd32, %r6;
	cvt.u64.u32 	%rd33, %r7;
	cvt.u64.u32 	%rd34, %r8;
	cvt.u64.u32 	%rd35, %r9;
	.loc	1 57 18
	or.b64  	%rd36, %rd2, %rd30;
	or.b64  	%rd37, %rd2, %rd31;
	or.b64  	%rd38, %rd2, %rd32;
	or.b64  	%rd39, %rd2, %rd33;
	or.b64  	%rd40, %rd2, %rd34;
	or.b64  	%rd41, %rd2, %rd35;
	shl.b64 	%rd42, %rd38, 5;
	add.s64 	%rd14, %rd29, %rd42;
	shl.b64 	%rd43, %rd39, 5;
	add.s64 	%rd15, %rd29, %rd43;
	shl.b64 	%rd44, %rd40, 5;
	add.s64 	%rd16, %rd29, %rd44;
	shl.b64 	%rd45, %rd41, 5;
	add.s64 	%rd17, %rd29, %rd45;
	setp.gt.s64 	%p13, %rd38, -1;
	setp.gt.s64 	%p14, %rd39, -1;
	setp.gt.s64 	%p15, %rd40, -1;
	setp.gt.s64 	%p16, %rd41, -1;
	setp.lt.s64 	%p17, %rd38, %rd1;
	setp.lt.s64 	%p18, %rd39, %rd1;
	setp.lt.s64 	%p19, %rd40, %rd1;
	setp.lt.s64 	%p20, %rd41, %rd1;
	and.pred  	%p7, %p13, %p17;
	and.pred  	%p8, %p14, %p18;
	and.pred  	%p9, %p15, %p19;
	and.pred  	%p10, %p16, %p20;
	// begin inline asm
	mov.u16 %rs1, 0x0;
	@%p7 ld.global.b16 { %rs1 }, [ %rd14 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u16 %rs2, 0x0;
	@%p8 ld.global.b16 { %rs2 }, [ %rd15 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u16 %rs3, 0x0;
	@%p9 ld.global.b16 { %rs3 }, [ %rd16 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u16 %rs4, 0x0;
	@%p10 ld.global.b16 { %rs4 }, [ %rd17 + 0 ];
	// end inline asm
	.loc	1 61 45
	add.s32 	%r10, %r52, %r43;
	.loc	1 61 52
	shl.b32 	%r53, %r10, 7;
	.loc	1 61 36
	mul.wide.s32 	%rd46, %r53, 2;
	add.s64 	%rd47, %rd20, %rd46;
	.loc	1 62 22
	shl.b32 	%r54, %r1, 3;
	and.b32  	%r55, %r54, 24;
	setp.gt.s64 	%p21, %rd36, -1;
	setp.gt.s64 	%p22, %rd37, -1;
	setp.lt.s64 	%p23, %rd36, %rd1;
	setp.lt.s64 	%p24, %rd37, %rd1;
	and.pred  	%p1, %p21, %p23;
	and.pred  	%p2, %p22, %p24;
	mul.wide.u32 	%rd48, %r55, 2;
	shl.b64 	%rd49, %rd36, 12;
	or.b64  	%rd50, %rd49, %rd48;
	add.s64 	%rd18, %rd47, %rd50;
	shl.b64 	%rd51, %rd37, 12;
	or.b64  	%rd52, %rd51, %rd48;
	add.s64 	%rd19, %rd47, %rd52;
	xor.b32  	%r56, %r1, %r54;
	and.b32  	%r57, %r56, 24;
	shl.b32 	%r58, %r49, 6;
	shl.b32 	%r59, %r57, 1;
	or.b32  	%r60, %r58, %r59;
	mov.u32 	%r61, global_smem;
	add.s32 	%r226, %r61, %r60;
	shl.b32 	%r62, %r51, 6;
	or.b32  	%r63, %r62, %r59;
	add.s32 	%r228, %r61, %r63;
	selp.b32 	%r35, 16, 0, %p1;
	// begin inline asm
	@%p3 cp.async.cg.shared.global [ %r226 + 0 ], [ %rd18 + 0 ], 0x10, %r35;
	// end inline asm
	selp.b32 	%r37, 16, 0, %p2;
	// begin inline asm
	@%p3 cp.async.cg.shared.global [ %r228 + 0 ], [ %rd19 + 0 ], 0x10, %r37;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	and.b32  	%r64, %r1, 7;
	bfe.u32 	%r65, %r1, 3, 1;
	bfe.u32 	%r13, %r1, 4, 1;
	and.b32  	%r66, %r3, 2;
	or.b32  	%r67, %r66, %r65;
	bfe.u32 	%r68, %r1, 1, 2;
	xor.b32  	%r69, %r13, %r68;
	shl.b32 	%r70, %r69, 4;
	shl.b32 	%r71, %r67, 9;
	shl.b32 	%r72, %r64, 6;
	or.b32  	%r73, %r71, %r72;
	or.b32  	%r74, %r70, %r73;
	add.s32 	%r94, %r61, %r74;
	or.b32  	%r75, %r13, 2;
	xor.b32  	%r76, %r75, %r68;
	shl.b32 	%r77, %r76, 4;
	or.b32  	%r78, %r77, %r73;
	add.s32 	%r99, %r61, %r78;
	add.s32 	%r104, %r94, 2048;
	add.s32 	%r109, %r99, 2048;
	and.b32  	%r18, %r4, 8;
	and.b32  	%r79, %r1, 23;
	or.b32  	%r80, %r79, %r18;
	xor.b32  	%r81, %r65, %r68;
	shl.b32 	%r82, %r81, 4;
	shl.b32 	%r83, %r80, 6;
	or.b32  	%r84, %r83, %r82;
	add.s32 	%r114, %r61, %r84;
	or.b32  	%r85, %r65, 2;
	xor.b32  	%r86, %r85, %r68;
	shl.b32 	%r87, %r86, 4;
	or.b32  	%r88, %r87, %r83;
	add.s32 	%r119, %r61, %r88;
	add.s32 	%r124, %r114, 2048;
	add.s32 	%r129, %r119, 2048;
	.loc	1 60 21
	or.b32  	%r314, %r55, 32;
	and.b32  	%r89, %r1, 3;
	mul.wide.u32 	%rd53, %r89, 16;
	or.b64  	%rd54, %rd51, %rd53;
	add.s64 	%rd55, %rd54, %rd46;
	add.s64 	%rd56, %rd55, %rd20;
	add.s64 	%rd106, %rd56, 64;
	or.b64  	%rd57, %rd49, %rd53;
	add.s64 	%rd58, %rd57, %rd46;
	add.s64 	%rd59, %rd58, %rd20;
	add.s64 	%rd105, %rd59, 64;
	mov.f32 	%f287, 0f00000000;
	mov.b32 	%r315, -1;
	mov.f32 	%f288, %f287;
	mov.f32 	%f289, %f287;
	mov.f32 	%f290, %f287;
	mov.f32 	%f291, %f287;
	mov.f32 	%f292, %f287;
	mov.f32 	%f293, %f287;
	mov.f32 	%f294, %f287;
	mov.f32 	%f295, %f287;
	mov.f32 	%f296, %f287;
	mov.f32 	%f297, %f287;
	mov.f32 	%f298, %f287;
	mov.f32 	%f299, %f287;
	mov.f32 	%f300, %f287;
	mov.f32 	%f301, %f287;
	mov.f32 	%f302, %f287;
	mov.f32 	%f303, %f287;
	mov.f32 	%f304, %f287;
	mov.f32 	%f305, %f287;
	mov.f32 	%f306, %f287;
	mov.f32 	%f307, %f287;
	mov.f32 	%f308, %f287;
	mov.f32 	%f309, %f287;
	mov.f32 	%f310, %f287;
	mov.f32 	%f311, %f287;
	mov.f32 	%f312, %f287;
	mov.f32 	%f313, %f287;
	mov.f32 	%f314, %f287;
	mov.f32 	%f315, %f287;
	mov.f32 	%f316, %f287;
	mov.f32 	%f317, %f287;
	mov.f32 	%f318, %f287;
$L__BB0_1:
	setp.ne.s32 	%p27, %r315, 2;
	.loc	1 62 22
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r130, %r131, %r132, %r133 }, [ %r94 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r178, %r179, %r180, %r181 }, [ %r99 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r154, %r155, %r156, %r157 }, [ %r104 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r202, %r203, %r204, %r205 }, [ %r109 + 0 ];
	// end inline asm
	.loc	1 63 36
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r134, %r135, %r140, %r141 }, [ %r114 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r182, %r183, %r188, %r189 }, [ %r119 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r146, %r147, %r152, %r153 }, [ %r124 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r194, %r195, %r200, %r201 }, [ %r129 + 0 ];
	// end inline asm
	.loc	1 63 27
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f287, %f288, %f289, %f290 }, { %r130, %r131, %r132, %r133 }, { %r134, %r135 }, { %f287, %f288, %f289, %f290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r130, %r131, %r132, %r133 }, { %r140, %r141 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f295, %f296, %f297, %f298 }, { %r130, %r131, %r132, %r133 }, { %r146, %r147 }, { %f295, %f296, %f297, %f298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r130, %r131, %r132, %r133 }, { %r152, %r153 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f303, %f304, %f305, %f306 }, { %r154, %r155, %r156, %r157 }, { %r134, %r135 }, { %f303, %f304, %f305, %f306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r154, %r155, %r156, %r157 }, { %r140, %r141 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f311, %f312, %f313, %f314 }, { %r154, %r155, %r156, %r157 }, { %r146, %r147 }, { %f311, %f312, %f313, %f314 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r154, %r155, %r156, %r157 }, { %r152, %r153 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f287, %f288, %f289, %f290 }, { %r178, %r179, %r180, %r181 }, { %r182, %r183 }, { %f287, %f288, %f289, %f290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r178, %r179, %r180, %r181 }, { %r188, %r189 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f295, %f296, %f297, %f298 }, { %r178, %r179, %r180, %r181 }, { %r194, %r195 }, { %f295, %f296, %f297, %f298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r178, %r179, %r180, %r181 }, { %r200, %r201 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f303, %f304, %f305, %f306 }, { %r202, %r203, %r204, %r205 }, { %r182, %r183 }, { %f303, %f304, %f305, %f306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r202, %r203, %r204, %r205 }, { %r188, %r189 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f311, %f312, %f313, %f314 }, { %r202, %r203, %r204, %r205 }, { %r194, %r195 }, { %f311, %f312, %f313, %f314 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r202, %r203, %r204, %r205 }, { %r200, %r201 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	.loc	1 62 22
	setp.lt.u32 	%p28, %r314, 128;
	bar.sync 	0;
	selp.b32 	%r230, 16, 0, %p28;
	selp.b32 	%r231, %r230, 0, %p1;
	selp.b32 	%r227, %r231, 0, %p27;
	// begin inline asm
	@%p3 cp.async.cg.shared.global [ %r226 + 0 ], [ %rd105 + 0 ], 0x10, %r227;
	// end inline asm
	selp.b32 	%r232, %r230, 0, %p2;
	selp.b32 	%r229, %r232, 0, %p27;
	// begin inline asm
	@%p3 cp.async.cg.shared.global [ %r228 + 0 ], [ %rd106 + 0 ], 0x10, %r229;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 60 21
	add.s32 	%r315, %r315, 1;
	add.s32 	%r314, %r314, 32;
	add.s64 	%rd106, %rd106, 64;
	add.s64 	%rd105, %rd105, 64;
	setp.lt.u32 	%p29, %r315, 3;
	@%p29 bra 	$L__BB0_1;
	.loc	1 0 21
	cvt.u32.u64 	%r265, %rd2;
	cvt.u32.u64 	%r266, %rd1;
	.loc	1 53 34
	shl.b32 	%r268, %r1, 2;
	and.b32  	%r269, %r268, 60;
	shl.b32 	%r270, %r1, 1;
	and.b32  	%r271, %r270, 6;
	or.b32  	%r272, %r271, %r18;
	or.b32  	%r273, %r272, 49;
	.loc	1 53 21
	or.b32  	%r274, %r265, %r273;
	.loc	1 54 16
	setp.lt.s32 	%p38, %r274, %r266;
	.loc	1 53 34
	or.b32  	%r275, %r272, 48;
	.loc	1 53 21
	or.b32  	%r276, %r265, %r275;
	.loc	1 54 16
	setp.lt.s32 	%p39, %r276, %r266;
	.loc	1 53 34
	or.b32  	%r277, %r272, 33;
	.loc	1 53 21
	or.b32  	%r278, %r265, %r277;
	.loc	1 54 16
	setp.lt.s32 	%p40, %r278, %r266;
	.loc	1 53 34
	or.b32  	%r279, %r272, 32;
	.loc	1 53 21
	or.b32  	%r280, %r265, %r279;
	.loc	1 54 16
	setp.lt.s32 	%p41, %r280, %r266;
	.loc	1 53 34
	or.b32  	%r281, %r272, 17;
	or.b32  	%r282, %r272, 16;
	or.b32  	%r283, %r272, 1;
	.loc	1 53 21
	or.b32  	%r284, %r265, %r272;
	or.b32  	%r285, %r265, %r6;
	or.b32  	%r286, %r265, %r283;
	or.b32  	%r287, %r265, %r7;
	or.b32  	%r288, %r265, %r282;
	or.b32  	%r289, %r265, %r281;
	or.b32  	%r290, %r265, %r8;
	or.b32  	%r291, %r265, %r9;
	.loc	1 54 16
	setp.lt.s32 	%p42, %r291, %r266;
	setp.lt.s32 	%p43, %r290, %r266;
	setp.lt.s32 	%p44, %r289, %r266;
	setp.lt.s32 	%p45, %r288, %r266;
	setp.lt.s32 	%p46, %r287, %r266;
	setp.lt.s32 	%p47, %r286, %r266;
	setp.lt.s32 	%p48, %r285, %r266;
	setp.lt.s32 	%p49, %r284, %r266;
	.loc	1 60 21
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 70 11
	cvt.f32.f16 	%f194, %rs1;
	cvt.f32.f16 	%f195, %rs2;
	cvt.f32.f16 	%f196, %rs3;
	cvt.f32.f16 	%f197, %rs4;
	mul.f32 	%f198, %f287, %f194;
	mul.f32 	%f199, %f288, %f194;
	mul.f32 	%f200, %f289, %f195;
	mul.f32 	%f201, %f290, %f195;
	mul.f32 	%f202, %f291, %f194;
	mul.f32 	%f203, %f292, %f194;
	mul.f32 	%f204, %f293, %f195;
	mul.f32 	%f205, %f294, %f195;
	mul.f32 	%f206, %f303, %f196;
	mul.f32 	%f207, %f304, %f196;
	mul.f32 	%f208, %f305, %f197;
	mul.f32 	%f209, %f306, %f197;
	mul.f32 	%f210, %f307, %f196;
	mul.f32 	%f211, %f308, %f196;
	mul.f32 	%f212, %f309, %f197;
	mul.f32 	%f213, %f310, %f197;
	mul.f32 	%f214, %f311, %f196;
	mul.f32 	%f215, %f312, %f196;
	mul.f32 	%f216, %f313, %f197;
	mul.f32 	%f217, %f314, %f197;
	mul.f32 	%f218, %f315, %f196;
	mul.f32 	%f219, %f316, %f196;
	mul.f32 	%f220, %f317, %f197;
	mul.f32 	%f221, %f318, %f197;
	.loc	1 72 26
	setp.gt.u32 	%p50, %r6, %r272;
	setp.gt.u32 	%p51, %r6, %r283;
	setp.gt.u32 	%p52, %r7, %r272;
	setp.gt.u32 	%p53, %r7, %r283;
	setp.gt.u32 	%p54, %r6, %r282;
	setp.gt.u32 	%p55, %r6, %r281;
	setp.gt.u32 	%p56, %r7, %r282;
	setp.gt.u32 	%p57, %r7, %r281;
	setp.gt.u32 	%p58, %r8, %r279;
	setp.gt.u32 	%p59, %r8, %r277;
	setp.gt.u32 	%p60, %r9, %r279;
	setp.gt.u32 	%p61, %r9, %r277;
	setp.gt.u32 	%p62, %r8, %r275;
	setp.gt.u32 	%p63, %r8, %r273;
	setp.gt.u32 	%p64, %r9, %r275;
	setp.gt.u32 	%p65, %r9, %r273;
	.loc	1 73 29
	selp.f32 	%f222, %f198, 0f00000000, %p49;
	selp.f32 	%f223, %f222, 0f00000000, %p50;
	selp.f32 	%f224, %f223, 0f00000000, %p48;
	selp.f32 	%f225, %f199, 0f00000000, %p47;
	selp.f32 	%f226, %f225, 0f00000000, %p51;
	selp.f32 	%f227, %f226, 0f00000000, %p48;
	selp.f32 	%f228, %f200, 0f00000000, %p49;
	selp.f32 	%f229, %f228, 0f00000000, %p52;
	selp.f32 	%f230, %f229, 0f00000000, %p46;
	selp.f32 	%f231, %f201, 0f00000000, %p47;
	selp.f32 	%f232, %f231, 0f00000000, %p53;
	selp.f32 	%f233, %f232, 0f00000000, %p46;
	selp.f32 	%f234, %f202, 0f00000000, %p45;
	selp.f32 	%f235, %f234, 0f00000000, %p54;
	selp.f32 	%f236, %f235, 0f00000000, %p48;
	selp.f32 	%f237, %f203, 0f00000000, %p44;
	selp.f32 	%f238, %f237, 0f00000000, %p55;
	selp.f32 	%f239, %f238, 0f00000000, %p48;
	selp.f32 	%f240, %f204, 0f00000000, %p45;
	selp.f32 	%f241, %f240, 0f00000000, %p56;
	selp.f32 	%f242, %f241, 0f00000000, %p46;
	selp.f32 	%f243, %f205, 0f00000000, %p44;
	selp.f32 	%f244, %f243, 0f00000000, %p57;
	selp.f32 	%f245, %f244, 0f00000000, %p46;
	selp.f32 	%f246, %f206, 0f00000000, %p49;
	selp.f32 	%f247, %f246, 0f00000000, %p43;
	selp.f32 	%f248, %f207, 0f00000000, %p47;
	selp.f32 	%f249, %f248, 0f00000000, %p43;
	selp.f32 	%f250, %f208, 0f00000000, %p49;
	selp.f32 	%f251, %f250, 0f00000000, %p42;
	selp.f32 	%f252, %f209, 0f00000000, %p47;
	selp.f32 	%f253, %f252, 0f00000000, %p42;
	selp.f32 	%f254, %f210, 0f00000000, %p45;
	selp.f32 	%f255, %f254, 0f00000000, %p43;
	selp.f32 	%f256, %f211, 0f00000000, %p44;
	selp.f32 	%f257, %f256, 0f00000000, %p43;
	selp.f32 	%f258, %f212, 0f00000000, %p45;
	selp.f32 	%f259, %f258, 0f00000000, %p42;
	selp.f32 	%f260, %f213, 0f00000000, %p44;
	selp.f32 	%f261, %f260, 0f00000000, %p42;
	selp.f32 	%f262, %f214, 0f00000000, %p41;
	selp.f32 	%f263, %f262, 0f00000000, %p58;
	selp.f32 	%f264, %f263, 0f00000000, %p43;
	selp.f32 	%f265, %f215, 0f00000000, %p40;
	selp.f32 	%f266, %f265, 0f00000000, %p59;
	selp.f32 	%f267, %f266, 0f00000000, %p43;
	selp.f32 	%f268, %f216, 0f00000000, %p41;
	selp.f32 	%f269, %f268, 0f00000000, %p60;
	selp.f32 	%f270, %f269, 0f00000000, %p42;
	selp.f32 	%f271, %f217, 0f00000000, %p40;
	selp.f32 	%f272, %f271, 0f00000000, %p61;
	selp.f32 	%f273, %f272, 0f00000000, %p42;
	selp.f32 	%f274, %f218, 0f00000000, %p39;
	selp.f32 	%f275, %f274, 0f00000000, %p62;
	selp.f32 	%f276, %f275, 0f00000000, %p43;
	selp.f32 	%f277, %f219, 0f00000000, %p38;
	selp.f32 	%f278, %f277, 0f00000000, %p63;
	selp.f32 	%f279, %f278, 0f00000000, %p43;
	selp.f32 	%f280, %f220, 0f00000000, %p39;
	selp.f32 	%f281, %f280, 0f00000000, %p64;
	selp.f32 	%f282, %f281, 0f00000000, %p42;
	selp.f32 	%f283, %f221, 0f00000000, %p38;
	selp.f32 	%f284, %f283, 0f00000000, %p65;
	selp.f32 	%f285, %f284, 0f00000000, %p42;
	.loc	1 74 48
	shl.b32 	%r292, %r10, 6;
	.loc	1 74 32
	mul.wide.s32 	%rd70, %r292, 4;
	add.s64 	%rd71, %rd9, %rd70;
	.loc	1 53 34
	bfe.u32 	%r293, %r1, 4, 3;
	or.b32  	%r294, %r293, 56;
	or.b32  	%r295, %r293, 48;
	or.b32  	%r296, %r293, 40;
	or.b32  	%r297, %r293, 32;
	or.b32  	%r298, %r293, 24;
	or.b32  	%r299, %r293, 16;
	or.b32  	%r300, %r293, 8;
	cvt.u64.u32 	%rd72, %r293;
	cvt.u64.u32 	%rd73, %r300;
	cvt.u64.u32 	%rd74, %r299;
	cvt.u64.u32 	%rd75, %r298;
	cvt.u64.u32 	%rd76, %r297;
	cvt.u64.u32 	%rd77, %r296;
	cvt.u64.u32 	%rd78, %r295;
	cvt.u64.u32 	%rd79, %r294;
	.loc	1 57 18
	or.b64  	%rd80, %rd2, %rd79;
	or.b64  	%rd81, %rd2, %rd78;
	or.b64  	%rd82, %rd2, %rd77;
	or.b64  	%rd83, %rd2, %rd76;
	or.b64  	%rd84, %rd2, %rd75;
	or.b64  	%rd85, %rd2, %rd74;
	or.b64  	%rd86, %rd2, %rd73;
	or.b64  	%rd87, %rd2, %rd72;
	.loc	1 75 18
	shl.b64 	%rd88, %rd87, 12;
	mul.wide.u32 	%rd89, %r269, 4;
	or.b64  	%rd90, %rd88, %rd89;
	add.s64 	%rd62, %rd71, %rd90;
	shl.b64 	%rd91, %rd86, 12;
	or.b64  	%rd92, %rd91, %rd89;
	add.s64 	%rd63, %rd71, %rd92;
	shl.b64 	%rd93, %rd85, 12;
	or.b64  	%rd94, %rd93, %rd89;
	add.s64 	%rd64, %rd71, %rd94;
	shl.b64 	%rd95, %rd84, 12;
	or.b64  	%rd96, %rd95, %rd89;
	add.s64 	%rd65, %rd71, %rd96;
	shl.b64 	%rd97, %rd83, 12;
	or.b64  	%rd98, %rd97, %rd89;
	add.s64 	%rd66, %rd71, %rd98;
	shl.b64 	%rd99, %rd82, 12;
	or.b64  	%rd100, %rd99, %rd89;
	add.s64 	%rd67, %rd71, %rd100;
	shl.b64 	%rd101, %rd81, 12;
	or.b64  	%rd102, %rd101, %rd89;
	add.s64 	%rd68, %rd71, %rd102;
	shl.b64 	%rd103, %rd80, 12;
	or.b64  	%rd104, %rd103, %rd89;
	add.s64 	%rd69, %rd71, %rd104;
	setp.gt.s64 	%p66, %rd80, -1;
	setp.gt.s64 	%p67, %rd81, -1;
	setp.gt.s64 	%p68, %rd82, -1;
	setp.gt.s64 	%p69, %rd83, -1;
	setp.gt.s64 	%p70, %rd84, -1;
	setp.gt.s64 	%p71, %rd85, -1;
	setp.gt.s64 	%p72, %rd86, -1;
	setp.gt.s64 	%p73, %rd87, -1;
	setp.lt.s64 	%p74, %rd87, %rd1;
	setp.lt.s64 	%p75, %rd86, %rd1;
	setp.lt.s64 	%p76, %rd85, %rd1;
	setp.lt.s64 	%p77, %rd84, %rd1;
	setp.lt.s64 	%p78, %rd83, %rd1;
	setp.lt.s64 	%p79, %rd82, %rd1;
	setp.lt.s64 	%p80, %rd81, %rd1;
	setp.lt.s64 	%p81, %rd80, %rd1;
	and.pred  	%p30, %p73, %p74;
	and.pred  	%p31, %p72, %p75;
	and.pred  	%p32, %p71, %p76;
	and.pred  	%p33, %p70, %p77;
	and.pred  	%p34, %p69, %p78;
	and.pred  	%p35, %p68, %p79;
	and.pred  	%p36, %p67, %p80;
	and.pred  	%p37, %p66, %p81;
	shr.u32 	%r301, %r2, 2;
	or.b32  	%r302, %r301, %r5;
	or.b32  	%r303, %r18, %r271;
	mad.lo.s32 	%r304, %r302, 68, %r303;
	shl.b32 	%r305, %r304, 2;
	add.s32 	%r307, %r61, %r305;
	st.shared.v2.f32 	[%r307], {%f224, %f227};
	st.shared.v2.f32 	[%r307+2176], {%f230, %f233};
	st.shared.v2.f32 	[%r307+64], {%f236, %f239};
	st.shared.v2.f32 	[%r307+2240], {%f242, %f245};
	mov.f32 	%f286, 0f00000000;
	st.shared.v2.f32 	[%r307+128], {%f286, %f286};
	st.shared.v2.f32 	[%r307+2304], {%f286, %f286};
	st.shared.v2.f32 	[%r307+192], {%f286, %f286};
	st.shared.v2.f32 	[%r307+2368], {%f286, %f286};
	bar.sync 	0;
	shl.b32 	%r308, %r3, 1;
	and.b32  	%r309, %r308, 6;
	or.b32  	%r310, %r309, %r13;
	mad.lo.s32 	%r311, %r310, 68, %r269;
	shl.b32 	%r312, %r311, 2;
	add.s32 	%r313, %r61, %r312;
	ld.shared.v4.u32 	{%r233, %r234, %r235, %r236}, [%r313];
	ld.shared.v4.u32 	{%r237, %r238, %r239, %r240}, [%r313+2176];
	ld.shared.v4.u32 	{%r241, %r242, %r243, %r244}, [%r313+4352];
	ld.shared.v4.u32 	{%r245, %r246, %r247, %r248}, [%r313+6528];
	bar.sync 	0;
	st.shared.v2.f32 	[%r307], {%f247, %f249};
	st.shared.v2.f32 	[%r307+2176], {%f251, %f253};
	st.shared.v2.f32 	[%r307+64], {%f255, %f257};
	st.shared.v2.f32 	[%r307+2240], {%f259, %f261};
	st.shared.v2.f32 	[%r307+128], {%f264, %f267};
	st.shared.v2.f32 	[%r307+2304], {%f270, %f273};
	st.shared.v2.f32 	[%r307+192], {%f276, %f279};
	st.shared.v2.f32 	[%r307+2368], {%f282, %f285};
	bar.sync 	0;
	ld.shared.v4.u32 	{%r249, %r250, %r251, %r252}, [%r313];
	ld.shared.v4.u32 	{%r253, %r254, %r255, %r256}, [%r313+2176];
	ld.shared.v4.u32 	{%r257, %r258, %r259, %r260}, [%r313+4352];
	ld.shared.v4.u32 	{%r261, %r262, %r263, %r264}, [%r313+6528];
	// begin inline asm
	@%p30 st.global.v4.b32 [ %rd62 + 0 ], { %r233, %r234, %r235, %r236 };
	// end inline asm
	// begin inline asm
	@%p31 st.global.v4.b32 [ %rd63 + 0 ], { %r237, %r238, %r239, %r240 };
	// end inline asm
	// begin inline asm
	@%p32 st.global.v4.b32 [ %rd64 + 0 ], { %r241, %r242, %r243, %r244 };
	// end inline asm
	// begin inline asm
	@%p33 st.global.v4.b32 [ %rd65 + 0 ], { %r245, %r246, %r247, %r248 };
	// end inline asm
	// begin inline asm
	@%p34 st.global.v4.b32 [ %rd66 + 0 ], { %r249, %r250, %r251, %r252 };
	// end inline asm
	// begin inline asm
	@%p35 st.global.v4.b32 [ %rd67 + 0 ], { %r253, %r254, %r255, %r256 };
	// end inline asm
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd68 + 0 ], { %r257, %r258, %r259, %r260 };
	// end inline asm
	// begin inline asm
	@%p37 st.global.v4.b32 [ %rd69 + 0 ], { %r261, %r262, %r263, %r264 };
	// end inline asm
	.loc	1 75 4
	ret;
$L__tmp1:
$L__func_end0:

}
	.file	1 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\fla\\ops\\common\\chunk_scaled_dot_kkt.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 0
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 164
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 104
.b8 117
.b8 110
.b8 107
.b8 95
.b8 115
.b8 99
.b8 97
.b8 108
.b8 101
.b8 100
.b8 95
.b8 100
.b8 111
.b8 116
.b8 95
.b8 107
.b8 107
.b8 116
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 68
.b8 58
.b8 92
.b8 85
.b8 115
.b8 101
.b8 114
.b8 115
.b8 92
.b8 76
.b8 111
.b8 117
.b8 105
.b8 115
.b8 92
.b8 80
.b8 121
.b8 99
.b8 104
.b8 97
.b8 114
.b8 109
.b8 80
.b8 114
.b8 111
.b8 106
.b8 101
.b8 99
.b8 116
.b8 115
.b8 92
.b8 77
.b8 97
.b8 115
.b8 116
.b8 101
.b8 114
.b8 95
.b8 116
.b8 104
.b8 101
.b8 115
.b8 105
.b8 115
.b8 92
.b8 66
.b8 97
.b8 98
.b8 105
.b8 108
.b8 111
.b8 110
.b8 103
.b8 95
.b8 66
.b8 101
.b8 110
.b8 99
.b8 104
.b8 109
.b8 97
.b8 114
.b8 107
.b8 92
.b8 46
.b8 118
.b8 101
.b8 110
.b8 118
.b8 92
.b8 76
.b8 105
.b8 98
.b8 92
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 92
.b8 102
.b8 108
.b8 97
.b8 92
.b8 111
.b8 112
.b8 115
.b8 92
.b8 99
.b8 111
.b8 109
.b8 109
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
	}
	.section	.debug_loc	{	}
