//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_86
.address_size 64

	// .globl	layer_norm_fwd_kernel
.extern .shared .align 16 .b8 global_smem[];
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};
.global .align 1 .b8 _$_str_$_2[17] = {95, 95, 67, 85, 68, 65, 95, 80, 82, 69, 67, 95, 83, 81, 82, 84};

.visible .entry layer_norm_fwd_kernel(
	.param .u64 layer_norm_fwd_kernel_param_0,
	.param .u64 layer_norm_fwd_kernel_param_1,
	.param .u64 layer_norm_fwd_kernel_param_2,
	.param .u64 layer_norm_fwd_kernel_param_3,
	.param .f32 layer_norm_fwd_kernel_param_4,
	.param .u32 layer_norm_fwd_kernel_param_5
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<92>;
	.reg .f32 	%f<106>;
	.reg .b64 	%rd<26>;
	.loc	1 190 0
$L__func_begin0:
	.loc	1 190 0

	ld.param.u64 	%rd8, [layer_norm_fwd_kernel_param_0];
	ld.param.u64 	%rd9, [layer_norm_fwd_kernel_param_1];
$L__tmp0:
	.loc	1 212 24
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	ld.param.u64 	%rd10, [layer_norm_fwd_kernel_param_2];
	.loc	1 216 23
	mov.u32 	%r39, %tid.x;
	shl.b32 	%r40, %r39, 3;
	ld.param.u64 	%rd11, [layer_norm_fwd_kernel_param_3];
	and.b32  	%r41, %r40, 120;
	ld.param.f32 	%f1, [layer_norm_fwd_kernel_param_4];
	.loc	1 219 54
	shl.b32 	%r42, %r1, 5;
	ld.param.s32 	%rd12, [layer_norm_fwd_kernel_param_5];
	.loc	1 219 72
	cvt.s64.s32 	%rd13, %r42;
	.loc	1 220 18
	bfe.u32 	%r43, %r39, 4, 4;
	or.b32  	%r44, %r43, 16;
	cvt.u64.u32 	%rd14, %r43;
	cvt.u64.u32 	%rd15, %r44;
	or.b64  	%rd16, %rd13, %rd14;
	or.b64  	%rd17, %rd13, %rd15;
	shl.b64 	%rd18, %rd16, 8;
	mul.wide.u32 	%rd19, %r41, 2;
	or.b64  	%rd20, %rd18, %rd19;
	add.s64 	%rd1, %rd8, %rd20;
	shl.b64 	%rd21, %rd17, 8;
	or.b64  	%rd22, %rd21, %rd19;
	add.s64 	%rd2, %rd8, %rd22;
	setp.gt.s64 	%p8, %rd16, -1;
	setp.gt.s64 	%p9, %rd17, -1;
	setp.lt.s64 	%p10, %rd16, %rd12;
	setp.lt.s64 	%p11, %rd17, %rd12;
	and.pred  	%p1, %p8, %p10;
	and.pred  	%p2, %p9, %p11;
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	mov.u32 %r4, 0x0;
	mov.u32 %r5, 0x0;
	@%p1 ld.global.v4.b32 { %r2, %r3, %r4, %r5 }, [ %rd1 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r6, 0x0;
	mov.u32 %r7, 0x0;
	mov.u32 %r8, 0x0;
	mov.u32 %r9, 0x0;
	@%p2 ld.global.v4.b32 { %r6, %r7, %r8, %r9 }, [ %rd2 + 0 ];
	// end inline asm
	.loc	1 220 49
	mov.b32 	{%rs1, %rs2}, %r2;
	cvt.f32.f16 	%f2, %rs1;
	cvt.f32.f16 	%f3, %rs2;
	mov.b32 	{%rs3, %rs4}, %r3;
	cvt.f32.f16 	%f4, %rs4;
	cvt.f32.f16 	%f5, %rs3;
	mov.b32 	{%rs5, %rs6}, %r4;
	cvt.f32.f16 	%f6, %rs6;
	cvt.f32.f16 	%f7, %rs5;
	mov.b32 	{%rs7, %rs8}, %r5;
	cvt.f32.f16 	%f8, %rs8;
	cvt.f32.f16 	%f9, %rs7;
	mov.b32 	{%rs9, %rs10}, %r6;
	cvt.f32.f16 	%f10, %rs9;
	cvt.f32.f16 	%f11, %rs10;
	mov.b32 	{%rs11, %rs12}, %r7;
	cvt.f32.f16 	%f12, %rs12;
	cvt.f32.f16 	%f13, %rs11;
	mov.b32 	{%rs13, %rs14}, %r8;
	cvt.f32.f16 	%f14, %rs14;
	cvt.f32.f16 	%f15, %rs13;
	mov.b32 	{%rs15, %rs16}, %r9;
	cvt.f32.f16 	%f16, %rs16;
	cvt.f32.f16 	%f17, %rs15;
	.loc	1 235 32
	mul.f32 	%f18, %f3, %f3;
	mul.f32 	%f19, %f11, %f11;
$L__tmp1:
	.loc	2 256 15
	fma.rn.f32 	%f20, %f2, %f2, %f18;
	fma.rn.f32 	%f21, %f5, %f5, %f20;
	fma.rn.f32 	%f22, %f4, %f4, %f21;
	fma.rn.f32 	%f23, %f7, %f7, %f22;
	fma.rn.f32 	%f24, %f6, %f6, %f23;
	fma.rn.f32 	%f25, %f9, %f9, %f24;
	fma.rn.f32 	%f26, %f8, %f8, %f25;
	fma.rn.f32 	%f27, %f10, %f10, %f19;
	fma.rn.f32 	%f28, %f13, %f13, %f27;
	fma.rn.f32 	%f29, %f12, %f12, %f28;
	fma.rn.f32 	%f30, %f15, %f15, %f29;
	fma.rn.f32 	%f31, %f14, %f14, %f30;
	fma.rn.f32 	%f32, %f17, %f17, %f31;
	fma.rn.f32 	%f33, %f16, %f16, %f32;
	.loc	2 267 36
	mov.b32 	%r53, %f26;
	shfl.sync.bfly.b32	%r54, %r53, 8, 31, -1;
	mov.b32 	%f34, %r54;
	.loc	2 256 15
	add.f32 	%f35, %f26, %f34;
	.loc	2 267 36
	mov.b32 	%r55, %f35;
	shfl.sync.bfly.b32	%r56, %r55, 4, 31, -1;
	mov.b32 	%f36, %r56;
	.loc	2 256 15
	add.f32 	%f37, %f35, %f36;
	.loc	2 267 36
	mov.b32 	%r57, %f37;
	shfl.sync.bfly.b32	%r58, %r57, 2, 31, -1;
	mov.b32 	%f38, %r58;
	.loc	2 256 15
	add.f32 	%f39, %f37, %f38;
	.loc	2 267 36
	mov.b32 	%r59, %f39;
	shfl.sync.bfly.b32	%r60, %r59, 1, 31, -1;
	mov.b32 	%f40, %r60;
	.loc	2 256 15
	add.f32 	%f41, %f39, %f40;
	.loc	2 267 36
	mov.b32 	%r61, %f33;
	shfl.sync.bfly.b32	%r62, %r61, 8, 31, -1;
	mov.b32 	%f42, %r62;
	.loc	2 256 15
	add.f32 	%f43, %f33, %f42;
	.loc	2 267 36
	mov.b32 	%r63, %f43;
	shfl.sync.bfly.b32	%r64, %r63, 4, 31, -1;
	mov.b32 	%f44, %r64;
	.loc	2 256 15
	add.f32 	%f45, %f43, %f44;
	.loc	2 267 36
	mov.b32 	%r65, %f45;
	shfl.sync.bfly.b32	%r66, %r65, 2, 31, -1;
	mov.b32 	%f46, %r66;
	.loc	2 256 15
	add.f32 	%f47, %f45, %f46;
	.loc	2 267 36
	mov.b32 	%r67, %f47;
	shfl.sync.bfly.b32	%r68, %r67, 1, 31, -1;
	mov.b32 	%f48, %r68;
	.loc	2 256 15
	add.f32 	%f49, %f47, %f48;
$L__tmp2:
	.loc	1 235 50
	mov.b32 	%r11, %f41;
	mov.b32 	%r12, 1124073472;
	// begin inline asm
	div.full.f32 %r10, %r11, %r12;
	// end inline asm
	mov.b32 	%f50, %r10;
	mov.b32 	%r14, %f49;
	// begin inline asm
	div.full.f32 %r13, %r14, %r12;
	// end inline asm
	mov.b32 	%f51, %r13;
	.loc	1 236 33
	add.f32 	%f52, %f50, %f1;
	add.f32 	%f53, %f51, %f1;
	.loc	1 236 25
	sqrt.approx.ftz.f32 	%f54, %f52;
	sqrt.approx.ftz.f32 	%f55, %f53;
	.loc	1 216 23
	and.b32  	%r69, %r39, 31;
	cvt.u64.u32 	%rd23, %r69;
	.loc	1 220 18
	or.b64  	%rd24, %rd13, %rd23;
	.loc	1 236 17
	mov.b32 	%r18, %f54;
	mov.b32 	%r17, 1065353216;
	// begin inline asm
	div.full.f32 %r16, %r17, %r18;
	// end inline asm
	mov.b32 	%f56, %r16;
	mov.b32 	%r21, %f55;
	// begin inline asm
	div.full.f32 %r19, %r17, %r21;
	// end inline asm
	mov.b32 	%f57, %r19;
	.loc	1 239 21
	shl.b64 	%rd25, %rd24, 2;
	add.s64 	%rd3, %rd11, %rd25;
	setp.gt.s64 	%p12, %rd24, -1;
	setp.lt.s64 	%p13, %rd24, %rd12;
	and.pred  	%p14, %p12, %p13;
	shl.b32 	%r70, %r43, 2;
	mov.u32 	%r71, global_smem;
	add.s32 	%r72, %r71, %r70;
	st.shared.u32 	[%r72], %r16;
	st.shared.u32 	[%r72+64], %r19;
	bar.sync 	0;
	shl.b32 	%r73, %r69, 2;
	add.s32 	%r74, %r71, %r73;
	ld.shared.u32 	%r22, [%r74];
	and.b32  	%r75, %r39, 224;
	setp.eq.s32 	%p15, %r75, 0;
	and.pred  	%p3, %p15, %p14;
	// begin inline asm
	@%p3 st.global.b32 [ %rd3 + 0 ], { %r22 };
	// end inline asm
	.loc	1 242 45
	add.s64 	%rd4, %rd10, %rd19;
	mov.pred 	%p4, -1;
	.loc	1 242 22
	// begin inline asm
	mov.u32 %r23, 0x0;
	mov.u32 %r24, 0x0;
	mov.u32 %r25, 0x0;
	mov.u32 %r26, 0x0;
	@%p4 ld.global.v4.b32 { %r23, %r24, %r25, %r26 }, [ %rd4 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r27, 0x0;
	mov.u32 %r28, 0x0;
	mov.u32 %r29, 0x0;
	mov.u32 %r30, 0x0;
	@%p4 ld.global.v4.b32 { %r27, %r28, %r29, %r30 }, [ %rd4 + 0 ];
	// end inline asm
	.loc	1 242 81
	mov.b32 	{%rs17, %rs18}, %r23;
	cvt.f32.f16 	%f58, %rs18;
	cvt.f32.f16 	%f59, %rs17;
	.loc	1 245 86
	mul.f32 	%f60, %f56, %f3;
	mul.f32 	%f61, %f56, %f2;
	.loc	1 246 20
	mul.f32 	%f62, %f61, %f59;
	mul.f32 	%f63, %f60, %f58;
	.loc	1 252 25
	cvt.rn.f16.f32 	%rs19, %f63;
	cvt.rn.f16.f32 	%rs20, %f62;
	mov.b32 	%r84, {%rs20, %rs19};
	.loc	1 242 81
	mov.b32 	{%rs21, %rs22}, %r24;
	cvt.f32.f16 	%f64, %rs22;
	cvt.f32.f16 	%f65, %rs21;
	.loc	1 245 86
	mul.f32 	%f66, %f56, %f4;
	mul.f32 	%f67, %f56, %f5;
	.loc	1 246 20
	mul.f32 	%f68, %f67, %f65;
	mul.f32 	%f69, %f66, %f64;
	.loc	1 252 25
	cvt.rn.f16.f32 	%rs23, %f69;
	cvt.rn.f16.f32 	%rs24, %f68;
	mov.b32 	%r85, {%rs24, %rs23};
	.loc	1 242 81
	mov.b32 	{%rs25, %rs26}, %r25;
	cvt.f32.f16 	%f70, %rs26;
	cvt.f32.f16 	%f71, %rs25;
	.loc	1 245 86
	mul.f32 	%f72, %f56, %f6;
	mul.f32 	%f73, %f56, %f7;
	.loc	1 246 20
	mul.f32 	%f74, %f73, %f71;
	mul.f32 	%f75, %f72, %f70;
	.loc	1 252 25
	cvt.rn.f16.f32 	%rs27, %f75;
	cvt.rn.f16.f32 	%rs28, %f74;
	mov.b32 	%r86, {%rs28, %rs27};
	.loc	1 242 81
	mov.b32 	{%rs29, %rs30}, %r26;
	cvt.f32.f16 	%f76, %rs30;
	cvt.f32.f16 	%f77, %rs29;
	.loc	1 245 86
	mul.f32 	%f78, %f56, %f8;
	mul.f32 	%f79, %f56, %f9;
	.loc	1 246 20
	mul.f32 	%f80, %f79, %f77;
	mul.f32 	%f81, %f78, %f76;
	.loc	1 252 25
	cvt.rn.f16.f32 	%rs31, %f81;
	cvt.rn.f16.f32 	%rs32, %f80;
	mov.b32 	%r87, {%rs32, %rs31};
	.loc	1 242 81
	mov.b32 	{%rs33, %rs34}, %r27;
	cvt.f32.f16 	%f82, %rs34;
	cvt.f32.f16 	%f83, %rs33;
	.loc	1 245 86
	mul.f32 	%f84, %f57, %f11;
	mul.f32 	%f85, %f57, %f10;
	.loc	1 246 20
	mul.f32 	%f86, %f85, %f83;
	mul.f32 	%f87, %f84, %f82;
	.loc	1 252 25
	cvt.rn.f16.f32 	%rs35, %f87;
	cvt.rn.f16.f32 	%rs36, %f86;
	mov.b32 	%r88, {%rs36, %rs35};
	.loc	1 242 81
	mov.b32 	{%rs37, %rs38}, %r28;
	cvt.f32.f16 	%f88, %rs38;
	cvt.f32.f16 	%f89, %rs37;
	.loc	1 245 86
	mul.f32 	%f90, %f57, %f12;
	mul.f32 	%f91, %f57, %f13;
	.loc	1 246 20
	mul.f32 	%f92, %f91, %f89;
	mul.f32 	%f93, %f90, %f88;
	.loc	1 252 25
	cvt.rn.f16.f32 	%rs39, %f93;
	cvt.rn.f16.f32 	%rs40, %f92;
	mov.b32 	%r89, {%rs40, %rs39};
	.loc	1 242 81
	mov.b32 	{%rs41, %rs42}, %r29;
	cvt.f32.f16 	%f94, %rs42;
	cvt.f32.f16 	%f95, %rs41;
	.loc	1 245 86
	mul.f32 	%f96, %f57, %f14;
	mul.f32 	%f97, %f57, %f15;
	.loc	1 246 20
	mul.f32 	%f98, %f97, %f95;
	mul.f32 	%f99, %f96, %f94;
	.loc	1 252 25
	cvt.rn.f16.f32 	%rs43, %f99;
	cvt.rn.f16.f32 	%rs44, %f98;
	mov.b32 	%r90, {%rs44, %rs43};
	.loc	1 242 81
	mov.b32 	{%rs45, %rs46}, %r30;
	cvt.f32.f16 	%f100, %rs46;
	cvt.f32.f16 	%f101, %rs45;
	.loc	1 245 86
	mul.f32 	%f102, %f57, %f16;
	mul.f32 	%f103, %f57, %f17;
	.loc	1 246 20
	mul.f32 	%f104, %f103, %f101;
	mul.f32 	%f105, %f102, %f100;
	.loc	1 252 25
	cvt.rn.f16.f32 	%rs47, %f105;
	cvt.rn.f16.f32 	%rs48, %f104;
	mov.b32 	%r91, {%rs48, %rs47};
	.loc	1 252 18
	add.s64 	%rd6, %rd9, %rd20;
	add.s64 	%rd7, %rd9, %rd22;
	// begin inline asm
	@%p1 st.global.v4.b32 [ %rd6 + 0 ], { %r84, %r85, %r86, %r87 };
	// end inline asm
	// begin inline asm
	@%p2 st.global.v4.b32 [ %rd7 + 0 ], { %r88, %r89, %r90, %r91 };
	// end inline asm
	.loc	1 252 4
	ret;
$L__tmp3:
$L__func_end0:

}
	.file	1 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\fla\\modules\\layernorm.py"
	.file	2 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\triton\\language\\standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 221
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 108
.b8 97
.b8 121
.b8 101
.b8 114
.b8 110
.b8 111
.b8 114
.b8 109
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 68
.b8 58
.b8 92
.b8 85
.b8 115
.b8 101
.b8 114
.b8 115
.b8 92
.b8 76
.b8 111
.b8 117
.b8 105
.b8 115
.b8 92
.b8 80
.b8 121
.b8 99
.b8 104
.b8 97
.b8 114
.b8 109
.b8 80
.b8 114
.b8 111
.b8 106
.b8 101
.b8 99
.b8 116
.b8 115
.b8 92
.b8 77
.b8 97
.b8 115
.b8 116
.b8 101
.b8 114
.b8 95
.b8 116
.b8 104
.b8 101
.b8 115
.b8 105
.b8 115
.b8 92
.b8 66
.b8 97
.b8 98
.b8 105
.b8 108
.b8 111
.b8 110
.b8 103
.b8 95
.b8 66
.b8 101
.b8 110
.b8 99
.b8 104
.b8 109
.b8 97
.b8 114
.b8 107
.b8 92
.b8 46
.b8 118
.b8 101
.b8 110
.b8 118
.b8 92
.b8 76
.b8 105
.b8 98
.b8 92
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 92
.b8 102
.b8 108
.b8 97
.b8 92
.b8 109
.b8 111
.b8 100
.b8 117
.b8 108
.b8 101
.b8 115
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 108
.b8 97
.b8 121
.b8 101
.b8 114
.b8 95
.b8 110
.b8 111
.b8 114
.b8 109
.b8 95
.b8 102
.b8 119
.b8 100
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 154
.b8 4
.b32 154
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 235
.b8 23
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
