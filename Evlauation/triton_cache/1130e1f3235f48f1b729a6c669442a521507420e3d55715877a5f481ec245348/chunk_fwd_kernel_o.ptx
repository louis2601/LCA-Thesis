//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_86
.address_size 64

	// .globl	chunk_fwd_kernel_o
.extern .shared .align 16 .b8 global_smem[];

.visible .entry chunk_fwd_kernel_o(
	.param .u64 chunk_fwd_kernel_o_param_0,
	.param .u64 chunk_fwd_kernel_o_param_1,
	.param .u64 chunk_fwd_kernel_o_param_2,
	.param .u64 chunk_fwd_kernel_o_param_3,
	.param .u64 chunk_fwd_kernel_o_param_4,
	.param .u64 chunk_fwd_kernel_o_param_5,
	.param .u64 chunk_fwd_kernel_o_param_6,
	.param .f32 chunk_fwd_kernel_o_param_7,
	.param .u32 chunk_fwd_kernel_o_param_8
)
.maxntid 64, 1, 1
{
	.reg .pred 	%p<106>;
	.reg .b16 	%rs<189>;
	.reg .b32 	%r<952>;
	.reg .f32 	%f<995>;
	.reg .b64 	%rd<177>;
	.loc	1 33 0
$L__func_begin0:
	.loc	1 33 0

	ld.param.f32 	%f193, [chunk_fwd_kernel_o_param_7];
	ld.param.u64 	%rd44, [chunk_fwd_kernel_o_param_4];
	ld.param.u64 	%rd43, [chunk_fwd_kernel_o_param_2];
	ld.param.u64 	%rd70, [chunk_fwd_kernel_o_param_0];
$L__tmp0:
	.loc	1 55 35
	// begin inline asm
	mov.u32 %r38, %ctaid.x;
	// end inline asm
	ld.param.u64 	%rd71, [chunk_fwd_kernel_o_param_1];
	.loc	1 55 53
	// begin inline asm
	mov.u32 %r39, %ctaid.y;
	// end inline asm
	ld.param.u64 	%rd72, [chunk_fwd_kernel_o_param_3];
	.loc	1 55 71
	// begin inline asm
	mov.u32 %r40, %ctaid.z;
	// end inline asm
	.loc	1 56 33
	shr.s32 	%r91, %r40, 31;
	shr.u32 	%r92, %r91, 28;
	add.s32 	%r93, %r40, %r92;
	and.b32  	%r94, %r93, -16;
	sub.s32 	%r95, %r40, %r94;
	.loc	1 60 49
	shl.b32 	%r96, %r39, 1;
	ld.param.u64 	%rd73, [chunk_fwd_kernel_o_param_5];
	ld.param.u64 	%rd74, [chunk_fwd_kernel_o_param_6];
	.loc	1 60 43
	mul.wide.s32 	%rd75, %r96, 4;
	add.s64 	%rd45, %rd74, %rd75;
	mov.pred 	%p5, -1;
	.loc	1 60 27
	// begin inline asm
	mov.u32 %r41, 0x0;
	@%p5 ld.global.b32 { %r41 }, [ %rd45 + 0 ];
	// end inline asm
	.loc	1 60 100
	add.s64 	%rd46, %rd45, 4;
	.loc	1 60 74
	// begin inline asm
	mov.u32 %r42, 0x0;
	@%p5 ld.global.b32 { %r42 }, [ %rd46 + 0 ];
	// end inline asm
	.loc	1 61 40
	mul.wide.s32 	%rd76, %r41, 4;
	add.s64 	%rd47, %rd73, %rd76;
	.loc	1 61 27
	// begin inline asm
	mov.u32 %r43, 0x0;
	@%p5 ld.global.b32 { %r43 }, [ %rd47 + 0 ];
	// end inline asm
	.loc	1 61 86
	add.s64 	%rd48, %rd47, 4;
	.loc	1 61 67
	// begin inline asm
	mov.u32 %r44, 0x0;
	@%p5 ld.global.b32 { %r44 }, [ %rd48 + 0 ];
	// end inline asm
	.loc	1 62 18
	sub.s32 	%r1, %r44, %r43;
	.loc	1 70 27
	shl.b32 	%r97, %r43, 11;
	shl.b32 	%r98, %r95, 7;
	add.s32 	%r99, %r97, %r98;
	.loc	1 70 9
	cvt.s64.s32 	%rd1, %r99;
	mul.wide.s32 	%rd77, %r99, 2;
	add.s64 	%rd78, %rd70, %rd77;
	.loc	1 71 9
	add.s64 	%rd79, %rd71, %rd77;
	.loc	1 74 17
	shl.b32 	%r100, %r39, 4;
	.loc	1 74 21
	add.s32 	%r101, %r95, %r100;
	.loc	1 74 0
	mul.wide.s32 	%rd80, %r101, 16384;
	.loc	1 74 9
	shl.b64 	%rd81, %rd80, 1;
	add.s64 	%rd82, %rd72, %rd81;
	.loc	1 80 60
	shl.b32 	%r2, %r42, 6;
	.loc	1 80 85
	cvt.s64.s32 	%rd83, %r1;
	cvt.s64.s32 	%rd84, %r2;
	.loc	1 82 68
	shl.b32 	%r102, %r38, 5;
	.loc	1 82 83
	cvt.s64.s32 	%rd2, %r102;
	.loc	1 84 22
	mov.u32 	%r3, %tid.x;
	and.b32  	%r4, %r3, 31;
	shr.u32 	%r5, %r3, 5;
	bfe.u32 	%r6, %r3, 2, 3;
	and.b32  	%r7, %r3, 32;
	shr.u32 	%r103, %r7, 2;
	or.b32  	%r104, %r6, %r103;
	or.b32  	%r105, %r104, 16;
	or.b32  	%r8, %r6, 32;
	or.b32  	%r106, %r8, %r103;
	or.b32  	%r107, %r104, 48;
	cvt.u64.u32 	%rd85, %r104;
	cvt.u64.u32 	%rd14, %r105;
	cvt.u64.u32 	%rd86, %r106;
	cvt.u64.u32 	%rd87, %r107;
	or.b64  	%rd88, %rd84, %rd85;
	or.b64  	%rd89, %rd84, %rd14;
	or.b64  	%rd90, %rd84, %rd86;
	or.b64  	%rd91, %rd84, %rd87;
	shl.b64 	%rd3, %rd88, 11;
	shl.b64 	%rd4, %rd89, 11;
	shl.b64 	%rd5, %rd90, 11;
	shl.b64 	%rd6, %rd91, 11;
	shl.b32 	%r108, %r3, 3;
	and.b32  	%r109, %r108, 24;
	cvt.u64.u32 	%rd7, %r109;
	setp.gt.s64 	%p29, %rd88, -1;
	setp.gt.s64 	%p30, %rd89, -1;
	setp.gt.s64 	%p31, %rd90, -1;
	setp.gt.s64 	%p32, %rd91, -1;
	setp.lt.s64 	%p33, %rd88, %rd83;
	setp.lt.s64 	%p34, %rd89, %rd83;
	setp.lt.s64 	%p35, %rd90, %rd83;
	setp.lt.s64 	%p36, %rd91, %rd83;
	and.pred  	%p1, %p29, %p33;
	and.pred  	%p2, %p30, %p34;
	and.pred  	%p3, %p31, %p35;
	and.pred  	%p4, %p32, %p36;
	.loc	1 88 22
	or.b64  	%rd8, %rd2, %rd7;
	setp.lt.u64 	%p37, %rd8, 128;
	.loc	1 84 22
	or.b64  	%rd92, %rd3, %rd7;
	or.b64  	%rd93, %rd4, %rd7;
	or.b64  	%rd94, %rd5, %rd7;
	or.b64  	%rd95, %rd6, %rd7;
	shl.b64 	%rd96, %rd92, 1;
	add.s64 	%rd49, %rd78, %rd96;
	shl.b64 	%rd97, %rd93, 1;
	add.s64 	%rd50, %rd78, %rd97;
	shl.b64 	%rd98, %rd94, 1;
	add.s64 	%rd51, %rd78, %rd98;
	shl.b64 	%rd99, %rd95, 1;
	add.s64 	%rd52, %rd78, %rd99;
	shl.b32 	%r110, %r104, 5;
	xor.b32  	%r111, %r3, %r108;
	and.b32  	%r112, %r111, 24;
	or.b32  	%r9, %r112, %r110;
	shl.b32 	%r113, %r9, 1;
	mov.u32 	%r88, global_smem;
	add.s32 	%r10, %r88, %r113;
	shl.b32 	%r114, %r105, 5;
	or.b32  	%r11, %r114, %r112;
	shl.b32 	%r115, %r11, 1;
	add.s32 	%r12, %r88, %r115;
	shl.b32 	%r116, %r106, 5;
	or.b32  	%r13, %r116, %r112;
	shl.b32 	%r117, %r13, 1;
	add.s32 	%r14, %r88, %r117;
	shl.b32 	%r118, %r107, 5;
	or.b32  	%r15, %r118, %r112;
	shl.b32 	%r119, %r15, 1;
	add.s32 	%r16, %r88, %r119;
	selp.b32 	%r46, 16, 0, %p1;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r10 + 0 ], [ %rd49 + 0 ], 0x10, %r46;
	// end inline asm
	selp.b32 	%r48, 16, 0, %p2;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r12 + 0 ], [ %rd50 + 0 ], 0x10, %r48;
	// end inline asm
	selp.b32 	%r50, 16, 0, %p3;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r14 + 0 ], [ %rd51 + 0 ], 0x10, %r50;
	// end inline asm
	selp.b32 	%r52, 16, 0, %p4;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r16 + 0 ], [ %rd52 + 0 ], 0x10, %r52;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 86 22
	add.s64 	%rd53, %rd79, %rd96;
	add.s64 	%rd54, %rd79, %rd97;
	add.s64 	%rd55, %rd79, %rd98;
	add.s64 	%rd56, %rd79, %rd99;
	add.s32 	%r87, %r88, 12288;
	add.s32 	%r53, %r87, %r113;
	add.s32 	%r55, %r87, %r115;
	add.s32 	%r57, %r87, %r117;
	add.s32 	%r59, %r87, %r119;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r53 + 0 ], [ %rd53 + 0 ], 0x10, %r46;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r55 + 0 ], [ %rd54 + 0 ], 0x10, %r48;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r57 + 0 ], [ %rd55 + 0 ], 0x10, %r50;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r59 + 0 ], [ %rd56 + 0 ], 0x10, %r52;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 88 22
	mul.wide.u32 	%rd100, %r104, 128;
	mul.wide.u32 	%rd101, %r105, 128;
	shl.b64 	%rd102, %rd100, 1;
	add.s64 	%rd103, %rd82, %rd102;
	shl.b64 	%rd104, %rd8, 1;
	add.s64 	%rd57, %rd103, %rd104;
	shl.b64 	%rd105, %rd101, 1;
	add.s64 	%rd106, %rd82, %rd105;
	add.s64 	%rd58, %rd106, %rd104;
	add.s32 	%r86, %r88, 8192;
	add.s32 	%r61, %r86, %r113;
	add.s32 	%r63, %r86, %r115;
	selp.b32 	%r62, 16, 0, %p37;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r61 + 0 ], [ %rd57 + 0 ], 0x10, %r62;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r63 + 0 ], [ %rd58 + 0 ], 0x10, %r62;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 84 22
	add.s64 	%rd59, %rd49, 64;
	add.s64 	%rd60, %rd50, 64;
	add.s64 	%rd61, %rd51, 64;
	add.s64 	%rd62, %rd52, 64;
	bar.sync 	0;
	add.s32 	%r120, %r88, 4096;
	add.s32 	%r65, %r120, %r113;
	add.s32 	%r67, %r120, %r115;
	add.s32 	%r69, %r120, %r117;
	add.s32 	%r71, %r120, %r119;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r65 + 0 ], [ %rd59 + 0 ], 0x10, %r46;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r67 + 0 ], [ %rd60 + 0 ], 0x10, %r48;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r69 + 0 ], [ %rd61 + 0 ], 0x10, %r50;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r71 + 0 ], [ %rd62 + 0 ], 0x10, %r52;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 86 22
	add.s64 	%rd63, %rd53, 64;
	add.s64 	%rd64, %rd54, 64;
	add.s64 	%rd65, %rd55, 64;
	add.s64 	%rd66, %rd56, 64;
	add.s32 	%r121, %r88, 16384;
	add.s32 	%r73, %r121, %r113;
	add.s32 	%r75, %r121, %r115;
	add.s32 	%r77, %r121, %r117;
	add.s32 	%r79, %r121, %r119;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r73 + 0 ], [ %rd63 + 0 ], 0x10, %r46;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r75 + 0 ], [ %rd64 + 0 ], 0x10, %r48;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r77 + 0 ], [ %rd65 + 0 ], 0x10, %r50;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r79 + 0 ], [ %rd66 + 0 ], 0x10, %r52;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 88 22
	add.s64 	%rd67, %rd57, 8192;
	or.b64  	%rd107, %rd105, 8192;
	add.s64 	%rd108, %rd82, %rd107;
	add.s64 	%rd68, %rd108, %rd104;
	add.s32 	%r122, %r88, 10240;
	add.s32 	%r81, %r122, %r113;
	add.s32 	%r83, %r122, %r115;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r81 + 0 ], [ %rd67 + 0 ], 0x10, %r62;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r83 + 0 ], [ %rd68 + 0 ], 0x10, %r62;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 84 22
	// begin inline asm
	cp.async.wait_group 0x3;
	// end inline asm
	bar.sync 	0;
	and.b32  	%r123, %r3, 7;
	bfe.u32 	%r124, %r3, 3, 1;
	bfe.u32 	%r17, %r3, 4, 1;
	shr.u32 	%r125, %r3, 4;
	and.b32  	%r126, %r125, 2;
	or.b32  	%r127, %r126, %r124;
	bfe.u32 	%r128, %r3, 1, 2;
	or.b32  	%r18, %r17, 2;
	or.b32  	%r129, %r124, 2;
	xor.b32  	%r130, %r17, %r128;
	shl.b32 	%r131, %r127, 8;
	shl.b32 	%r132, %r123, 5;
	or.b32  	%r133, %r131, %r132;
	shl.b32 	%r134, %r130, 3;
	or.b32  	%r19, %r134, %r133;
	xor.b32  	%r135, %r18, %r128;
	shl.b32 	%r136, %r135, 3;
	or.b32  	%r20, %r136, %r133;
	shl.b32 	%r137, %r3, 5;
	and.b32  	%r21, %r137, 480;
	or.b32  	%r22, %r134, %r21;
	or.b32  	%r23, %r136, %r21;
	xor.b32  	%r138, %r124, %r128;
	shl.b32 	%r139, %r17, 8;
	or.b32  	%r140, %r139, %r132;
	shl.b32 	%r141, %r138, 3;
	or.b32  	%r24, %r141, %r140;
	xor.b32  	%r142, %r129, %r128;
	shl.b32 	%r143, %r142, 3;
	or.b32  	%r25, %r143, %r140;
	.loc	1 79 21
	and.b32  	%r144, %r3, 3;
	mul.wide.u32 	%rd9, %r144, 16;
	shl.b64 	%rd109, %rd91, 12;
	add.s64 	%rd110, %rd109, %rd77;
	or.b64  	%rd111, %rd110, 128;
	add.s64 	%rd176, %rd71, %rd111;
	add.s64 	%rd175, %rd70, %rd111;
	or.b32  	%r145, %r104, 32;
	cvt.u64.u32 	%rd112, %r145;
	or.b64  	%rd113, %rd84, %rd112;
	shl.b64 	%rd114, %rd113, 12;
	add.s64 	%rd115, %rd114, %rd77;
	or.b64  	%rd116, %rd115, 128;
	add.s64 	%rd174, %rd71, %rd116;
	add.s64 	%rd173, %rd70, %rd116;
	shl.b64 	%rd117, %rd89, 12;
	add.s64 	%rd118, %rd117, %rd77;
	or.b64  	%rd119, %rd118, 128;
	add.s64 	%rd172, %rd71, %rd119;
	add.s64 	%rd171, %rd70, %rd119;
	shl.b64 	%rd120, %rd88, 12;
	add.s64 	%rd121, %rd120, %rd77;
	or.b64  	%rd122, %rd121, 128;
	add.s64 	%rd170, %rd71, %rd122;
	add.s64 	%rd169, %rd70, %rd122;
	mul.wide.s32 	%rd123, %r101, 32768;
	mul.wide.u32 	%rd124, %r105, 256;
	or.b64  	%rd125, %rd123, %rd124;
	mul.wide.s32 	%rd126, %r102, 2;
	add.s64 	%rd127, %rd125, %rd126;
	add.s64 	%rd128, %rd127, %rd72;
	add.s64 	%rd167, %rd128, 16384;
	mul.wide.u32 	%rd129, %r104, 256;
	or.b64  	%rd130, %rd123, %rd129;
	add.s64 	%rd131, %rd130, %rd126;
	add.s64 	%rd132, %rd131, %rd72;
	add.s64 	%rd166, %rd132, 16384;
	mov.f32 	%f899, 0f00000000;
	mov.b32 	%r951, 1;
	mov.b32 	%r950, 0;
	mov.b32 	%r946, -1;
	mov.u64 	%rd168, 64;
	shl.b32 	%r534, %r19, 1;
	shl.b32 	%r535, %r20, 1;
	shl.b32 	%r536, %r22, 1;
	shl.b32 	%r537, %r23, 1;
	shl.b32 	%r538, %r24, 1;
	shl.b32 	%r539, %r25, 1;
	mov.u32 	%r947, %r86;
	mov.u32 	%r948, %r87;
	mov.u32 	%r949, %r88;
	mov.f32 	%f900, %f899;
	mov.f32 	%f901, %f899;
	mov.f32 	%f902, %f899;
	mov.f32 	%f903, %f899;
	mov.f32 	%f904, %f899;
	mov.f32 	%f905, %f899;
	mov.f32 	%f906, %f899;
	mov.f32 	%f907, %f899;
	mov.f32 	%f908, %f899;
	mov.f32 	%f909, %f899;
	mov.f32 	%f910, %f899;
	mov.f32 	%f911, %f899;
	mov.f32 	%f912, %f899;
	mov.f32 	%f913, %f899;
	mov.f32 	%f914, %f899;
	mov.f32 	%f915, %f899;
	mov.f32 	%f916, %f899;
	mov.f32 	%f917, %f899;
	mov.f32 	%f918, %f899;
	mov.f32 	%f919, %f899;
	mov.f32 	%f920, %f899;
	mov.f32 	%f921, %f899;
	mov.f32 	%f922, %f899;
	mov.f32 	%f923, %f899;
	mov.f32 	%f924, %f899;
	mov.f32 	%f925, %f899;
	mov.f32 	%f926, %f899;
	mov.f32 	%f927, %f899;
	mov.f32 	%f928, %f899;
	mov.f32 	%f929, %f899;
	mov.f32 	%f930, %f899;
	mov.f32 	%f931, %f899;
	mov.f32 	%f932, %f899;
	mov.f32 	%f933, %f899;
	mov.f32 	%f934, %f899;
	mov.f32 	%f935, %f899;
	mov.f32 	%f936, %f899;
	mov.f32 	%f937, %f899;
	mov.f32 	%f938, %f899;
	mov.f32 	%f939, %f899;
	mov.f32 	%f940, %f899;
	mov.f32 	%f941, %f899;
	mov.f32 	%f942, %f899;
	mov.f32 	%f943, %f899;
	mov.f32 	%f944, %f899;
	mov.f32 	%f945, %f899;
	mov.f32 	%f946, %f899;
	mov.f32 	%f947, %f899;
	mov.f32 	%f948, %f899;
	mov.f32 	%f949, %f899;
	mov.f32 	%f950, %f899;
	mov.f32 	%f951, %f899;
	mov.f32 	%f952, %f899;
	mov.f32 	%f953, %f899;
	mov.f32 	%f954, %f899;
	mov.f32 	%f955, %f899;
	mov.f32 	%f956, %f899;
	mov.f32 	%f957, %f899;
	mov.f32 	%f958, %f899;
	mov.f32 	%f959, %f899;
	mov.f32 	%f960, %f899;
	mov.f32 	%f961, %f899;
	mov.f32 	%f962, %f899;
	mov.f32 	%f963, %f899;
	mov.f32 	%f964, %f899;
	mov.f32 	%f965, %f899;
	mov.f32 	%f966, %f899;
	mov.f32 	%f967, %f899;
	mov.f32 	%f968, %f899;
	mov.f32 	%f969, %f899;
	mov.f32 	%f970, %f899;
	mov.f32 	%f971, %f899;
	mov.f32 	%f972, %f899;
	mov.f32 	%f973, %f899;
	mov.f32 	%f974, %f899;
	mov.f32 	%f975, %f899;
	mov.f32 	%f976, %f899;
	mov.f32 	%f977, %f899;
	mov.f32 	%f978, %f899;
	mov.f32 	%f979, %f899;
	mov.f32 	%f980, %f899;
	mov.f32 	%f981, %f899;
	mov.f32 	%f982, %f899;
	mov.f32 	%f983, %f899;
	mov.f32 	%f984, %f899;
	mov.f32 	%f985, %f899;
	mov.f32 	%f986, %f899;
	mov.f32 	%f987, %f899;
	mov.f32 	%f988, %f899;
	mov.f32 	%f989, %f899;
	mov.f32 	%f990, %f899;
	mov.f32 	%f991, %f899;
	mov.f32 	%f992, %f899;
	mov.f32 	%f993, %f899;
	mov.f32 	%f994, %f899;
$L__BB0_1:
	add.s32 	%r946, %r946, 1;
	setp.lt.u32 	%p48, %r946, 2;
	.loc	1 84 22
	add.s32 	%r150, %r949, %r534;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r186, %r187, %r188, %r189 }, [ %r150 + 0 ];
	// end inline asm
	add.s32 	%r155, %r949, %r535;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r234, %r235, %r236, %r237 }, [ %r155 + 0 ];
	// end inline asm
	add.s32 	%r160, %r150, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r210, %r211, %r212, %r213 }, [ %r160 + 0 ];
	// end inline asm
	add.s32 	%r165, %r155, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r258, %r259, %r260, %r261 }, [ %r165 + 0 ];
	// end inline asm
	.loc	1 88 22
	add.s32 	%r170, %r947, %r536;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r190, %r191, %r196, %r197 }, [ %r170 + 0 ];
	// end inline asm
	add.s32 	%r175, %r170, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r238, %r239, %r244, %r245 }, [ %r175 + 0 ];
	// end inline asm
	add.s32 	%r180, %r947, %r537;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r202, %r203, %r208, %r209 }, [ %r180 + 0 ];
	// end inline asm
	add.s32 	%r185, %r180, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r250, %r251, %r256, %r257 }, [ %r185 + 0 ];
	// end inline asm
	.loc	1 91 27
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f963, %f964, %f965, %f966 }, { %r186, %r187, %r188, %r189 }, { %r190, %r191 }, { %f963, %f964, %f965, %f966 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f967, %f968, %f969, %f970 }, { %r186, %r187, %r188, %r189 }, { %r196, %r197 }, { %f967, %f968, %f969, %f970 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f971, %f972, %f973, %f974 }, { %r186, %r187, %r188, %r189 }, { %r202, %r203 }, { %f971, %f972, %f973, %f974 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f975, %f976, %f977, %f978 }, { %r186, %r187, %r188, %r189 }, { %r208, %r209 }, { %f975, %f976, %f977, %f978 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f979, %f980, %f981, %f982 }, { %r210, %r211, %r212, %r213 }, { %r190, %r191 }, { %f979, %f980, %f981, %f982 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f983, %f984, %f985, %f986 }, { %r210, %r211, %r212, %r213 }, { %r196, %r197 }, { %f983, %f984, %f985, %f986 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f987, %f988, %f989, %f990 }, { %r210, %r211, %r212, %r213 }, { %r202, %r203 }, { %f987, %f988, %f989, %f990 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f991, %f992, %f993, %f994 }, { %r210, %r211, %r212, %r213 }, { %r208, %r209 }, { %f991, %f992, %f993, %f994 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f963, %f964, %f965, %f966 }, { %r234, %r235, %r236, %r237 }, { %r238, %r239 }, { %f963, %f964, %f965, %f966 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f967, %f968, %f969, %f970 }, { %r234, %r235, %r236, %r237 }, { %r244, %r245 }, { %f967, %f968, %f969, %f970 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f971, %f972, %f973, %f974 }, { %r234, %r235, %r236, %r237 }, { %r250, %r251 }, { %f971, %f972, %f973, %f974 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f975, %f976, %f977, %f978 }, { %r234, %r235, %r236, %r237 }, { %r256, %r257 }, { %f975, %f976, %f977, %f978 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f979, %f980, %f981, %f982 }, { %r258, %r259, %r260, %r261 }, { %r238, %r239 }, { %f979, %f980, %f981, %f982 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f983, %f984, %f985, %f986 }, { %r258, %r259, %r260, %r261 }, { %r244, %r245 }, { %f983, %f984, %f985, %f986 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f987, %f988, %f989, %f990 }, { %r258, %r259, %r260, %r261 }, { %r250, %r251 }, { %f987, %f988, %f989, %f990 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f991, %f992, %f993, %f994 }, { %r258, %r259, %r260, %r261 }, { %r256, %r257 }, { %f991, %f992, %f993, %f994 };
	// end inline asm
	.loc	1 86 22
	add.s32 	%r286, %r948, %r538;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r326, %r327, %r332, %r333 }, [ %r286 + 0 ];
	// end inline asm
	add.s32 	%r291, %r948, %r539;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r422, %r423, %r428, %r429 }, [ %r291 + 0 ];
	// end inline asm
	add.s32 	%r296, %r286, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r338, %r339, %r344, %r345 }, [ %r296 + 0 ];
	// end inline asm
	add.s32 	%r301, %r291, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r434, %r435, %r440, %r441 }, [ %r301 + 0 ];
	// end inline asm
	add.s32 	%r306, %r286, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r350, %r351, %r356, %r357 }, [ %r306 + 0 ];
	// end inline asm
	add.s32 	%r311, %r291, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r446, %r447, %r452, %r453 }, [ %r311 + 0 ];
	// end inline asm
	add.s32 	%r316, %r286, 3072;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r362, %r363, %r368, %r369 }, [ %r316 + 0 ];
	// end inline asm
	add.s32 	%r321, %r291, 3072;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r458, %r459, %r464, %r465 }, [ %r321 + 0 ];
	// end inline asm
	.loc	1 93 27
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f899, %f900, %f901, %f902 }, { %r186, %r187, %r188, %r189 }, { %r326, %r327 }, { %f899, %f900, %f901, %f902 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f903, %f904, %f905, %f906 }, { %r186, %r187, %r188, %r189 }, { %r332, %r333 }, { %f903, %f904, %f905, %f906 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f907, %f908, %f909, %f910 }, { %r186, %r187, %r188, %r189 }, { %r338, %r339 }, { %f907, %f908, %f909, %f910 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f911, %f912, %f913, %f914 }, { %r186, %r187, %r188, %r189 }, { %r344, %r345 }, { %f911, %f912, %f913, %f914 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f915, %f916, %f917, %f918 }, { %r186, %r187, %r188, %r189 }, { %r350, %r351 }, { %f915, %f916, %f917, %f918 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f919, %f920, %f921, %f922 }, { %r186, %r187, %r188, %r189 }, { %r356, %r357 }, { %f919, %f920, %f921, %f922 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f923, %f924, %f925, %f926 }, { %r186, %r187, %r188, %r189 }, { %r362, %r363 }, { %f923, %f924, %f925, %f926 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f927, %f928, %f929, %f930 }, { %r186, %r187, %r188, %r189 }, { %r368, %r369 }, { %f927, %f928, %f929, %f930 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f931, %f932, %f933, %f934 }, { %r210, %r211, %r212, %r213 }, { %r326, %r327 }, { %f931, %f932, %f933, %f934 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f935, %f936, %f937, %f938 }, { %r210, %r211, %r212, %r213 }, { %r332, %r333 }, { %f935, %f936, %f937, %f938 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f939, %f940, %f941, %f942 }, { %r210, %r211, %r212, %r213 }, { %r338, %r339 }, { %f939, %f940, %f941, %f942 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f943, %f944, %f945, %f946 }, { %r210, %r211, %r212, %r213 }, { %r344, %r345 }, { %f943, %f944, %f945, %f946 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f947, %f948, %f949, %f950 }, { %r210, %r211, %r212, %r213 }, { %r350, %r351 }, { %f947, %f948, %f949, %f950 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f951, %f952, %f953, %f954 }, { %r210, %r211, %r212, %r213 }, { %r356, %r357 }, { %f951, %f952, %f953, %f954 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f955, %f956, %f957, %f958 }, { %r210, %r211, %r212, %r213 }, { %r362, %r363 }, { %f955, %f956, %f957, %f958 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f959, %f960, %f961, %f962 }, { %r210, %r211, %r212, %r213 }, { %r368, %r369 }, { %f959, %f960, %f961, %f962 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f899, %f900, %f901, %f902 }, { %r234, %r235, %r236, %r237 }, { %r422, %r423 }, { %f899, %f900, %f901, %f902 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f903, %f904, %f905, %f906 }, { %r234, %r235, %r236, %r237 }, { %r428, %r429 }, { %f903, %f904, %f905, %f906 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f907, %f908, %f909, %f910 }, { %r234, %r235, %r236, %r237 }, { %r434, %r435 }, { %f907, %f908, %f909, %f910 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f911, %f912, %f913, %f914 }, { %r234, %r235, %r236, %r237 }, { %r440, %r441 }, { %f911, %f912, %f913, %f914 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f915, %f916, %f917, %f918 }, { %r234, %r235, %r236, %r237 }, { %r446, %r447 }, { %f915, %f916, %f917, %f918 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f919, %f920, %f921, %f922 }, { %r234, %r235, %r236, %r237 }, { %r452, %r453 }, { %f919, %f920, %f921, %f922 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f923, %f924, %f925, %f926 }, { %r234, %r235, %r236, %r237 }, { %r458, %r459 }, { %f923, %f924, %f925, %f926 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f927, %f928, %f929, %f930 }, { %r234, %r235, %r236, %r237 }, { %r464, %r465 }, { %f927, %f928, %f929, %f930 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f931, %f932, %f933, %f934 }, { %r258, %r259, %r260, %r261 }, { %r422, %r423 }, { %f931, %f932, %f933, %f934 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f935, %f936, %f937, %f938 }, { %r258, %r259, %r260, %r261 }, { %r428, %r429 }, { %f935, %f936, %f937, %f938 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f939, %f940, %f941, %f942 }, { %r258, %r259, %r260, %r261 }, { %r434, %r435 }, { %f939, %f940, %f941, %f942 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f943, %f944, %f945, %f946 }, { %r258, %r259, %r260, %r261 }, { %r440, %r441 }, { %f943, %f944, %f945, %f946 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f947, %f948, %f949, %f950 }, { %r258, %r259, %r260, %r261 }, { %r446, %r447 }, { %f947, %f948, %f949, %f950 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f951, %f952, %f953, %f954 }, { %r258, %r259, %r260, %r261 }, { %r452, %r453 }, { %f951, %f952, %f953, %f954 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f955, %f956, %f957, %f958 }, { %r258, %r259, %r260, %r261 }, { %r458, %r459 }, { %f955, %f956, %f957, %f958 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f959, %f960, %f961, %f962 }, { %r258, %r259, %r260, %r261 }, { %r464, %r465 }, { %f959, %f960, %f961, %f962 };
	// end inline asm
	.loc	1 79 21
	add.s32 	%r540, %r951, 1;
	setp.lt.s32 	%p49, %r540, 2;
	selp.b32 	%r951, %r540, 0, %p49;
	.loc	1 84 22
	add.s64 	%rd143, %rd7, %rd168;
	.loc	1 80 85
	add.s64 	%rd144, %rd14, %rd168;
	add.s64 	%rd133, %rd169, %rd9;
	add.s64 	%rd134, %rd171, %rd9;
	add.s64 	%rd135, %rd173, %rd9;
	.loc	1 84 22
	add.s64 	%rd136, %rd175, %rd9;
	setp.lt.u64 	%p50, %rd143, 128;
	shl.b32 	%r541, %r951, 11;
	shl.b32 	%r542, %r951, 12;
	add.s32 	%r544, %r88, %r542;
	bar.sync 	0;
	add.s32 	%r514, %r544, %r113;
	add.s32 	%r516, %r544, %r115;
	add.s32 	%r518, %r544, %r117;
	add.s32 	%r520, %r544, %r119;
	selp.b32 	%r549, 16, 0, %p50;
	selp.b32 	%r550, %r549, 0, %p1;
	selp.b32 	%r523, %r550, 0, %p48;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r514 + 0 ], [ %rd133 + 0 ], 0x10, %r523;
	// end inline asm
	selp.b32 	%r551, %r549, 0, %p2;
	selp.b32 	%r525, %r551, 0, %p48;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r516 + 0 ], [ %rd134 + 0 ], 0x10, %r525;
	// end inline asm
	selp.b32 	%r552, %r549, 0, %p3;
	selp.b32 	%r527, %r552, 0, %p48;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r518 + 0 ], [ %rd135 + 0 ], 0x10, %r527;
	// end inline asm
	selp.b32 	%r553, %r549, 0, %p4;
	selp.b32 	%r529, %r553, 0, %p48;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r520 + 0 ], [ %rd136 + 0 ], 0x10, %r529;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 86 22
	add.s64 	%rd137, %rd170, %rd9;
	add.s64 	%rd138, %rd172, %rd9;
	add.s64 	%rd139, %rd174, %rd9;
	add.s64 	%rd140, %rd176, %rd9;
	add.s32 	%r555, %r87, %r542;
	add.s32 	%r522, %r555, %r113;
	add.s32 	%r524, %r555, %r115;
	add.s32 	%r526, %r555, %r117;
	add.s32 	%r528, %r555, %r119;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r522 + 0 ], [ %rd137 + 0 ], 0x10, %r523;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r524 + 0 ], [ %rd138 + 0 ], 0x10, %r525;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r526 + 0 ], [ %rd139 + 0 ], 0x10, %r527;
	// end inline asm
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r528 + 0 ], [ %rd140 + 0 ], 0x10, %r529;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 88 22
	add.s64 	%rd141, %rd166, %rd9;
	add.s64 	%rd142, %rd167, %rd9;
	or.b64  	%rd145, %rd168, %rd2;
	setp.lt.u64 	%p51, %rd145, 128;
	or.b64  	%rd146, %rd144, %rd2;
	setp.lt.u64 	%p52, %rd146, 128;
	add.s32 	%r557, %r86, %r541;
	add.s32 	%r530, %r557, %r113;
	add.s32 	%r532, %r557, %r115;
	selp.b32 	%r558, 16, 0, %p51;
	selp.b32 	%r531, %r558, 0, %p48;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r530 + 0 ], [ %rd141 + 0 ], 0x10, %r531;
	// end inline asm
	selp.b32 	%r559, 16, 0, %p52;
	selp.b32 	%r533, %r559, 0, %p48;
	// begin inline asm
	@%p5 cp.async.cg.shared.global [ %r532 + 0 ], [ %rd142 + 0 ], 0x10, %r533;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 79 21
	add.s32 	%r560, %r950, 1;
	setp.lt.s32 	%p53, %r560, 2;
	selp.b32 	%r950, %r560, 0, %p53;
	.loc	1 84 22
	shl.b32 	%r561, %r950, 11;
	shl.b32 	%r562, %r950, 12;
	add.s32 	%r949, %r88, %r562;
	.loc	1 86 22
	add.s32 	%r948, %r87, %r562;
	.loc	1 84 22
	// begin inline asm
	cp.async.wait_group 0x3;
	// end inline asm
	bar.sync 	0;
	.loc	1 88 22
	add.s32 	%r947, %r86, %r561;
	.loc	1 79 21
	add.s64 	%rd176, %rd176, 64;
	add.s64 	%rd175, %rd175, 64;
	add.s64 	%rd174, %rd174, 64;
	add.s64 	%rd173, %rd173, 64;
	add.s64 	%rd172, %rd172, 64;
	add.s64 	%rd171, %rd171, 64;
	add.s64 	%rd170, %rd170, 64;
	add.s64 	%rd169, %rd169, 64;
	add.s64 	%rd168, %rd168, 32;
	add.s64 	%rd167, %rd167, 8192;
	add.s64 	%rd166, %rd166, 8192;
	setp.lt.u32 	%p54, %r946, 3;
	@%p54 bra 	$L__BB0_1;
	.loc	1 73 9
	shl.b64 	%rd155, %rd1, 1;
	add.s64 	%rd156, %rd44, %rd155;
	.loc	1 72 9
	add.s64 	%rd157, %rd43, %rd155;
	.loc	1 79 21
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 108 34
	shr.u32 	%r827, %r7, 1;
	or.b32  	%r828, %r6, %r827;
	or.b32  	%r829, %r828, 8;
	or.b32  	%r830, %r8, %r827;
	or.b32  	%r831, %r828, 40;
	shl.b32 	%r832, %r3, 1;
	and.b32  	%r833, %r832, 6;
	or.b32  	%r834, %r833, 8;
	or.b32  	%r835, %r833, 9;
	or.b32  	%r836, %r833, 16;
	or.b32  	%r837, %r833, 17;
	or.b32  	%r838, %r833, 24;
	or.b32  	%r839, %r833, 25;
	or.b32  	%r840, %r833, 32;
	or.b32  	%r841, %r833, 33;
	or.b32  	%r842, %r833, 40;
	or.b32  	%r843, %r833, 41;
	or.b32  	%r844, %r833, 48;
	or.b32  	%r845, %r833, 49;
	or.b32  	%r846, %r833, 56;
	or.b32  	%r847, %r833, 57;
	.loc	1 108 21
	or.b32  	%r848, %r2, %r828;
	or.b32  	%r849, %r2, %r829;
	or.b32  	%r850, %r2, %r830;
	or.b32  	%r851, %r2, %r831;
	or.b32  	%r852, %r2, %r833;
	or.b32  	%r853, %r852, 1;
	or.b32  	%r854, %r2, %r834;
	or.b32  	%r855, %r2, %r835;
	or.b32  	%r856, %r2, %r836;
	or.b32  	%r857, %r2, %r837;
	or.b32  	%r858, %r2, %r838;
	or.b32  	%r859, %r2, %r839;
	or.b32  	%r860, %r2, %r840;
	or.b32  	%r861, %r2, %r841;
	or.b32  	%r862, %r2, %r842;
	or.b32  	%r863, %r2, %r843;
	or.b32  	%r864, %r2, %r844;
	or.b32  	%r865, %r2, %r845;
	or.b32  	%r866, %r2, %r846;
	or.b32  	%r867, %r2, %r847;
	.loc	1 109 16
	setp.lt.s32 	%p64, %r848, %r1;
	setp.lt.s32 	%p65, %r849, %r1;
	setp.lt.s32 	%p66, %r850, %r1;
	setp.lt.s32 	%p67, %r851, %r1;
	setp.lt.s32 	%p68, %r852, %r1;
	setp.lt.s32 	%p69, %r853, %r1;
	setp.lt.s32 	%p70, %r854, %r1;
	setp.lt.s32 	%p71, %r855, %r1;
	setp.lt.s32 	%p72, %r856, %r1;
	setp.lt.s32 	%p73, %r857, %r1;
	setp.lt.s32 	%p74, %r858, %r1;
	setp.lt.s32 	%p75, %r859, %r1;
	setp.lt.s32 	%p76, %r860, %r1;
	setp.lt.s32 	%p77, %r861, %r1;
	setp.lt.s32 	%p78, %r862, %r1;
	setp.lt.s32 	%p79, %r863, %r1;
	setp.lt.s32 	%p80, %r864, %r1;
	setp.lt.s32 	%p81, %r865, %r1;
	setp.lt.s32 	%p82, %r866, %r1;
	setp.lt.s32 	%p83, %r867, %r1;
	.loc	1 110 27
	setp.ge.u32 	%p84, %r828, %r833;
	setp.gt.u32 	%p85, %r828, %r833;
	setp.ge.u32 	%p86, %r828, %r834;
	setp.ge.u32 	%p87, %r828, %r835;
	setp.ge.u32 	%p88, %r829, %r835;
	setp.ge.u32 	%p89, %r828, %r836;
	setp.ge.u32 	%p90, %r828, %r837;
	setp.ge.u32 	%p91, %r829, %r836;
	setp.ge.u32 	%p92, %r829, %r837;
	setp.ge.u32 	%p93, %r829, %r838;
	setp.ge.u32 	%p94, %r829, %r839;
	setp.ge.u32 	%p95, %r830, %r840;
	setp.ge.u32 	%p96, %r830, %r841;
	setp.ge.u32 	%p97, %r830, %r842;
	setp.ge.u32 	%p98, %r830, %r843;
	setp.ge.u32 	%p99, %r831, %r843;
	setp.ge.u32 	%p100, %r830, %r844;
	setp.ge.u32 	%p101, %r830, %r845;
	setp.ge.u32 	%p102, %r831, %r844;
	setp.ge.u32 	%p103, %r831, %r845;
	setp.ge.u32 	%p104, %r831, %r846;
	setp.ge.u32 	%p105, %r831, %r847;
	.loc	1 119 38
	cvt.rn.f16.f32 	%rs1, %f899;
	cvt.rn.f16.f32 	%rs2, %f900;
	cvt.rn.f16.f32 	%rs3, %f901;
	cvt.rn.f16.f32 	%rs4, %f902;
	cvt.rn.f16.f32 	%rs5, %f903;
	cvt.rn.f16.f32 	%rs6, %f904;
	cvt.rn.f16.f32 	%rs7, %f905;
	cvt.rn.f16.f32 	%rs8, %f906;
	cvt.rn.f16.f32 	%rs9, %f907;
	cvt.rn.f16.f32 	%rs10, %f908;
	cvt.rn.f16.f32 	%rs11, %f909;
	cvt.rn.f16.f32 	%rs12, %f910;
	cvt.rn.f16.f32 	%rs13, %f913;
	cvt.rn.f16.f32 	%rs14, %f914;
	cvt.rn.f16.f32 	%rs15, %f931;
	cvt.rn.f16.f32 	%rs16, %f932;
	cvt.rn.f16.f32 	%rs17, %f933;
	cvt.rn.f16.f32 	%rs18, %f934;
	cvt.rn.f16.f32 	%rs19, %f935;
	cvt.rn.f16.f32 	%rs20, %f936;
	cvt.rn.f16.f32 	%rs21, %f937;
	cvt.rn.f16.f32 	%rs22, %f938;
	cvt.rn.f16.f32 	%rs23, %f939;
	cvt.rn.f16.f32 	%rs24, %f940;
	cvt.rn.f16.f32 	%rs25, %f941;
	cvt.rn.f16.f32 	%rs26, %f942;
	cvt.rn.f16.f32 	%rs27, %f943;
	cvt.rn.f16.f32 	%rs28, %f944;
	cvt.rn.f16.f32 	%rs29, %f945;
	cvt.rn.f16.f32 	%rs30, %f946;
	cvt.rn.f16.f32 	%rs31, %f947;
	cvt.rn.f16.f32 	%rs32, %f948;
	cvt.rn.f16.f32 	%rs33, %f949;
	cvt.rn.f16.f32 	%rs34, %f950;
	cvt.rn.f16.f32 	%rs35, %f951;
	cvt.rn.f16.f32 	%rs36, %f952;
	cvt.rn.f16.f32 	%rs37, %f953;
	cvt.rn.f16.f32 	%rs38, %f954;
	cvt.rn.f16.f32 	%rs39, %f955;
	cvt.rn.f16.f32 	%rs40, %f956;
	cvt.rn.f16.f32 	%rs41, %f957;
	cvt.rn.f16.f32 	%rs42, %f958;
	cvt.rn.f16.f32 	%rs43, %f961;
	cvt.rn.f16.f32 	%rs44, %f962;
	.loc	1 116 18
	add.s64 	%rd158, %rd3, %rd8;
	add.s64 	%rd159, %rd4, %rd8;
	add.s64 	%rd160, %rd5, %rd8;
	add.s64 	%rd161, %rd6, %rd8;
	shl.b64 	%rd162, %rd158, 1;
	add.s64 	%rd147, %rd157, %rd162;
	shl.b64 	%rd163, %rd159, 1;
	add.s64 	%rd148, %rd157, %rd163;
	shl.b64 	%rd164, %rd160, 1;
	add.s64 	%rd149, %rd157, %rd164;
	shl.b64 	%rd165, %rd161, 1;
	add.s64 	%rd150, %rd157, %rd165;
	and.pred  	%p55, %p37, %p1;
	and.pred  	%p56, %p37, %p2;
	and.pred  	%p57, %p37, %p3;
	and.pred  	%p58, %p37, %p4;
	// begin inline asm
	mov.u32 %r563, 0x0;
	mov.u32 %r564, 0x0;
	mov.u32 %r565, 0x0;
	mov.u32 %r566, 0x0;
	@%p55 ld.global.v4.b32 { %r563, %r564, %r565, %r566 }, [ %rd147 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r567, 0x0;
	mov.u32 %r568, 0x0;
	mov.u32 %r569, 0x0;
	mov.u32 %r570, 0x0;
	@%p56 ld.global.v4.b32 { %r567, %r568, %r569, %r570 }, [ %rd148 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r571, 0x0;
	mov.u32 %r572, 0x0;
	mov.u32 %r573, 0x0;
	mov.u32 %r574, 0x0;
	@%p57 ld.global.v4.b32 { %r571, %r572, %r573, %r574 }, [ %rd149 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r575, 0x0;
	mov.u32 %r576, 0x0;
	mov.u32 %r577, 0x0;
	mov.u32 %r578, 0x0;
	@%p58 ld.global.v4.b32 { %r575, %r576, %r577, %r578 }, [ %rd150 + 0 ];
	// end inline asm
	st.shared.v4.b32 	[%r10], {%r563, %r564, %r565, %r566};
	st.shared.v4.b32 	[%r12], {%r567, %r568, %r569, %r570};
	st.shared.v4.b32 	[%r14], {%r571, %r572, %r573, %r574};
	st.shared.v4.b32 	[%r16], {%r575, %r576, %r577, %r578};
	.loc	1 119 38
	selp.b16 	%rs45, %rs1, 0x0000, %p68;
	selp.b16 	%rs46, %rs45, 0x0000, %p84;
	selp.b16 	%rs47, %rs46, 0x0000, %p64;
	selp.b16 	%rs48, %rs2, 0x0000, %p69;
	selp.b16 	%rs49, %rs48, 0x0000, %p85;
	selp.b16 	%rs50, %rs49, 0x0000, %p64;
	selp.b16 	%rs51, %rs3, 0x0000, %p68;
	selp.b16 	%rs52, %rs51, 0x0000, %p65;
	selp.b16 	%rs53, %rs4, 0x0000, %p69;
	selp.b16 	%rs54, %rs53, 0x0000, %p65;
	selp.b16 	%rs55, %rs5, 0x0000, %p70;
	selp.b16 	%rs56, %rs55, 0x0000, %p86;
	selp.b16 	%rs57, %rs56, 0x0000, %p64;
	selp.b16 	%rs58, %rs6, 0x0000, %p71;
	selp.b16 	%rs59, %rs58, 0x0000, %p87;
	selp.b16 	%rs60, %rs59, 0x0000, %p64;
	selp.b16 	%rs61, %rs7, 0x0000, %p70;
	selp.b16 	%rs62, %rs61, 0x0000, %p84;
	selp.b16 	%rs63, %rs62, 0x0000, %p65;
	selp.b16 	%rs64, %rs8, 0x0000, %p71;
	selp.b16 	%rs65, %rs64, 0x0000, %p88;
	selp.b16 	%rs66, %rs65, 0x0000, %p65;
	selp.b16 	%rs67, %rs9, 0x0000, %p72;
	selp.b16 	%rs68, %rs67, 0x0000, %p89;
	selp.b16 	%rs69, %rs68, 0x0000, %p64;
	selp.b16 	%rs70, %rs10, 0x0000, %p73;
	selp.b16 	%rs71, %rs70, 0x0000, %p90;
	selp.b16 	%rs72, %rs71, 0x0000, %p64;
	selp.b16 	%rs73, %rs11, 0x0000, %p72;
	selp.b16 	%rs74, %rs73, 0x0000, %p91;
	selp.b16 	%rs75, %rs74, 0x0000, %p65;
	selp.b16 	%rs76, %rs12, 0x0000, %p73;
	selp.b16 	%rs77, %rs76, 0x0000, %p92;
	selp.b16 	%rs78, %rs77, 0x0000, %p65;
	selp.b16 	%rs79, %rs13, 0x0000, %p74;
	selp.b16 	%rs80, %rs79, 0x0000, %p93;
	selp.b16 	%rs81, %rs80, 0x0000, %p65;
	selp.b16 	%rs82, %rs14, 0x0000, %p75;
	selp.b16 	%rs83, %rs82, 0x0000, %p94;
	selp.b16 	%rs84, %rs83, 0x0000, %p65;
	selp.b16 	%rs85, %rs15, 0x0000, %p68;
	selp.b16 	%rs86, %rs85, 0x0000, %p66;
	selp.b16 	%rs87, %rs16, 0x0000, %p69;
	selp.b16 	%rs88, %rs87, 0x0000, %p66;
	selp.b16 	%rs89, %rs17, 0x0000, %p68;
	selp.b16 	%rs90, %rs89, 0x0000, %p67;
	selp.b16 	%rs91, %rs18, 0x0000, %p69;
	selp.b16 	%rs92, %rs91, 0x0000, %p67;
	selp.b16 	%rs93, %rs19, 0x0000, %p70;
	selp.b16 	%rs94, %rs93, 0x0000, %p66;
	selp.b16 	%rs95, %rs20, 0x0000, %p71;
	selp.b16 	%rs96, %rs95, 0x0000, %p66;
	selp.b16 	%rs97, %rs21, 0x0000, %p70;
	selp.b16 	%rs98, %rs97, 0x0000, %p67;
	selp.b16 	%rs99, %rs22, 0x0000, %p71;
	selp.b16 	%rs100, %rs99, 0x0000, %p67;
	selp.b16 	%rs101, %rs23, 0x0000, %p72;
	selp.b16 	%rs102, %rs101, 0x0000, %p66;
	selp.b16 	%rs103, %rs24, 0x0000, %p73;
	selp.b16 	%rs104, %rs103, 0x0000, %p66;
	selp.b16 	%rs105, %rs25, 0x0000, %p72;
	selp.b16 	%rs106, %rs105, 0x0000, %p67;
	selp.b16 	%rs107, %rs26, 0x0000, %p73;
	selp.b16 	%rs108, %rs107, 0x0000, %p67;
	selp.b16 	%rs109, %rs27, 0x0000, %p74;
	selp.b16 	%rs110, %rs109, 0x0000, %p66;
	selp.b16 	%rs111, %rs28, 0x0000, %p75;
	selp.b16 	%rs112, %rs111, 0x0000, %p66;
	selp.b16 	%rs113, %rs29, 0x0000, %p74;
	selp.b16 	%rs114, %rs113, 0x0000, %p67;
	selp.b16 	%rs115, %rs30, 0x0000, %p75;
	selp.b16 	%rs116, %rs115, 0x0000, %p67;
	selp.b16 	%rs117, %rs31, 0x0000, %p76;
	selp.b16 	%rs118, %rs117, 0x0000, %p95;
	selp.b16 	%rs119, %rs118, 0x0000, %p66;
	selp.b16 	%rs120, %rs32, 0x0000, %p77;
	selp.b16 	%rs121, %rs120, 0x0000, %p96;
	selp.b16 	%rs122, %rs121, 0x0000, %p66;
	selp.b16 	%rs123, %rs33, 0x0000, %p76;
	selp.b16 	%rs124, %rs123, 0x0000, %p67;
	selp.b16 	%rs125, %rs34, 0x0000, %p77;
	selp.b16 	%rs126, %rs125, 0x0000, %p67;
	selp.b16 	%rs127, %rs35, 0x0000, %p78;
	selp.b16 	%rs128, %rs127, 0x0000, %p97;
	selp.b16 	%rs129, %rs128, 0x0000, %p66;
	selp.b16 	%rs130, %rs36, 0x0000, %p79;
	selp.b16 	%rs131, %rs130, 0x0000, %p98;
	selp.b16 	%rs132, %rs131, 0x0000, %p66;
	selp.b16 	%rs133, %rs37, 0x0000, %p78;
	selp.b16 	%rs134, %rs133, 0x0000, %p84;
	selp.b16 	%rs135, %rs134, 0x0000, %p67;
	selp.b16 	%rs136, %rs38, 0x0000, %p79;
	selp.b16 	%rs137, %rs136, 0x0000, %p99;
	selp.b16 	%rs138, %rs137, 0x0000, %p67;
	selp.b16 	%rs139, %rs39, 0x0000, %p80;
	selp.b16 	%rs140, %rs139, 0x0000, %p100;
	selp.b16 	%rs141, %rs140, 0x0000, %p66;
	selp.b16 	%rs142, %rs40, 0x0000, %p81;
	selp.b16 	%rs143, %rs142, 0x0000, %p101;
	selp.b16 	%rs144, %rs143, 0x0000, %p66;
	selp.b16 	%rs145, %rs41, 0x0000, %p80;
	selp.b16 	%rs146, %rs145, 0x0000, %p102;
	selp.b16 	%rs147, %rs146, 0x0000, %p67;
	selp.b16 	%rs148, %rs42, 0x0000, %p81;
	selp.b16 	%rs149, %rs148, 0x0000, %p103;
	selp.b16 	%rs150, %rs149, 0x0000, %p67;
	selp.b16 	%rs151, %rs43, 0x0000, %p82;
	selp.b16 	%rs152, %rs151, 0x0000, %p104;
	selp.b16 	%rs153, %rs152, 0x0000, %p67;
	selp.b16 	%rs154, %rs44, 0x0000, %p83;
	selp.b16 	%rs155, %rs154, 0x0000, %p105;
	selp.b16 	%rs156, %rs155, 0x0000, %p67;
	mov.b32 	%r619, {%rs47, %rs50};
	mov.b32 	%r620, {%rs52, %rs54};
	mov.b32 	%r621, {%rs57, %rs60};
	mov.b32 	%r622, {%rs63, %rs66};
	mov.b32 	%r667, {%rs69, %rs72};
	mov.b32 	%r668, {%rs75, %rs78};
	mov.b32 	%r670, {%rs81, %rs84};
	mov.b32 	%r643, {%rs86, %rs88};
	mov.b32 	%r644, {%rs90, %rs92};
	mov.b32 	%r645, {%rs94, %rs96};
	mov.b32 	%r646, {%rs98, %rs100};
	mov.b32 	%r691, {%rs102, %rs104};
	mov.b32 	%r692, {%rs106, %rs108};
	mov.b32 	%r693, {%rs110, %rs112};
	mov.b32 	%r694, {%rs114, %rs116};
	mov.b32 	%r739, {%rs119, %rs122};
	mov.b32 	%r740, {%rs124, %rs126};
	mov.b32 	%r741, {%rs129, %rs132};
	mov.b32 	%r742, {%rs135, %rs138};
	mov.b32 	%r787, {%rs141, %rs144};
	mov.b32 	%r788, {%rs147, %rs150};
	mov.b32 	%r790, {%rs153, %rs156};
	.loc	1 116 18
	bar.sync 	0;
	shl.b32 	%r908, %r130, 4;
	shl.b32 	%r909, %r21, 1;
	or.b32  	%r910, %r908, %r909;
	mov.u32 	%r911, global_smem;
	add.s32 	%r583, %r911, %r910;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r623, %r624, %r629, %r630 }, [ %r583 + 0 ];
	// end inline asm
	add.s32 	%r588, %r583, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r671, %r672, %r677, %r678 }, [ %r588 + 0 ];
	// end inline asm
	add.s32 	%r593, %r583, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r719, %r720, %r725, %r726 }, [ %r593 + 0 ];
	// end inline asm
	add.s32 	%r598, %r583, 3072;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r767, %r768, %r773, %r774 }, [ %r598 + 0 ];
	// end inline asm
	shl.b32 	%r913, %r135, 4;
	or.b32  	%r914, %r913, %r909;
	add.s32 	%r603, %r911, %r914;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r635, %r636, %r641, %r642 }, [ %r603 + 0 ];
	// end inline asm
	add.s32 	%r608, %r603, 1024;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r683, %r684, %r689, %r690 }, [ %r608 + 0 ];
	// end inline asm
	add.s32 	%r613, %r603, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r731, %r732, %r737, %r738 }, [ %r613 + 0 ];
	// end inline asm
	add.s32 	%r618, %r603, 3072;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r779, %r780, %r785, %r786 }, [ %r618 + 0 ];
	// end inline asm
	mov.f32 	%f699, 0f00000000;
	.loc	1 119 50
	mov.f32 	%f643, %f699;
	mov.f32 	%f644, %f699;
	mov.f32 	%f645, %f699;
	mov.f32 	%f646, %f699;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f643, %f644, %f645, %f646 }, { %r619, %r620, %r621, %r622 }, { %r623, %r624 }, { %f643, %f644, %f645, %f646 };
	// end inline asm
	mov.f32 	%f651, %f699;
	mov.f32 	%f652, %f699;
	mov.f32 	%f653, %f699;
	mov.f32 	%f654, %f699;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f651, %f652, %f653, %f654 }, { %r619, %r620, %r621, %r622 }, { %r629, %r630 }, { %f651, %f652, %f653, %f654 };
	// end inline asm
	mov.f32 	%f659, %f699;
	mov.f32 	%f660, %f699;
	mov.f32 	%f661, %f699;
	mov.f32 	%f662, %f699;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f659, %f660, %f661, %f662 }, { %r619, %r620, %r621, %r622 }, { %r635, %r636 }, { %f659, %f660, %f661, %f662 };
	// end inline asm
	mov.f32 	%f667, %f699;
	mov.f32 	%f668, %f699;
	mov.f32 	%f669, %f699;
	mov.f32 	%f670, %f699;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f667, %f668, %f669, %f670 }, { %r619, %r620, %r621, %r622 }, { %r641, %r642 }, { %f667, %f668, %f669, %f670 };
	// end inline asm
	mov.f32 	%f675, %f699;
	mov.f32 	%f676, %f699;
	mov.f32 	%f677, %f699;
	mov.f32 	%f678, %f699;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f675, %f676, %f677, %f678 }, { %r643, %r644, %r645, %r646 }, { %r623, %r624 }, { %f675, %f676, %f677, %f678 };
	// end inline asm
	mov.f32 	%f683, %f699;
	mov.f32 	%f684, %f699;
	mov.f32 	%f685, %f699;
	mov.f32 	%f686, %f699;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f683, %f684, %f685, %f686 }, { %r643, %r644, %r645, %r646 }, { %r629, %r630 }, { %f683, %f684, %f685, %f686 };
	// end inline asm
	mov.f32 	%f691, %f699;
	mov.f32 	%f692, %f699;
	mov.f32 	%f693, %f699;
	mov.f32 	%f694, %f699;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f691, %f692, %f693, %f694 }, { %r643, %r644, %r645, %r646 }, { %r635, %r636 }, { %f691, %f692, %f693, %f694 };
	// end inline asm
	mov.f32 	%f700, %f699;
	mov.f32 	%f701, %f699;
	mov.f32 	%f702, %f699;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f699, %f700, %f701, %f702 }, { %r643, %r644, %r645, %r646 }, { %r641, %r642 }, { %f699, %f700, %f701, %f702 };
	// end inline asm
	mov.b32 	%r669, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f643, %f644, %f645, %f646 }, { %r667, %r668, %r669, %r670 }, { %r671, %r672 }, { %f643, %f644, %f645, %f646 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f651, %f652, %f653, %f654 }, { %r667, %r668, %r669, %r670 }, { %r677, %r678 }, { %f651, %f652, %f653, %f654 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f659, %f660, %f661, %f662 }, { %r667, %r668, %r669, %r670 }, { %r683, %r684 }, { %f659, %f660, %f661, %f662 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f667, %f668, %f669, %f670 }, { %r667, %r668, %r669, %r670 }, { %r689, %r690 }, { %f667, %f668, %f669, %f670 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f675, %f676, %f677, %f678 }, { %r691, %r692, %r693, %r694 }, { %r671, %r672 }, { %f675, %f676, %f677, %f678 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f683, %f684, %f685, %f686 }, { %r691, %r692, %r693, %r694 }, { %r677, %r678 }, { %f683, %f684, %f685, %f686 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f691, %f692, %f693, %f694 }, { %r691, %r692, %r693, %r694 }, { %r683, %r684 }, { %f691, %f692, %f693, %f694 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f699, %f700, %f701, %f702 }, { %r691, %r692, %r693, %r694 }, { %r689, %r690 }, { %f699, %f700, %f701, %f702 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f643, %f644, %f645, %f646 }, { %r669, %r669, %r669, %r669 }, { %r719, %r720 }, { %f643, %f644, %f645, %f646 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f651, %f652, %f653, %f654 }, { %r669, %r669, %r669, %r669 }, { %r725, %r726 }, { %f651, %f652, %f653, %f654 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f659, %f660, %f661, %f662 }, { %r669, %r669, %r669, %r669 }, { %r731, %r732 }, { %f659, %f660, %f661, %f662 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f667, %f668, %f669, %f670 }, { %r669, %r669, %r669, %r669 }, { %r737, %r738 }, { %f667, %f668, %f669, %f670 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f675, %f676, %f677, %f678 }, { %r739, %r740, %r741, %r742 }, { %r719, %r720 }, { %f675, %f676, %f677, %f678 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f683, %f684, %f685, %f686 }, { %r739, %r740, %r741, %r742 }, { %r725, %r726 }, { %f683, %f684, %f685, %f686 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f691, %f692, %f693, %f694 }, { %r739, %r740, %r741, %r742 }, { %r731, %r732 }, { %f691, %f692, %f693, %f694 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f699, %f700, %f701, %f702 }, { %r739, %r740, %r741, %r742 }, { %r737, %r738 }, { %f699, %f700, %f701, %f702 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f643, %f644, %f645, %f646 }, { %r669, %r669, %r669, %r669 }, { %r767, %r768 }, { %f643, %f644, %f645, %f646 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f651, %f652, %f653, %f654 }, { %r669, %r669, %r669, %r669 }, { %r773, %r774 }, { %f651, %f652, %f653, %f654 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f659, %f660, %f661, %f662 }, { %r669, %r669, %r669, %r669 }, { %r779, %r780 }, { %f659, %f660, %f661, %f662 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f667, %f668, %f669, %f670 }, { %r669, %r669, %r669, %r669 }, { %r785, %r786 }, { %f667, %f668, %f669, %f670 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f675, %f676, %f677, %f678 }, { %r787, %r788, %r669, %r790 }, { %r767, %r768 }, { %f675, %f676, %f677, %f678 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f683, %f684, %f685, %f686 }, { %r787, %r788, %r669, %r790 }, { %r773, %r774 }, { %f683, %f684, %f685, %f686 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f691, %f692, %f693, %f694 }, { %r787, %r788, %r669, %r790 }, { %r779, %r780 }, { %f691, %f692, %f693, %f694 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f699, %f700, %f701, %f702 }, { %r787, %r788, %r669, %r790 }, { %r785, %r786 }, { %f699, %f700, %f701, %f702 };
	// end inline asm
	.loc	1 119 57
	mul.f32 	%f835, %f644, %f193;
	mul.f32 	%f836, %f643, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f837, %f963, %f193, %f836;
	fma.rn.f32 	%f838, %f964, %f193, %f835;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs157, %f838;
	cvt.rn.f16.f32 	%rs158, %f837;
	mov.b32 	%r915, {%rs158, %rs157};
	.loc	1 119 57
	mul.f32 	%f839, %f646, %f193;
	mul.f32 	%f840, %f645, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f841, %f965, %f193, %f840;
	fma.rn.f32 	%f842, %f966, %f193, %f839;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs159, %f842;
	cvt.rn.f16.f32 	%rs160, %f841;
	mov.b32 	%r916, {%rs160, %rs159};
	.loc	1 119 57
	mul.f32 	%f843, %f652, %f193;
	mul.f32 	%f844, %f651, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f845, %f967, %f193, %f844;
	fma.rn.f32 	%f846, %f968, %f193, %f843;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs161, %f846;
	cvt.rn.f16.f32 	%rs162, %f845;
	mov.b32 	%r917, {%rs162, %rs161};
	.loc	1 119 57
	mul.f32 	%f847, %f654, %f193;
	mul.f32 	%f848, %f653, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f849, %f969, %f193, %f848;
	fma.rn.f32 	%f850, %f970, %f193, %f847;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs163, %f850;
	cvt.rn.f16.f32 	%rs164, %f849;
	mov.b32 	%r918, {%rs164, %rs163};
	.loc	1 119 57
	mul.f32 	%f851, %f660, %f193;
	mul.f32 	%f852, %f659, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f853, %f971, %f193, %f852;
	fma.rn.f32 	%f854, %f972, %f193, %f851;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs165, %f854;
	cvt.rn.f16.f32 	%rs166, %f853;
	mov.b32 	%r919, {%rs166, %rs165};
	.loc	1 119 57
	mul.f32 	%f855, %f662, %f193;
	mul.f32 	%f856, %f661, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f857, %f973, %f193, %f856;
	fma.rn.f32 	%f858, %f974, %f193, %f855;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs167, %f858;
	cvt.rn.f16.f32 	%rs168, %f857;
	mov.b32 	%r920, {%rs168, %rs167};
	.loc	1 119 57
	mul.f32 	%f859, %f668, %f193;
	mul.f32 	%f860, %f667, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f861, %f975, %f193, %f860;
	fma.rn.f32 	%f862, %f976, %f193, %f859;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs169, %f862;
	cvt.rn.f16.f32 	%rs170, %f861;
	mov.b32 	%r921, {%rs170, %rs169};
	.loc	1 119 57
	mul.f32 	%f863, %f670, %f193;
	mul.f32 	%f864, %f669, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f865, %f977, %f193, %f864;
	fma.rn.f32 	%f866, %f978, %f193, %f863;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs171, %f866;
	cvt.rn.f16.f32 	%rs172, %f865;
	mov.b32 	%r922, {%rs172, %rs171};
	.loc	1 119 57
	mul.f32 	%f867, %f676, %f193;
	mul.f32 	%f868, %f675, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f869, %f979, %f193, %f868;
	fma.rn.f32 	%f870, %f980, %f193, %f867;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs173, %f870;
	cvt.rn.f16.f32 	%rs174, %f869;
	mov.b32 	%r923, {%rs174, %rs173};
	.loc	1 119 57
	mul.f32 	%f871, %f678, %f193;
	mul.f32 	%f872, %f677, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f873, %f981, %f193, %f872;
	fma.rn.f32 	%f874, %f982, %f193, %f871;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs175, %f874;
	cvt.rn.f16.f32 	%rs176, %f873;
	mov.b32 	%r924, {%rs176, %rs175};
	.loc	1 119 57
	mul.f32 	%f875, %f684, %f193;
	mul.f32 	%f876, %f683, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f877, %f983, %f193, %f876;
	fma.rn.f32 	%f878, %f984, %f193, %f875;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs177, %f878;
	cvt.rn.f16.f32 	%rs178, %f877;
	mov.b32 	%r925, {%rs178, %rs177};
	.loc	1 119 57
	mul.f32 	%f879, %f686, %f193;
	mul.f32 	%f880, %f685, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f881, %f985, %f193, %f880;
	fma.rn.f32 	%f882, %f986, %f193, %f879;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs179, %f882;
	cvt.rn.f16.f32 	%rs180, %f881;
	mov.b32 	%r926, {%rs180, %rs179};
	.loc	1 119 57
	mul.f32 	%f883, %f692, %f193;
	mul.f32 	%f884, %f691, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f885, %f987, %f193, %f884;
	fma.rn.f32 	%f886, %f988, %f193, %f883;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs181, %f886;
	cvt.rn.f16.f32 	%rs182, %f885;
	mov.b32 	%r927, {%rs182, %rs181};
	.loc	1 119 57
	mul.f32 	%f887, %f694, %f193;
	mul.f32 	%f888, %f693, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f889, %f989, %f193, %f888;
	fma.rn.f32 	%f890, %f990, %f193, %f887;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs183, %f890;
	cvt.rn.f16.f32 	%rs184, %f889;
	mov.b32 	%r928, {%rs184, %rs183};
	.loc	1 119 57
	mul.f32 	%f891, %f700, %f193;
	mul.f32 	%f892, %f699, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f893, %f991, %f193, %f892;
	fma.rn.f32 	%f894, %f992, %f193, %f891;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs185, %f894;
	cvt.rn.f16.f32 	%rs186, %f893;
	mov.b32 	%r929, {%rs186, %rs185};
	.loc	1 119 57
	mul.f32 	%f895, %f702, %f193;
	mul.f32 	%f896, %f701, %f193;
	.loc	1 119 24
	fma.rn.f32 	%f897, %f993, %f193, %f896;
	fma.rn.f32 	%f898, %f994, %f193, %f895;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs187, %f898;
	cvt.rn.f16.f32 	%rs188, %f897;
	mov.b32 	%r930, {%rs188, %rs187};
	.loc	1 120 18
	add.s64 	%rd151, %rd156, %rd162;
	add.s64 	%rd152, %rd156, %rd163;
	add.s64 	%rd153, %rd156, %rd164;
	add.s64 	%rd154, %rd156, %rd165;
	bar.sync 	0;
	and.b32  	%r931, %r5, 1;
	shr.u32 	%r932, %r4, 2;
	shl.b32 	%r934, %r931, 4;
	or.b32  	%r935, %r934, %r932;
	mul.lo.s32 	%r936, %r935, 80;
	shl.b32 	%r937, %r144, 2;
	or.b32  	%r938, %r937, %r936;
	add.s32 	%r939, %r911, %r938;
	st.shared.b32 	[%r939], %r915;
	st.shared.b32 	[%r939+640], %r916;
	st.shared.b32 	[%r939+16], %r917;
	st.shared.b32 	[%r939+656], %r918;
	st.shared.b32 	[%r939+32], %r919;
	st.shared.b32 	[%r939+672], %r920;
	st.shared.b32 	[%r939+48], %r921;
	st.shared.b32 	[%r939+688], %r922;
	bar.sync 	0;
	shl.b32 	%r940, %r931, 3;
	or.b32  	%r941, %r940, %r932;
	shl.b32 	%r942, %r144, 3;
	mad.lo.s32 	%r943, %r941, 40, %r942;
	shl.b32 	%r944, %r943, 1;
	add.s32 	%r945, %r911, %r944;
	ld.shared.v4.u32 	{%r811, %r812, %r813, %r814}, [%r945];
	ld.shared.v4.u32 	{%r815, %r816, %r817, %r818}, [%r945+1280];
	bar.sync 	0;
	st.shared.b32 	[%r939], %r923;
	st.shared.b32 	[%r939+640], %r924;
	st.shared.b32 	[%r939+16], %r925;
	st.shared.b32 	[%r939+656], %r926;
	st.shared.b32 	[%r939+32], %r927;
	st.shared.b32 	[%r939+672], %r928;
	st.shared.b32 	[%r939+48], %r929;
	st.shared.b32 	[%r939+688], %r930;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r819, %r820, %r821, %r822}, [%r945];
	ld.shared.v4.u32 	{%r823, %r824, %r825, %r826}, [%r945+1280];
	// begin inline asm
	@%p55 st.global.v4.b32 [ %rd151 + 0 ], { %r811, %r812, %r813, %r814 };
	// end inline asm
	// begin inline asm
	@%p56 st.global.v4.b32 [ %rd152 + 0 ], { %r815, %r816, %r817, %r818 };
	// end inline asm
	// begin inline asm
	@%p57 st.global.v4.b32 [ %rd153 + 0 ], { %r819, %r820, %r821, %r822 };
	// end inline asm
	// begin inline asm
	@%p58 st.global.v4.b32 [ %rd154 + 0 ], { %r823, %r824, %r825, %r826 };
	// end inline asm
	.loc	1 120 4
	ret;
$L__tmp1:
$L__func_end0:

}
	.file	1 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\fla\\ops\\common\\chunk_o.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 0
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 151
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 104
.b8 117
.b8 110
.b8 107
.b8 95
.b8 111
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 68
.b8 58
.b8 92
.b8 85
.b8 115
.b8 101
.b8 114
.b8 115
.b8 92
.b8 76
.b8 111
.b8 117
.b8 105
.b8 115
.b8 92
.b8 80
.b8 121
.b8 99
.b8 104
.b8 97
.b8 114
.b8 109
.b8 80
.b8 114
.b8 111
.b8 106
.b8 101
.b8 99
.b8 116
.b8 115
.b8 92
.b8 77
.b8 97
.b8 115
.b8 116
.b8 101
.b8 114
.b8 95
.b8 116
.b8 104
.b8 101
.b8 115
.b8 105
.b8 115
.b8 92
.b8 66
.b8 97
.b8 98
.b8 105
.b8 108
.b8 111
.b8 110
.b8 103
.b8 95
.b8 66
.b8 101
.b8 110
.b8 99
.b8 104
.b8 109
.b8 97
.b8 114
.b8 107
.b8 92
.b8 46
.b8 118
.b8 101
.b8 110
.b8 118
.b8 92
.b8 76
.b8 105
.b8 98
.b8 92
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 92
.b8 102
.b8 108
.b8 97
.b8 92
.b8 111
.b8 112
.b8 115
.b8 92
.b8 99
.b8 111
.b8 109
.b8 109
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
	}
	.section	.debug_loc	{	}
