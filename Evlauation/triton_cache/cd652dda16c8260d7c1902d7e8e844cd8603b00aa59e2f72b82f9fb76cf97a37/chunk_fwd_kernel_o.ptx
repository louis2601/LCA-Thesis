//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_86
.address_size 64

	// .globl	chunk_fwd_kernel_o
.extern .shared .align 16 .b8 global_smem[];

.visible .entry chunk_fwd_kernel_o(
	.param .u64 chunk_fwd_kernel_o_param_0,
	.param .u64 chunk_fwd_kernel_o_param_1,
	.param .u64 chunk_fwd_kernel_o_param_2,
	.param .u64 chunk_fwd_kernel_o_param_3,
	.param .u64 chunk_fwd_kernel_o_param_4,
	.param .u64 chunk_fwd_kernel_o_param_5,
	.param .u64 chunk_fwd_kernel_o_param_6,
	.param .f32 chunk_fwd_kernel_o_param_7,
	.param .u32 chunk_fwd_kernel_o_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<114>;
	.reg .b16 	%rs<151>;
	.reg .b32 	%r<1307>;
	.reg .f32 	%f<1059>;
	.reg .b64 	%rd<129>;
	.loc	1 33 0
$L__func_begin0:
	.loc	1 33 0

	ld.param.f32 	%f129, [chunk_fwd_kernel_o_param_7];
	ld.param.u64 	%rd16, [chunk_fwd_kernel_o_param_4];
	ld.param.u64 	%rd15, [chunk_fwd_kernel_o_param_2];
	ld.param.u64 	%rd45, [chunk_fwd_kernel_o_param_0];
$L__tmp0:
	.loc	1 55 35
	// begin inline asm
	mov.u32 %r44, %ctaid.x;
	// end inline asm
	ld.param.u64 	%rd46, [chunk_fwd_kernel_o_param_1];
	.loc	1 55 53
	// begin inline asm
	mov.u32 %r45, %ctaid.y;
	// end inline asm
	ld.param.u64 	%rd47, [chunk_fwd_kernel_o_param_3];
	.loc	1 55 71
	// begin inline asm
	mov.u32 %r46, %ctaid.z;
	// end inline asm
	.loc	1 56 33
	shr.s32 	%r105, %r46, 31;
	shr.u32 	%r106, %r105, 28;
	add.s32 	%r107, %r46, %r106;
	and.b32  	%r108, %r107, -16;
	sub.s32 	%r109, %r46, %r108;
	.loc	1 60 49
	shl.b32 	%r110, %r45, 1;
	ld.param.u64 	%rd48, [chunk_fwd_kernel_o_param_5];
	ld.param.u64 	%rd49, [chunk_fwd_kernel_o_param_6];
	.loc	1 60 43
	mul.wide.s32 	%rd50, %r110, 4;
	add.s64 	%rd17, %rd49, %rd50;
	mov.pred 	%p6, -1;
	.loc	1 60 27
	// begin inline asm
	mov.u32 %r47, 0x0;
	@%p6 ld.global.b32 { %r47 }, [ %rd17 + 0 ];
	// end inline asm
	.loc	1 60 100
	add.s64 	%rd18, %rd17, 4;
	.loc	1 60 74
	// begin inline asm
	mov.u32 %r48, 0x0;
	@%p6 ld.global.b32 { %r48 }, [ %rd18 + 0 ];
	// end inline asm
	.loc	1 61 40
	mul.wide.s32 	%rd51, %r47, 4;
	add.s64 	%rd19, %rd48, %rd51;
	.loc	1 61 27
	// begin inline asm
	mov.u32 %r49, 0x0;
	@%p6 ld.global.b32 { %r49 }, [ %rd19 + 0 ];
	// end inline asm
	.loc	1 61 86
	add.s64 	%rd20, %rd19, 4;
	.loc	1 61 67
	// begin inline asm
	mov.u32 %r50, 0x0;
	@%p6 ld.global.b32 { %r50 }, [ %rd20 + 0 ];
	// end inline asm
	.loc	1 62 18
	sub.s32 	%r1, %r50, %r49;
	.loc	1 70 27
	shl.b32 	%r111, %r49, 11;
	shl.b32 	%r112, %r109, 7;
	add.s32 	%r113, %r111, %r112;
	.loc	1 70 9
	cvt.s64.s32 	%rd1, %r113;
	mul.wide.s32 	%rd52, %r113, 2;
	add.s64 	%rd2, %rd45, %rd52;
	.loc	1 71 9
	add.s64 	%rd3, %rd46, %rd52;
	.loc	1 74 17
	shl.b32 	%r114, %r45, 4;
	.loc	1 74 21
	add.s32 	%r115, %r109, %r114;
	.loc	1 74 0
	mul.wide.s32 	%rd53, %r115, 16384;
	.loc	1 74 9
	shl.b64 	%rd54, %rd53, 1;
	add.s64 	%rd55, %rd47, %rd54;
	.loc	1 80 60
	shl.b32 	%r2, %r48, 6;
	.loc	1 80 85
	cvt.s64.s32 	%rd56, %r1;
	cvt.s64.s32 	%rd57, %r2;
	.loc	1 82 68
	shl.b32 	%r116, %r44, 6;
	.loc	1 82 83
	cvt.s64.s32 	%rd58, %r116;
	.loc	1 84 22
	mov.u32 	%r3, %tid.x;
	and.b32  	%r4, %r3, 31;
	shr.u32 	%r5, %r3, 5;
	bfe.u32 	%r117, %r3, 3, 4;
	or.b32  	%r118, %r117, 16;
	or.b32  	%r119, %r117, 32;
	or.b32  	%r120, %r117, 48;
	shl.b32 	%r121, %r3, 3;
	and.b32  	%r122, %r121, 56;
	cvt.u64.u32 	%rd4, %r117;
	cvt.u64.u32 	%rd5, %r118;
	cvt.u64.u32 	%rd6, %r119;
	cvt.u64.u32 	%rd7, %r120;
	cvt.u64.u32 	%rd8, %r122;
	or.b64  	%rd59, %rd57, %rd4;
	or.b64  	%rd60, %rd57, %rd5;
	or.b64  	%rd61, %rd57, %rd6;
	or.b64  	%rd62, %rd57, %rd7;
	shl.b64 	%rd9, %rd59, 11;
	shl.b64 	%rd10, %rd60, 11;
	shl.b64 	%rd11, %rd61, 11;
	shl.b64 	%rd12, %rd62, 11;
	setp.gt.s64 	%p35, %rd59, -1;
	setp.gt.s64 	%p36, %rd60, -1;
	setp.gt.s64 	%p37, %rd61, -1;
	setp.gt.s64 	%p38, %rd62, -1;
	setp.lt.s64 	%p39, %rd59, %rd56;
	setp.lt.s64 	%p40, %rd60, %rd56;
	setp.lt.s64 	%p41, %rd61, %rd56;
	setp.lt.s64 	%p42, %rd62, %rd56;
	and.pred  	%p1, %p35, %p39;
	and.pred  	%p2, %p36, %p40;
	and.pred  	%p3, %p37, %p41;
	and.pred  	%p4, %p38, %p42;
	.loc	1 88 22
	or.b64  	%rd13, %rd58, %rd8;
	setp.lt.u64 	%p43, %rd13, 128;
	.loc	1 84 22
	or.b64  	%rd63, %rd9, %rd8;
	or.b64  	%rd64, %rd10, %rd8;
	or.b64  	%rd65, %rd11, %rd8;
	or.b64  	%rd66, %rd12, %rd8;
	shl.b64 	%rd67, %rd63, 1;
	add.s64 	%rd21, %rd2, %rd67;
	shl.b64 	%rd68, %rd64, 1;
	add.s64 	%rd22, %rd2, %rd68;
	shl.b64 	%rd69, %rd65, 1;
	add.s64 	%rd23, %rd2, %rd69;
	shl.b64 	%rd70, %rd66, 1;
	add.s64 	%rd24, %rd2, %rd70;
	shl.b32 	%r123, %r117, 6;
	xor.b32  	%r124, %r121, %r3;
	and.b32  	%r125, %r124, 56;
	or.b32  	%r6, %r123, %r125;
	shl.b32 	%r126, %r6, 1;
	mov.u32 	%r101, global_smem;
	add.s32 	%r7, %r101, %r126;
	shl.b32 	%r127, %r118, 6;
	or.b32  	%r8, %r127, %r125;
	shl.b32 	%r128, %r8, 1;
	add.s32 	%r9, %r101, %r128;
	shl.b32 	%r129, %r119, 6;
	or.b32  	%r10, %r129, %r125;
	shl.b32 	%r130, %r10, 1;
	add.s32 	%r11, %r101, %r130;
	shl.b32 	%r131, %r120, 6;
	or.b32  	%r12, %r131, %r125;
	shl.b32 	%r132, %r12, 1;
	add.s32 	%r13, %r101, %r132;
	selp.b32 	%r52, 16, 0, %p1;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r7 + 0 ], [ %rd21 + 0 ], 0x10, %r52;
	// end inline asm
	selp.b32 	%r54, 16, 0, %p2;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r9 + 0 ], [ %rd22 + 0 ], 0x10, %r54;
	// end inline asm
	selp.b32 	%r56, 16, 0, %p3;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r11 + 0 ], [ %rd23 + 0 ], 0x10, %r56;
	// end inline asm
	selp.b32 	%r58, 16, 0, %p4;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r13 + 0 ], [ %rd24 + 0 ], 0x10, %r58;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 86 22
	add.s64 	%rd25, %rd3, %rd67;
	add.s64 	%rd26, %rd3, %rd68;
	add.s64 	%rd27, %rd3, %rd69;
	add.s64 	%rd28, %rd3, %rd70;
	add.s32 	%r100, %r101, 32768;
	add.s32 	%r59, %r100, %r126;
	add.s32 	%r61, %r100, %r128;
	add.s32 	%r63, %r100, %r130;
	add.s32 	%r65, %r100, %r132;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r59 + 0 ], [ %rd25 + 0 ], 0x10, %r52;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r61 + 0 ], [ %rd26 + 0 ], 0x10, %r54;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r63 + 0 ], [ %rd27 + 0 ], 0x10, %r56;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r65 + 0 ], [ %rd28 + 0 ], 0x10, %r58;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 88 22
	mul.wide.u32 	%rd71, %r117, 128;
	mul.wide.u32 	%rd72, %r118, 128;
	mul.wide.u32 	%rd73, %r119, 128;
	mul.wide.u32 	%rd74, %r120, 128;
	shl.b64 	%rd75, %rd71, 1;
	shl.b64 	%rd76, %rd13, 1;
	.loc	1 79 21
	add.s64 	%rd14, %rd55, %rd76;
	.loc	1 88 22
	add.s64 	%rd29, %rd14, %rd75;
	shl.b64 	%rd77, %rd72, 1;
	add.s64 	%rd30, %rd14, %rd77;
	shl.b64 	%rd78, %rd73, 1;
	add.s64 	%rd31, %rd14, %rd78;
	shl.b64 	%rd79, %rd74, 1;
	add.s64 	%rd32, %rd14, %rd79;
	add.s32 	%r99, %r101, 16384;
	add.s32 	%r67, %r99, %r126;
	add.s32 	%r69, %r99, %r128;
	add.s32 	%r71, %r99, %r130;
	add.s32 	%r73, %r99, %r132;
	selp.b32 	%r68, 16, 0, %p43;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r67 + 0 ], [ %rd29 + 0 ], 0x10, %r68;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r69 + 0 ], [ %rd30 + 0 ], 0x10, %r68;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r71 + 0 ], [ %rd31 + 0 ], 0x10, %r68;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r73 + 0 ], [ %rd32 + 0 ], 0x10, %r68;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 84 22
	add.s64 	%rd33, %rd21, 128;
	add.s64 	%rd34, %rd22, 128;
	add.s64 	%rd35, %rd23, 128;
	add.s64 	%rd36, %rd24, 128;
	bar.sync 	0;
	add.s32 	%r133, %r101, 8192;
	add.s32 	%r75, %r133, %r126;
	add.s32 	%r77, %r133, %r128;
	add.s32 	%r79, %r133, %r130;
	add.s32 	%r81, %r133, %r132;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r75 + 0 ], [ %rd33 + 0 ], 0x10, %r52;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r77 + 0 ], [ %rd34 + 0 ], 0x10, %r54;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r79 + 0 ], [ %rd35 + 0 ], 0x10, %r56;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r81 + 0 ], [ %rd36 + 0 ], 0x10, %r58;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 86 22
	add.s64 	%rd37, %rd25, 128;
	add.s64 	%rd38, %rd26, 128;
	add.s64 	%rd39, %rd27, 128;
	add.s64 	%rd40, %rd28, 128;
	add.s32 	%r134, %r101, 40960;
	add.s32 	%r83, %r134, %r126;
	add.s32 	%r85, %r134, %r128;
	add.s32 	%r87, %r134, %r130;
	add.s32 	%r89, %r134, %r132;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r83 + 0 ], [ %rd37 + 0 ], 0x10, %r52;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r85 + 0 ], [ %rd38 + 0 ], 0x10, %r54;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r87 + 0 ], [ %rd39 + 0 ], 0x10, %r56;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r89 + 0 ], [ %rd40 + 0 ], 0x10, %r58;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 88 22
	add.s64 	%rd41, %rd29, 16384;
	add.s64 	%rd42, %rd30, 16384;
	add.s64 	%rd43, %rd31, 16384;
	add.s64 	%rd44, %rd32, 16384;
	add.s32 	%r135, %r101, 24576;
	add.s32 	%r91, %r135, %r126;
	add.s32 	%r93, %r135, %r128;
	add.s32 	%r95, %r135, %r130;
	add.s32 	%r97, %r135, %r132;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r91 + 0 ], [ %rd41 + 0 ], 0x10, %r68;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r93 + 0 ], [ %rd42 + 0 ], 0x10, %r68;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r95 + 0 ], [ %rd43 + 0 ], 0x10, %r68;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r97 + 0 ], [ %rd44 + 0 ], 0x10, %r68;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 84 22
	// begin inline asm
	cp.async.wait_group 0x3;
	// end inline asm
	bar.sync 	0;
	and.b32  	%r14, %r3, 7;
	bfe.u32 	%r136, %r3, 3, 1;
	bfe.u32 	%r15, %r3, 4, 1;
	shr.u32 	%r137, %r3, 4;
	and.b32  	%r138, %r137, 6;
	or.b32  	%r139, %r138, %r136;
	or.b32  	%r140, %r15, 2;
	or.b32  	%r141, %r15, 4;
	or.b32  	%r142, %r15, 6;
	or.b32  	%r143, %r136, 2;
	or.b32  	%r144, %r136, 4;
	or.b32  	%r145, %r136, 6;
	xor.b32  	%r146, %r15, %r14;
	shl.b32 	%r147, %r139, 9;
	shl.b32 	%r16, %r14, 6;
	or.b32  	%r148, %r147, %r16;
	shl.b32 	%r17, %r146, 3;
	or.b32  	%r18, %r17, %r148;
	xor.b32  	%r149, %r140, %r14;
	shl.b32 	%r19, %r149, 3;
	or.b32  	%r20, %r19, %r148;
	xor.b32  	%r150, %r141, %r14;
	shl.b32 	%r21, %r150, 3;
	or.b32  	%r22, %r21, %r148;
	xor.b32  	%r151, %r142, %r14;
	shl.b32 	%r23, %r151, 3;
	or.b32  	%r24, %r23, %r148;
	shl.b32 	%r152, %r3, 6;
	and.b32  	%r153, %r152, 960;
	or.b32  	%r25, %r17, %r153;
	or.b32  	%r26, %r19, %r153;
	or.b32  	%r27, %r21, %r153;
	or.b32  	%r28, %r23, %r153;
	xor.b32  	%r154, %r136, %r14;
	shl.b32 	%r155, %r15, 9;
	or.b32  	%r156, %r155, %r16;
	shl.b32 	%r157, %r154, 3;
	or.b32  	%r29, %r157, %r156;
	xor.b32  	%r158, %r143, %r14;
	shl.b32 	%r159, %r158, 3;
	or.b32  	%r30, %r159, %r156;
	xor.b32  	%r160, %r144, %r14;
	shl.b32 	%r161, %r160, 3;
	or.b32  	%r31, %r161, %r156;
	xor.b32  	%r162, %r145, %r14;
	shl.b32 	%r163, %r162, 3;
	or.b32  	%r32, %r163, %r156;
	mov.b32 	%r1306, 128;
	mov.f32 	%f995, 0f00000000;
	mov.b32 	%r1305, 1;
	mov.b32 	%r729, 0;
	shl.b32 	%r753, %r18, 1;
	shl.b32 	%r754, %r20, 1;
	shl.b32 	%r755, %r22, 1;
	shl.b32 	%r756, %r24, 1;
	shl.b32 	%r757, %r25, 1;
	shl.b32 	%r758, %r26, 1;
	shl.b32 	%r759, %r27, 1;
	shl.b32 	%r760, %r28, 1;
	shl.b32 	%r761, %r29, 1;
	shl.b32 	%r762, %r30, 1;
	shl.b32 	%r763, %r31, 1;
	shl.b32 	%r764, %r32, 1;
	mov.u32 	%r1301, %r99;
	mov.u32 	%r1302, %r100;
	mov.u32 	%r1303, %r101;
	mov.u32 	%r1304, %r729;
	mov.f32 	%f996, %f995;
	mov.f32 	%f997, %f995;
	mov.f32 	%f998, %f995;
	mov.f32 	%f999, %f995;
	mov.f32 	%f1000, %f995;
	mov.f32 	%f1001, %f995;
	mov.f32 	%f1002, %f995;
	mov.f32 	%f1003, %f995;
	mov.f32 	%f1004, %f995;
	mov.f32 	%f1005, %f995;
	mov.f32 	%f1006, %f995;
	mov.f32 	%f1007, %f995;
	mov.f32 	%f1008, %f995;
	mov.f32 	%f1009, %f995;
	mov.f32 	%f1010, %f995;
	mov.f32 	%f1011, %f995;
	mov.f32 	%f1012, %f995;
	mov.f32 	%f1013, %f995;
	mov.f32 	%f1014, %f995;
	mov.f32 	%f1015, %f995;
	mov.f32 	%f1016, %f995;
	mov.f32 	%f1017, %f995;
	mov.f32 	%f1018, %f995;
	mov.f32 	%f1019, %f995;
	mov.f32 	%f1020, %f995;
	mov.f32 	%f1021, %f995;
	mov.f32 	%f1022, %f995;
	mov.f32 	%f1023, %f995;
	mov.f32 	%f1024, %f995;
	mov.f32 	%f1025, %f995;
	mov.f32 	%f1026, %f995;
	mov.f32 	%f1027, %f995;
	mov.f32 	%f1028, %f995;
	mov.f32 	%f1029, %f995;
	mov.f32 	%f1030, %f995;
	mov.f32 	%f1031, %f995;
	mov.f32 	%f1032, %f995;
	mov.f32 	%f1033, %f995;
	mov.f32 	%f1034, %f995;
	mov.f32 	%f1035, %f995;
	mov.f32 	%f1036, %f995;
	mov.f32 	%f1037, %f995;
	mov.f32 	%f1038, %f995;
	mov.f32 	%f1039, %f995;
	mov.f32 	%f1040, %f995;
	mov.f32 	%f1041, %f995;
	mov.f32 	%f1042, %f995;
	mov.f32 	%f1043, %f995;
	mov.f32 	%f1044, %f995;
	mov.f32 	%f1045, %f995;
	mov.f32 	%f1046, %f995;
	mov.f32 	%f1047, %f995;
	mov.f32 	%f1048, %f995;
	mov.f32 	%f1049, %f995;
	mov.f32 	%f1050, %f995;
	mov.f32 	%f1051, %f995;
	mov.f32 	%f1052, %f995;
	mov.f32 	%f1053, %f995;
	mov.f32 	%f1054, %f995;
	mov.f32 	%f1055, %f995;
	mov.f32 	%f1056, %f995;
	mov.f32 	%f1057, %f995;
	mov.f32 	%f1058, %f995;
	mov.pred 	%p113, %p6;
$L__BB0_1:
	.loc	1 0 22
	mov.pred 	%p5, %p113;
	.loc	1 84 22
	add.s32 	%r168, %r1303, %r753;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r264, %r265, %r266, %r267 }, [ %r168 + 0 ];
	// end inline asm
	add.s32 	%r173, %r1303, %r754;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r312, %r313, %r314, %r315 }, [ %r173 + 0 ];
	// end inline asm
	add.s32 	%r178, %r1303, %r755;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r360, %r361, %r362, %r363 }, [ %r178 + 0 ];
	// end inline asm
	add.s32 	%r183, %r1303, %r756;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r408, %r409, %r410, %r411 }, [ %r183 + 0 ];
	// end inline asm
	.loc	1 88 22
	add.s32 	%r188, %r1301, %r757;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r184, %r185, %r186, %r187 }, [ %r188 + 0 ];
	// end inline asm
	add.s32 	%r193, %r188, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r189, %r190, %r191, %r192 }, [ %r193 + 0 ];
	// end inline asm
	add.s32 	%r198, %r188, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r194, %r195, %r196, %r197 }, [ %r198 + 0 ];
	// end inline asm
	add.s32 	%r203, %r188, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r199, %r200, %r201, %r202 }, [ %r203 + 0 ];
	// end inline asm
	add.s32 	%r208, %r1301, %r758;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r204, %r205, %r206, %r207 }, [ %r208 + 0 ];
	// end inline asm
	add.s32 	%r213, %r208, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r209, %r210, %r211, %r212 }, [ %r213 + 0 ];
	// end inline asm
	add.s32 	%r218, %r208, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r214, %r215, %r216, %r217 }, [ %r218 + 0 ];
	// end inline asm
	add.s32 	%r223, %r208, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r219, %r220, %r221, %r222 }, [ %r223 + 0 ];
	// end inline asm
	add.s32 	%r228, %r1301, %r759;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r224, %r225, %r226, %r227 }, [ %r228 + 0 ];
	// end inline asm
	add.s32 	%r233, %r228, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r229, %r230, %r231, %r232 }, [ %r233 + 0 ];
	// end inline asm
	add.s32 	%r238, %r228, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r234, %r235, %r236, %r237 }, [ %r238 + 0 ];
	// end inline asm
	add.s32 	%r243, %r228, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r239, %r240, %r241, %r242 }, [ %r243 + 0 ];
	// end inline asm
	add.s32 	%r248, %r1301, %r760;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r244, %r245, %r246, %r247 }, [ %r248 + 0 ];
	// end inline asm
	add.s32 	%r253, %r248, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r249, %r250, %r251, %r252 }, [ %r253 + 0 ];
	// end inline asm
	add.s32 	%r258, %r248, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r254, %r255, %r256, %r257 }, [ %r258 + 0 ];
	// end inline asm
	add.s32 	%r263, %r248, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r259, %r260, %r261, %r262 }, [ %r263 + 0 ];
	// end inline asm
	.loc	1 91 27
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1027, %f1028, %f1029, %f1030 }, { %r264, %r265, %r266, %r267 }, { %r184, %r185 }, { %f1027, %f1028, %f1029, %f1030 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1031, %f1032, %f1033, %f1034 }, { %r264, %r265, %r266, %r267 }, { %r186, %r187 }, { %f1031, %f1032, %f1033, %f1034 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1035, %f1036, %f1037, %f1038 }, { %r264, %r265, %r266, %r267 }, { %r204, %r205 }, { %f1035, %f1036, %f1037, %f1038 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1039, %f1040, %f1041, %f1042 }, { %r264, %r265, %r266, %r267 }, { %r206, %r207 }, { %f1039, %f1040, %f1041, %f1042 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1043, %f1044, %f1045, %f1046 }, { %r264, %r265, %r266, %r267 }, { %r224, %r225 }, { %f1043, %f1044, %f1045, %f1046 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1047, %f1048, %f1049, %f1050 }, { %r264, %r265, %r266, %r267 }, { %r226, %r227 }, { %f1047, %f1048, %f1049, %f1050 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1051, %f1052, %f1053, %f1054 }, { %r264, %r265, %r266, %r267 }, { %r244, %r245 }, { %f1051, %f1052, %f1053, %f1054 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1055, %f1056, %f1057, %f1058 }, { %r264, %r265, %r266, %r267 }, { %r246, %r247 }, { %f1055, %f1056, %f1057, %f1058 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1027, %f1028, %f1029, %f1030 }, { %r312, %r313, %r314, %r315 }, { %r189, %r190 }, { %f1027, %f1028, %f1029, %f1030 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1031, %f1032, %f1033, %f1034 }, { %r312, %r313, %r314, %r315 }, { %r191, %r192 }, { %f1031, %f1032, %f1033, %f1034 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1035, %f1036, %f1037, %f1038 }, { %r312, %r313, %r314, %r315 }, { %r209, %r210 }, { %f1035, %f1036, %f1037, %f1038 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1039, %f1040, %f1041, %f1042 }, { %r312, %r313, %r314, %r315 }, { %r211, %r212 }, { %f1039, %f1040, %f1041, %f1042 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1043, %f1044, %f1045, %f1046 }, { %r312, %r313, %r314, %r315 }, { %r229, %r230 }, { %f1043, %f1044, %f1045, %f1046 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1047, %f1048, %f1049, %f1050 }, { %r312, %r313, %r314, %r315 }, { %r231, %r232 }, { %f1047, %f1048, %f1049, %f1050 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1051, %f1052, %f1053, %f1054 }, { %r312, %r313, %r314, %r315 }, { %r249, %r250 }, { %f1051, %f1052, %f1053, %f1054 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1055, %f1056, %f1057, %f1058 }, { %r312, %r313, %r314, %r315 }, { %r251, %r252 }, { %f1055, %f1056, %f1057, %f1058 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1027, %f1028, %f1029, %f1030 }, { %r360, %r361, %r362, %r363 }, { %r194, %r195 }, { %f1027, %f1028, %f1029, %f1030 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1031, %f1032, %f1033, %f1034 }, { %r360, %r361, %r362, %r363 }, { %r196, %r197 }, { %f1031, %f1032, %f1033, %f1034 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1035, %f1036, %f1037, %f1038 }, { %r360, %r361, %r362, %r363 }, { %r214, %r215 }, { %f1035, %f1036, %f1037, %f1038 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1039, %f1040, %f1041, %f1042 }, { %r360, %r361, %r362, %r363 }, { %r216, %r217 }, { %f1039, %f1040, %f1041, %f1042 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1043, %f1044, %f1045, %f1046 }, { %r360, %r361, %r362, %r363 }, { %r234, %r235 }, { %f1043, %f1044, %f1045, %f1046 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1047, %f1048, %f1049, %f1050 }, { %r360, %r361, %r362, %r363 }, { %r236, %r237 }, { %f1047, %f1048, %f1049, %f1050 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1051, %f1052, %f1053, %f1054 }, { %r360, %r361, %r362, %r363 }, { %r254, %r255 }, { %f1051, %f1052, %f1053, %f1054 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1055, %f1056, %f1057, %f1058 }, { %r360, %r361, %r362, %r363 }, { %r256, %r257 }, { %f1055, %f1056, %f1057, %f1058 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1027, %f1028, %f1029, %f1030 }, { %r408, %r409, %r410, %r411 }, { %r199, %r200 }, { %f1027, %f1028, %f1029, %f1030 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1031, %f1032, %f1033, %f1034 }, { %r408, %r409, %r410, %r411 }, { %r201, %r202 }, { %f1031, %f1032, %f1033, %f1034 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1035, %f1036, %f1037, %f1038 }, { %r408, %r409, %r410, %r411 }, { %r219, %r220 }, { %f1035, %f1036, %f1037, %f1038 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1039, %f1040, %f1041, %f1042 }, { %r408, %r409, %r410, %r411 }, { %r221, %r222 }, { %f1039, %f1040, %f1041, %f1042 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1043, %f1044, %f1045, %f1046 }, { %r408, %r409, %r410, %r411 }, { %r239, %r240 }, { %f1043, %f1044, %f1045, %f1046 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1047, %f1048, %f1049, %f1050 }, { %r408, %r409, %r410, %r411 }, { %r241, %r242 }, { %f1047, %f1048, %f1049, %f1050 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1051, %f1052, %f1053, %f1054 }, { %r408, %r409, %r410, %r411 }, { %r259, %r260 }, { %f1051, %f1052, %f1053, %f1054 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1055, %f1056, %f1057, %f1058 }, { %r408, %r409, %r410, %r411 }, { %r261, %r262 }, { %f1055, %f1056, %f1057, %f1058 };
	// end inline asm
	.loc	1 86 22
	add.s32 	%r460, %r1302, %r761;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r456, %r457, %r458, %r459 }, [ %r460 + 0 ];
	// end inline asm
	add.s32 	%r465, %r1302, %r762;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r461, %r462, %r463, %r464 }, [ %r465 + 0 ];
	// end inline asm
	add.s32 	%r470, %r1302, %r763;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r466, %r467, %r468, %r469 }, [ %r470 + 0 ];
	// end inline asm
	add.s32 	%r475, %r1302, %r764;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r471, %r472, %r473, %r474 }, [ %r475 + 0 ];
	// end inline asm
	add.s32 	%r480, %r460, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r476, %r477, %r478, %r479 }, [ %r480 + 0 ];
	// end inline asm
	add.s32 	%r485, %r465, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r481, %r482, %r483, %r484 }, [ %r485 + 0 ];
	// end inline asm
	add.s32 	%r490, %r470, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r486, %r487, %r488, %r489 }, [ %r490 + 0 ];
	// end inline asm
	add.s32 	%r495, %r475, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r491, %r492, %r493, %r494 }, [ %r495 + 0 ];
	// end inline asm
	add.s32 	%r500, %r460, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r496, %r497, %r498, %r499 }, [ %r500 + 0 ];
	// end inline asm
	add.s32 	%r505, %r465, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r501, %r502, %r503, %r504 }, [ %r505 + 0 ];
	// end inline asm
	add.s32 	%r510, %r470, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r506, %r507, %r508, %r509 }, [ %r510 + 0 ];
	// end inline asm
	add.s32 	%r515, %r475, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r511, %r512, %r513, %r514 }, [ %r515 + 0 ];
	// end inline asm
	add.s32 	%r520, %r460, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r516, %r517, %r518, %r519 }, [ %r520 + 0 ];
	// end inline asm
	add.s32 	%r525, %r465, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r521, %r522, %r523, %r524 }, [ %r525 + 0 ];
	// end inline asm
	add.s32 	%r530, %r470, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r526, %r527, %r528, %r529 }, [ %r530 + 0 ];
	// end inline asm
	add.s32 	%r535, %r475, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r531, %r532, %r533, %r534 }, [ %r535 + 0 ];
	// end inline asm
	.loc	1 93 27
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f995, %f996, %f997, %f998 }, { %r264, %r265, %r266, %r267 }, { %r456, %r457 }, { %f995, %f996, %f997, %f998 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f999, %f1000, %f1001, %f1002 }, { %r264, %r265, %r266, %r267 }, { %r458, %r459 }, { %f999, %f1000, %f1001, %f1002 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1003, %f1004, %f1005, %f1006 }, { %r264, %r265, %r266, %r267 }, { %r476, %r477 }, { %f1003, %f1004, %f1005, %f1006 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1007, %f1008, %f1009, %f1010 }, { %r264, %r265, %r266, %r267 }, { %r478, %r479 }, { %f1007, %f1008, %f1009, %f1010 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1011, %f1012, %f1013, %f1014 }, { %r264, %r265, %r266, %r267 }, { %r496, %r497 }, { %f1011, %f1012, %f1013, %f1014 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1015, %f1016, %f1017, %f1018 }, { %r264, %r265, %r266, %r267 }, { %r498, %r499 }, { %f1015, %f1016, %f1017, %f1018 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1019, %f1020, %f1021, %f1022 }, { %r264, %r265, %r266, %r267 }, { %r516, %r517 }, { %f1019, %f1020, %f1021, %f1022 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1023, %f1024, %f1025, %f1026 }, { %r264, %r265, %r266, %r267 }, { %r518, %r519 }, { %f1023, %f1024, %f1025, %f1026 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f995, %f996, %f997, %f998 }, { %r312, %r313, %r314, %r315 }, { %r461, %r462 }, { %f995, %f996, %f997, %f998 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f999, %f1000, %f1001, %f1002 }, { %r312, %r313, %r314, %r315 }, { %r463, %r464 }, { %f999, %f1000, %f1001, %f1002 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1003, %f1004, %f1005, %f1006 }, { %r312, %r313, %r314, %r315 }, { %r481, %r482 }, { %f1003, %f1004, %f1005, %f1006 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1007, %f1008, %f1009, %f1010 }, { %r312, %r313, %r314, %r315 }, { %r483, %r484 }, { %f1007, %f1008, %f1009, %f1010 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1011, %f1012, %f1013, %f1014 }, { %r312, %r313, %r314, %r315 }, { %r501, %r502 }, { %f1011, %f1012, %f1013, %f1014 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1015, %f1016, %f1017, %f1018 }, { %r312, %r313, %r314, %r315 }, { %r503, %r504 }, { %f1015, %f1016, %f1017, %f1018 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1019, %f1020, %f1021, %f1022 }, { %r312, %r313, %r314, %r315 }, { %r521, %r522 }, { %f1019, %f1020, %f1021, %f1022 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1023, %f1024, %f1025, %f1026 }, { %r312, %r313, %r314, %r315 }, { %r523, %r524 }, { %f1023, %f1024, %f1025, %f1026 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f995, %f996, %f997, %f998 }, { %r360, %r361, %r362, %r363 }, { %r466, %r467 }, { %f995, %f996, %f997, %f998 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f999, %f1000, %f1001, %f1002 }, { %r360, %r361, %r362, %r363 }, { %r468, %r469 }, { %f999, %f1000, %f1001, %f1002 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1003, %f1004, %f1005, %f1006 }, { %r360, %r361, %r362, %r363 }, { %r486, %r487 }, { %f1003, %f1004, %f1005, %f1006 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1007, %f1008, %f1009, %f1010 }, { %r360, %r361, %r362, %r363 }, { %r488, %r489 }, { %f1007, %f1008, %f1009, %f1010 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1011, %f1012, %f1013, %f1014 }, { %r360, %r361, %r362, %r363 }, { %r506, %r507 }, { %f1011, %f1012, %f1013, %f1014 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1015, %f1016, %f1017, %f1018 }, { %r360, %r361, %r362, %r363 }, { %r508, %r509 }, { %f1015, %f1016, %f1017, %f1018 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1019, %f1020, %f1021, %f1022 }, { %r360, %r361, %r362, %r363 }, { %r526, %r527 }, { %f1019, %f1020, %f1021, %f1022 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1023, %f1024, %f1025, %f1026 }, { %r360, %r361, %r362, %r363 }, { %r528, %r529 }, { %f1023, %f1024, %f1025, %f1026 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f995, %f996, %f997, %f998 }, { %r408, %r409, %r410, %r411 }, { %r471, %r472 }, { %f995, %f996, %f997, %f998 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f999, %f1000, %f1001, %f1002 }, { %r408, %r409, %r410, %r411 }, { %r473, %r474 }, { %f999, %f1000, %f1001, %f1002 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1003, %f1004, %f1005, %f1006 }, { %r408, %r409, %r410, %r411 }, { %r491, %r492 }, { %f1003, %f1004, %f1005, %f1006 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1007, %f1008, %f1009, %f1010 }, { %r408, %r409, %r410, %r411 }, { %r493, %r494 }, { %f1007, %f1008, %f1009, %f1010 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1011, %f1012, %f1013, %f1014 }, { %r408, %r409, %r410, %r411 }, { %r511, %r512 }, { %f1011, %f1012, %f1013, %f1014 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1015, %f1016, %f1017, %f1018 }, { %r408, %r409, %r410, %r411 }, { %r513, %r514 }, { %f1015, %f1016, %f1017, %f1018 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1019, %f1020, %f1021, %f1022 }, { %r408, %r409, %r410, %r411 }, { %r531, %r532 }, { %f1019, %f1020, %f1021, %f1022 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1023, %f1024, %f1025, %f1026 }, { %r408, %r409, %r410, %r411 }, { %r533, %r534 }, { %f1023, %f1024, %f1025, %f1026 };
	// end inline asm
	.loc	1 79 21
	add.s32 	%r765, %r1305, 1;
	setp.lt.s32 	%p57, %r765, 2;
	selp.b32 	%r1305, %r765, 0, %p57;
	cvt.u64.u32 	%rd92, %r1306;
	.loc	1 84 22
	or.b64  	%rd93, %rd92, %rd8;
	or.b64  	%rd94, %rd92, %rd4;
	or.b64  	%rd95, %rd92, %rd5;
	or.b64  	%rd96, %rd92, %rd6;
	or.b64  	%rd97, %rd92, %rd7;
	or.b64  	%rd98, %rd93, %rd9;
	or.b64  	%rd99, %rd93, %rd10;
	or.b64  	%rd100, %rd93, %rd11;
	or.b64  	%rd101, %rd93, %rd12;
	shl.b64 	%rd102, %rd98, 1;
	add.s64 	%rd80, %rd2, %rd102;
	shl.b64 	%rd103, %rd99, 1;
	add.s64 	%rd81, %rd2, %rd103;
	shl.b64 	%rd104, %rd100, 1;
	add.s64 	%rd82, %rd2, %rd104;
	shl.b64 	%rd105, %rd101, 1;
	add.s64 	%rd83, %rd2, %rd105;
	shl.b32 	%r766, %r1305, 13;
	add.s32 	%r768, %r101, %r766;
	bar.sync 	0;
	add.s32 	%r728, %r768, %r126;
	add.s32 	%r730, %r768, %r128;
	add.s32 	%r732, %r768, %r130;
	add.s32 	%r734, %r768, %r132;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r728 + 0 ], [ %rd80 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r730 + 0 ], [ %rd81 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r732 + 0 ], [ %rd82 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r734 + 0 ], [ %rd83 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 86 22
	add.s64 	%rd84, %rd3, %rd102;
	add.s64 	%rd85, %rd3, %rd103;
	add.s64 	%rd86, %rd3, %rd104;
	add.s64 	%rd87, %rd3, %rd105;
	add.s32 	%r774, %r100, %r766;
	add.s32 	%r736, %r774, %r126;
	add.s32 	%r738, %r774, %r128;
	add.s32 	%r740, %r774, %r130;
	add.s32 	%r742, %r774, %r132;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r736 + 0 ], [ %rd84 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r738 + 0 ], [ %rd85 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r740 + 0 ], [ %rd86 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r742 + 0 ], [ %rd87 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 88 22
	shl.b64 	%rd106, %rd94, 8;
	add.s64 	%rd88, %rd14, %rd106;
	shl.b64 	%rd107, %rd95, 8;
	add.s64 	%rd89, %rd14, %rd107;
	shl.b64 	%rd108, %rd96, 8;
	add.s64 	%rd90, %rd14, %rd108;
	shl.b64 	%rd109, %rd97, 8;
	add.s64 	%rd91, %rd14, %rd109;
	add.s32 	%r776, %r99, %r766;
	add.s32 	%r744, %r776, %r126;
	add.s32 	%r746, %r776, %r128;
	add.s32 	%r748, %r776, %r130;
	add.s32 	%r750, %r776, %r132;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r744 + 0 ], [ %rd88 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r746 + 0 ], [ %rd89 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r748 + 0 ], [ %rd90 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r750 + 0 ], [ %rd91 + 0 ], 0x10, %r729;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 79 21
	add.s32 	%r777, %r1304, 1;
	setp.lt.s32 	%p58, %r777, 2;
	selp.b32 	%r1304, %r777, 0, %p58;
	.loc	1 84 22
	shl.b32 	%r778, %r1304, 13;
	add.s32 	%r1303, %r101, %r778;
	.loc	1 86 22
	add.s32 	%r1302, %r100, %r778;
	.loc	1 84 22
	// begin inline asm
	cp.async.wait_group 0x3;
	// end inline asm
	bar.sync 	0;
	.loc	1 88 22
	add.s32 	%r1301, %r99, %r778;
	mov.b32 	%r1306, 192;
	mov.pred 	%p113, 0;
	.loc	1 79 21
	@%p5 bra 	$L__BB0_1;
	.loc	1 73 9
	shl.b64 	%rd118, %rd1, 1;
	add.s64 	%rd119, %rd16, %rd118;
	.loc	1 72 9
	add.s64 	%rd120, %rd15, %rd118;
	.loc	1 79 21
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 108 34
	shr.u32 	%r1083, %r3, 2;
	bfe.u32 	%r1084, %r3, 2, 3;
	shr.u32 	%r1085, %r3, 1;
	and.b32  	%r1086, %r1085, 48;
	or.b32  	%r1087, %r1084, %r1086;
	or.b32  	%r1088, %r1087, 8;
	shl.b32 	%r1089, %r3, 1;
	and.b32  	%r1090, %r1089, 6;
	or.b32  	%r1091, %r1090, 8;
	or.b32  	%r1092, %r1090, 9;
	or.b32  	%r1093, %r1090, 16;
	or.b32  	%r1094, %r1090, 17;
	or.b32  	%r1095, %r1090, 24;
	or.b32  	%r1096, %r1090, 25;
	or.b32  	%r1097, %r1090, 32;
	or.b32  	%r1098, %r1090, 33;
	or.b32  	%r1099, %r1090, 40;
	or.b32  	%r1100, %r1090, 41;
	or.b32  	%r1101, %r1090, 48;
	or.b32  	%r1102, %r1090, 49;
	or.b32  	%r1103, %r1090, 56;
	or.b32  	%r1104, %r1090, 57;
	.loc	1 108 21
	or.b32  	%r1105, %r2, %r1087;
	or.b32  	%r1106, %r2, %r1088;
	or.b32  	%r1107, %r2, %r1090;
	or.b32  	%r1108, %r1107, 1;
	or.b32  	%r1109, %r2, %r1091;
	or.b32  	%r1110, %r2, %r1092;
	or.b32  	%r1111, %r2, %r1093;
	or.b32  	%r1112, %r2, %r1094;
	or.b32  	%r1113, %r2, %r1095;
	or.b32  	%r1114, %r2, %r1096;
	or.b32  	%r1115, %r2, %r1097;
	or.b32  	%r1116, %r2, %r1098;
	or.b32  	%r1117, %r2, %r1099;
	or.b32  	%r1118, %r2, %r1100;
	or.b32  	%r1119, %r2, %r1101;
	or.b32  	%r1120, %r2, %r1102;
	or.b32  	%r1121, %r2, %r1103;
	or.b32  	%r1122, %r2, %r1104;
	.loc	1 109 16
	setp.lt.s32 	%p68, %r1105, %r1;
	setp.lt.s32 	%p69, %r1106, %r1;
	setp.lt.s32 	%p70, %r1107, %r1;
	setp.lt.s32 	%p71, %r1108, %r1;
	setp.lt.s32 	%p72, %r1109, %r1;
	setp.lt.s32 	%p73, %r1110, %r1;
	setp.lt.s32 	%p74, %r1111, %r1;
	setp.lt.s32 	%p75, %r1112, %r1;
	setp.lt.s32 	%p76, %r1113, %r1;
	setp.lt.s32 	%p77, %r1114, %r1;
	setp.lt.s32 	%p78, %r1115, %r1;
	setp.lt.s32 	%p79, %r1116, %r1;
	setp.lt.s32 	%p80, %r1117, %r1;
	setp.lt.s32 	%p81, %r1118, %r1;
	setp.lt.s32 	%p82, %r1119, %r1;
	setp.lt.s32 	%p83, %r1120, %r1;
	setp.lt.s32 	%p84, %r1121, %r1;
	setp.lt.s32 	%p85, %r1122, %r1;
	.loc	1 110 27
	setp.ge.u32 	%p86, %r1087, %r1090;
	setp.gt.u32 	%p87, %r1087, %r1090;
	setp.ge.u32 	%p88, %r1087, %r1091;
	setp.ge.u32 	%p89, %r1087, %r1092;
	setp.ge.u32 	%p90, %r1088, %r1092;
	setp.ge.u32 	%p91, %r1087, %r1093;
	setp.ge.u32 	%p92, %r1087, %r1094;
	setp.ge.u32 	%p93, %r1088, %r1093;
	setp.ge.u32 	%p94, %r1088, %r1094;
	setp.ge.u32 	%p95, %r1087, %r1095;
	setp.ge.u32 	%p96, %r1087, %r1096;
	setp.ge.u32 	%p97, %r1088, %r1095;
	setp.ge.u32 	%p98, %r1088, %r1096;
	setp.ge.u32 	%p99, %r1087, %r1097;
	setp.ge.u32 	%p100, %r1087, %r1098;
	setp.ge.u32 	%p101, %r1088, %r1097;
	setp.ge.u32 	%p102, %r1088, %r1098;
	setp.ge.u32 	%p103, %r1087, %r1099;
	setp.ge.u32 	%p104, %r1087, %r1100;
	setp.ge.u32 	%p105, %r1088, %r1099;
	setp.ge.u32 	%p106, %r1088, %r1100;
	setp.ge.u32 	%p107, %r1087, %r1101;
	setp.ge.u32 	%p108, %r1087, %r1102;
	setp.ge.u32 	%p109, %r1088, %r1101;
	setp.ge.u32 	%p110, %r1088, %r1102;
	setp.ge.u32 	%p111, %r1088, %r1103;
	setp.ge.u32 	%p112, %r1088, %r1104;
	.loc	1 119 38
	cvt.rn.f16.f32 	%rs1, %f995;
	cvt.rn.f16.f32 	%rs2, %f996;
	cvt.rn.f16.f32 	%rs3, %f997;
	cvt.rn.f16.f32 	%rs4, %f998;
	cvt.rn.f16.f32 	%rs5, %f999;
	cvt.rn.f16.f32 	%rs6, %f1000;
	cvt.rn.f16.f32 	%rs7, %f1001;
	cvt.rn.f16.f32 	%rs8, %f1002;
	cvt.rn.f16.f32 	%rs9, %f1003;
	cvt.rn.f16.f32 	%rs10, %f1004;
	cvt.rn.f16.f32 	%rs11, %f1005;
	cvt.rn.f16.f32 	%rs12, %f1006;
	cvt.rn.f16.f32 	%rs13, %f1007;
	cvt.rn.f16.f32 	%rs14, %f1008;
	cvt.rn.f16.f32 	%rs15, %f1009;
	cvt.rn.f16.f32 	%rs16, %f1010;
	cvt.rn.f16.f32 	%rs17, %f1011;
	cvt.rn.f16.f32 	%rs18, %f1012;
	cvt.rn.f16.f32 	%rs19, %f1013;
	cvt.rn.f16.f32 	%rs20, %f1014;
	cvt.rn.f16.f32 	%rs21, %f1015;
	cvt.rn.f16.f32 	%rs22, %f1016;
	cvt.rn.f16.f32 	%rs23, %f1017;
	cvt.rn.f16.f32 	%rs24, %f1018;
	cvt.rn.f16.f32 	%rs25, %f1019;
	cvt.rn.f16.f32 	%rs26, %f1020;
	cvt.rn.f16.f32 	%rs27, %f1021;
	cvt.rn.f16.f32 	%rs28, %f1022;
	cvt.rn.f16.f32 	%rs29, %f1025;
	cvt.rn.f16.f32 	%rs30, %f1026;
	.loc	1 116 18
	add.s64 	%rd121, %rd9, %rd13;
	add.s64 	%rd122, %rd10, %rd13;
	add.s64 	%rd123, %rd11, %rd13;
	add.s64 	%rd124, %rd12, %rd13;
	shl.b64 	%rd125, %rd121, 1;
	add.s64 	%rd110, %rd120, %rd125;
	shl.b64 	%rd126, %rd122, 1;
	add.s64 	%rd111, %rd120, %rd126;
	shl.b64 	%rd127, %rd123, 1;
	add.s64 	%rd112, %rd120, %rd127;
	shl.b64 	%rd128, %rd124, 1;
	add.s64 	%rd113, %rd120, %rd128;
	and.pred  	%p59, %p43, %p1;
	and.pred  	%p60, %p43, %p2;
	and.pred  	%p61, %p43, %p3;
	and.pred  	%p62, %p43, %p4;
	// begin inline asm
	mov.u32 %r779, 0x0;
	mov.u32 %r780, 0x0;
	mov.u32 %r781, 0x0;
	mov.u32 %r782, 0x0;
	@%p59 ld.global.v4.b32 { %r779, %r780, %r781, %r782 }, [ %rd110 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r783, 0x0;
	mov.u32 %r784, 0x0;
	mov.u32 %r785, 0x0;
	mov.u32 %r786, 0x0;
	@%p60 ld.global.v4.b32 { %r783, %r784, %r785, %r786 }, [ %rd111 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r787, 0x0;
	mov.u32 %r788, 0x0;
	mov.u32 %r789, 0x0;
	mov.u32 %r790, 0x0;
	@%p61 ld.global.v4.b32 { %r787, %r788, %r789, %r790 }, [ %rd112 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r791, 0x0;
	mov.u32 %r792, 0x0;
	mov.u32 %r793, 0x0;
	mov.u32 %r794, 0x0;
	@%p62 ld.global.v4.b32 { %r791, %r792, %r793, %r794 }, [ %rd113 + 0 ];
	// end inline asm
	st.shared.v4.b32 	[%r7], {%r779, %r780, %r781, %r782};
	st.shared.v4.b32 	[%r9], {%r783, %r784, %r785, %r786};
	st.shared.v4.b32 	[%r11], {%r787, %r788, %r789, %r790};
	st.shared.v4.b32 	[%r13], {%r791, %r792, %r793, %r794};
	.loc	1 119 16
	mul.f32 	%f899, %f1027, %f129;
	mul.f32 	%f900, %f1028, %f129;
	mul.f32 	%f901, %f1029, %f129;
	mul.f32 	%f902, %f1030, %f129;
	mul.f32 	%f903, %f1031, %f129;
	mul.f32 	%f904, %f1032, %f129;
	mul.f32 	%f905, %f1033, %f129;
	mul.f32 	%f906, %f1034, %f129;
	mul.f32 	%f907, %f1035, %f129;
	mul.f32 	%f908, %f1036, %f129;
	mul.f32 	%f909, %f1037, %f129;
	mul.f32 	%f910, %f1038, %f129;
	mul.f32 	%f911, %f1039, %f129;
	mul.f32 	%f912, %f1040, %f129;
	mul.f32 	%f913, %f1041, %f129;
	mul.f32 	%f914, %f1042, %f129;
	mul.f32 	%f915, %f1043, %f129;
	mul.f32 	%f916, %f1044, %f129;
	mul.f32 	%f917, %f1045, %f129;
	mul.f32 	%f918, %f1046, %f129;
	mul.f32 	%f919, %f1047, %f129;
	mul.f32 	%f920, %f1048, %f129;
	mul.f32 	%f921, %f1049, %f129;
	mul.f32 	%f922, %f1050, %f129;
	mul.f32 	%f923, %f1051, %f129;
	mul.f32 	%f924, %f1052, %f129;
	mul.f32 	%f925, %f1053, %f129;
	mul.f32 	%f926, %f1054, %f129;
	mul.f32 	%f927, %f1055, %f129;
	mul.f32 	%f928, %f1056, %f129;
	mul.f32 	%f929, %f1057, %f129;
	mul.f32 	%f930, %f1058, %f129;
	and.b32  	%r1139, %r5, 3;
	shr.u32 	%r1140, %r4, 2;
	shl.b32 	%r1141, %r1139, 4;
	or.b32  	%r1142, %r1141, %r1140;
	mad.lo.s32 	%r1143, %r1142, 18, %r1090;
	shl.b32 	%r1144, %r1143, 2;
	mov.u32 	%r1145, global_smem;
	add.s32 	%r1146, %r1145, 8192;
	add.s32 	%r1147, %r1146, %r1144;
	st.shared.v2.f32 	[%r1147], {%f899, %f900};
	or.b32  	%r1148, %r1140, 8;
	or.b32  	%r1149, %r1148, %r1141;
	mad.lo.s32 	%r1150, %r1149, 18, %r1090;
	shl.b32 	%r1151, %r1150, 2;
	add.s32 	%r1152, %r1146, %r1151;
	st.shared.v2.f32 	[%r1152], {%f901, %f902};
	st.shared.v2.f32 	[%r1147+32], {%f903, %f904};
	st.shared.v2.f32 	[%r1152+32], {%f905, %f906};
	bar.sync 	0;
	and.b32  	%r1153, %r5, 1;
	and.b32  	%r1154, %r1083, 16;
	or.b32  	%r1155, %r1140, %r1154;
	shl.b32 	%r1156, %r1153, 3;
	or.b32  	%r1157, %r1156, %r1090;
	mad.lo.s32 	%r1158, %r1155, 18, %r1157;
	shl.b32 	%r1159, %r1158, 2;
	add.s32 	%r1160, %r1146, %r1159;
	or.b32  	%r1161, %r1148, %r1154;
	mad.lo.s32 	%r1162, %r1161, 18, %r1157;
	shl.b32 	%r1163, %r1162, 2;
	add.s32 	%r1164, %r1146, %r1163;
	.loc	1 119 38
	selp.b16 	%rs31, %rs1, 0x0000, %p70;
	selp.b16 	%rs32, %rs31, 0x0000, %p86;
	selp.b16 	%rs33, %rs32, 0x0000, %p68;
	selp.b16 	%rs34, %rs2, 0x0000, %p71;
	selp.b16 	%rs35, %rs34, 0x0000, %p87;
	selp.b16 	%rs36, %rs35, 0x0000, %p68;
	selp.b16 	%rs37, %rs3, 0x0000, %p69;
	selp.b16 	%rs38, %rs37, 0x0000, %p70;
	selp.b16 	%rs39, %rs4, 0x0000, %p69;
	selp.b16 	%rs40, %rs39, 0x0000, %p71;
	selp.b16 	%rs41, %rs5, 0x0000, %p72;
	selp.b16 	%rs42, %rs41, 0x0000, %p88;
	selp.b16 	%rs43, %rs42, 0x0000, %p68;
	selp.b16 	%rs44, %rs6, 0x0000, %p73;
	selp.b16 	%rs45, %rs44, 0x0000, %p89;
	selp.b16 	%rs46, %rs45, 0x0000, %p68;
	selp.b16 	%rs47, %rs7, 0x0000, %p72;
	selp.b16 	%rs48, %rs47, 0x0000, %p86;
	selp.b16 	%rs49, %rs48, 0x0000, %p69;
	selp.b16 	%rs50, %rs8, 0x0000, %p73;
	selp.b16 	%rs51, %rs50, 0x0000, %p90;
	selp.b16 	%rs52, %rs51, 0x0000, %p69;
	selp.b16 	%rs53, %rs9, 0x0000, %p74;
	selp.b16 	%rs54, %rs53, 0x0000, %p91;
	selp.b16 	%rs55, %rs54, 0x0000, %p68;
	selp.b16 	%rs56, %rs10, 0x0000, %p75;
	selp.b16 	%rs57, %rs56, 0x0000, %p92;
	selp.b16 	%rs58, %rs57, 0x0000, %p68;
	selp.b16 	%rs59, %rs11, 0x0000, %p74;
	selp.b16 	%rs60, %rs59, 0x0000, %p93;
	selp.b16 	%rs61, %rs60, 0x0000, %p69;
	selp.b16 	%rs62, %rs12, 0x0000, %p75;
	selp.b16 	%rs63, %rs62, 0x0000, %p94;
	selp.b16 	%rs64, %rs63, 0x0000, %p69;
	selp.b16 	%rs65, %rs13, 0x0000, %p76;
	selp.b16 	%rs66, %rs65, 0x0000, %p95;
	selp.b16 	%rs67, %rs66, 0x0000, %p68;
	selp.b16 	%rs68, %rs14, 0x0000, %p77;
	selp.b16 	%rs69, %rs68, 0x0000, %p96;
	selp.b16 	%rs70, %rs69, 0x0000, %p68;
	selp.b16 	%rs71, %rs15, 0x0000, %p76;
	selp.b16 	%rs72, %rs71, 0x0000, %p97;
	selp.b16 	%rs73, %rs72, 0x0000, %p69;
	selp.b16 	%rs74, %rs16, 0x0000, %p77;
	selp.b16 	%rs75, %rs74, 0x0000, %p98;
	selp.b16 	%rs76, %rs75, 0x0000, %p69;
	selp.b16 	%rs77, %rs17, 0x0000, %p78;
	selp.b16 	%rs78, %rs77, 0x0000, %p99;
	selp.b16 	%rs79, %rs78, 0x0000, %p68;
	selp.b16 	%rs80, %rs18, 0x0000, %p79;
	selp.b16 	%rs81, %rs80, 0x0000, %p100;
	selp.b16 	%rs82, %rs81, 0x0000, %p68;
	selp.b16 	%rs83, %rs19, 0x0000, %p78;
	selp.b16 	%rs84, %rs83, 0x0000, %p101;
	selp.b16 	%rs85, %rs84, 0x0000, %p69;
	selp.b16 	%rs86, %rs20, 0x0000, %p79;
	selp.b16 	%rs87, %rs86, 0x0000, %p102;
	selp.b16 	%rs88, %rs87, 0x0000, %p69;
	selp.b16 	%rs89, %rs21, 0x0000, %p80;
	selp.b16 	%rs90, %rs89, 0x0000, %p103;
	selp.b16 	%rs91, %rs90, 0x0000, %p68;
	selp.b16 	%rs92, %rs22, 0x0000, %p81;
	selp.b16 	%rs93, %rs92, 0x0000, %p104;
	selp.b16 	%rs94, %rs93, 0x0000, %p68;
	selp.b16 	%rs95, %rs23, 0x0000, %p80;
	selp.b16 	%rs96, %rs95, 0x0000, %p105;
	selp.b16 	%rs97, %rs96, 0x0000, %p69;
	selp.b16 	%rs98, %rs24, 0x0000, %p81;
	selp.b16 	%rs99, %rs98, 0x0000, %p106;
	selp.b16 	%rs100, %rs99, 0x0000, %p69;
	selp.b16 	%rs101, %rs25, 0x0000, %p82;
	selp.b16 	%rs102, %rs101, 0x0000, %p107;
	selp.b16 	%rs103, %rs102, 0x0000, %p68;
	selp.b16 	%rs104, %rs26, 0x0000, %p83;
	selp.b16 	%rs105, %rs104, 0x0000, %p108;
	selp.b16 	%rs106, %rs105, 0x0000, %p68;
	selp.b16 	%rs107, %rs27, 0x0000, %p82;
	selp.b16 	%rs108, %rs107, 0x0000, %p109;
	selp.b16 	%rs109, %rs108, 0x0000, %p69;
	selp.b16 	%rs110, %rs28, 0x0000, %p83;
	selp.b16 	%rs111, %rs110, 0x0000, %p110;
	selp.b16 	%rs112, %rs111, 0x0000, %p69;
	selp.b16 	%rs113, %rs29, 0x0000, %p84;
	selp.b16 	%rs114, %rs113, 0x0000, %p111;
	selp.b16 	%rs115, %rs114, 0x0000, %p69;
	selp.b16 	%rs116, %rs30, 0x0000, %p85;
	selp.b16 	%rs117, %rs116, 0x0000, %p112;
	selp.b16 	%rs118, %rs117, 0x0000, %p69;
	shl.b32 	%r1165, %r1087, 6;
	shl.b32 	%r1166, %r1084, 3;
	or.b32  	%r1167, %r1166, %r1090;
	or.b32  	%r1168, %r1165, %r1167;
	shl.b32 	%r1169, %r1168, 1;
	add.s32 	%r1170, %r1146, %r1169;
	shl.b32 	%r1171, %r1088, 6;
	or.b32  	%r1172, %r1171, %r1167;
	shl.b32 	%r1173, %r1172, 1;
	add.s32 	%r1174, %r1146, %r1173;
	xor.b32  	%r1175, %r1167, 8;
	or.b32  	%r1176, %r1165, %r1175;
	shl.b32 	%r1177, %r1176, 1;
	add.s32 	%r1178, %r1146, %r1177;
	or.b32  	%r1179, %r1171, %r1175;
	shl.b32 	%r1180, %r1179, 1;
	add.s32 	%r1181, %r1146, %r1180;
	xor.b32  	%r1182, %r1167, 16;
	or.b32  	%r1183, %r1165, %r1182;
	shl.b32 	%r1184, %r1183, 1;
	add.s32 	%r1185, %r1146, %r1184;
	or.b32  	%r1186, %r1171, %r1182;
	shl.b32 	%r1187, %r1186, 1;
	add.s32 	%r1188, %r1146, %r1187;
	xor.b32  	%r1189, %r1167, 24;
	or.b32  	%r1190, %r1165, %r1189;
	shl.b32 	%r1191, %r1190, 1;
	add.s32 	%r1192, %r1146, %r1191;
	or.b32  	%r1193, %r1171, %r1189;
	shl.b32 	%r1194, %r1193, 1;
	add.s32 	%r1195, %r1146, %r1194;
	xor.b32  	%r1196, %r1167, 32;
	or.b32  	%r1197, %r1165, %r1196;
	shl.b32 	%r1198, %r1197, 1;
	add.s32 	%r1199, %r1146, %r1198;
	or.b32  	%r1200, %r1171, %r1196;
	shl.b32 	%r1201, %r1200, 1;
	add.s32 	%r1202, %r1146, %r1201;
	xor.b32  	%r1203, %r1167, 40;
	or.b32  	%r1204, %r1165, %r1203;
	shl.b32 	%r1205, %r1204, 1;
	add.s32 	%r1206, %r1146, %r1205;
	or.b32  	%r1207, %r1171, %r1203;
	shl.b32 	%r1208, %r1207, 1;
	add.s32 	%r1209, %r1146, %r1208;
	xor.b32  	%r1210, %r1167, 48;
	or.b32  	%r1211, %r1165, %r1210;
	shl.b32 	%r1212, %r1211, 1;
	add.s32 	%r1213, %r1146, %r1212;
	or.b32  	%r1214, %r1171, %r1210;
	shl.b32 	%r1215, %r1214, 1;
	add.s32 	%r1216, %r1146, %r1215;
	xor.b32  	%r1217, %r1167, 56;
	or.b32  	%r1218, %r1165, %r1217;
	shl.b32 	%r1219, %r1218, 1;
	add.s32 	%r1220, %r1146, %r1219;
	or.b32  	%r1221, %r1171, %r1217;
	shl.b32 	%r1222, %r1221, 1;
	add.s32 	%r1223, %r1146, %r1222;
	mov.b32 	%r1224, {%rs33, %rs36};
	mov.b32 	%r1225, {%rs38, %rs40};
	mov.b32 	%r1226, {%rs43, %rs46};
	mov.b32 	%r1227, {%rs49, %rs52};
	mov.b32 	%r1228, {%rs55, %rs58};
	mov.b32 	%r1229, {%rs61, %rs64};
	mov.b32 	%r1230, {%rs67, %rs70};
	mov.b32 	%r1231, {%rs73, %rs76};
	mov.b32 	%r1232, {%rs79, %rs82};
	mov.b32 	%r1233, {%rs85, %rs88};
	mov.b32 	%r1234, {%rs91, %rs94};
	mov.b32 	%r1235, {%rs97, %rs100};
	mov.b32 	%r1236, {%rs103, %rs106};
	mov.b32 	%r1237, {%rs109, %rs112};
	mov.b32 	%r1238, {%rs115, %rs118};
	shr.u32 	%r1239, %r4, 3;
	bfe.u32 	%r1240, %r4, 3, 1;
	shl.b32 	%r1241, %r1240, 3;
	or.b32  	%r1242, %r1241, %r1154;
	or.b32  	%r1243, %r1242, %r14;
	shl.b32 	%r1244, %r1243, 6;
	or.b32  	%r1245, %r1244, %r17;
	shl.b32 	%r1246, %r1245, 1;
	add.s32 	%r799, %r1146, %r1246;
	or.b32  	%r1247, %r1244, %r19;
	shl.b32 	%r1248, %r1247, 1;
	add.s32 	%r804, %r1146, %r1248;
	or.b32  	%r1249, %r1244, %r21;
	shl.b32 	%r1250, %r1249, 1;
	add.s32 	%r809, %r1146, %r1250;
	or.b32  	%r1251, %r1244, %r23;
	shl.b32 	%r1252, %r1251, 1;
	add.s32 	%r814, %r1146, %r1252;
	add.s32 	%r819, %r799, 4096;
	add.s32 	%r824, %r804, 4096;
	add.s32 	%r829, %r809, 4096;
	add.s32 	%r834, %r814, 4096;
	.loc	1 116 18
	shl.b32 	%r1253, %r15, 1;
	or.b32  	%r1254, %r1253, %r1153;
	xor.b32  	%r1255, %r1254, %r14;
	shl.b32 	%r1256, %r1240, 9;
	or.b32  	%r1257, %r1256, %r16;
	shl.b32 	%r1258, %r1257, 1;
	shl.b32 	%r1259, %r1255, 4;
	or.b32  	%r1260, %r1259, %r1258;
	add.s32 	%r839, %r1145, %r1260;
	add.s32 	%r844, %r839, 2048;
	add.s32 	%r849, %r839, 4096;
	add.s32 	%r854, %r839, 6144;
	or.b32  	%r1261, %r1254, 4;
	xor.b32  	%r1262, %r1261, %r14;
	shl.b32 	%r1263, %r1262, 3;
	add.s32 	%r1264, %r1263, %r1257;
	shl.b32 	%r1265, %r1264, 1;
	add.s32 	%r859, %r1145, %r1265;
	add.s32 	%r864, %r859, 2048;
	add.s32 	%r869, %r859, 4096;
	add.s32 	%r874, %r859, 6144;
	.loc	1 119 16
	ld.shared.v2.f32 	{%f931, %f932}, [%r1160];
	ld.shared.v2.f32 	{%f933, %f934}, [%r1164];
	ld.shared.v2.f32 	{%f935, %f936}, [%r1160+2304];
	ld.shared.v2.f32 	{%f937, %f938}, [%r1164+2304];
	bar.sync 	0;
	st.shared.v2.f32 	[%r1147], {%f907, %f908};
	st.shared.v2.f32 	[%r1152], {%f909, %f910};
	st.shared.v2.f32 	[%r1147+32], {%f911, %f912};
	st.shared.v2.f32 	[%r1152+32], {%f913, %f914};
	bar.sync 	0;
	ld.shared.v2.f32 	{%f939, %f940}, [%r1160];
	ld.shared.v2.f32 	{%f941, %f942}, [%r1164];
	ld.shared.v2.f32 	{%f943, %f944}, [%r1160+2304];
	ld.shared.v2.f32 	{%f945, %f946}, [%r1164+2304];
	bar.sync 	0;
	st.shared.v2.f32 	[%r1147], {%f915, %f916};
	st.shared.v2.f32 	[%r1152], {%f917, %f918};
	st.shared.v2.f32 	[%r1147+32], {%f919, %f920};
	st.shared.v2.f32 	[%r1152+32], {%f921, %f922};
	bar.sync 	0;
	ld.shared.v2.f32 	{%f947, %f948}, [%r1160];
	ld.shared.v2.f32 	{%f949, %f950}, [%r1164];
	ld.shared.v2.f32 	{%f951, %f952}, [%r1160+2304];
	ld.shared.v2.f32 	{%f953, %f954}, [%r1164+2304];
	bar.sync 	0;
	st.shared.v2.f32 	[%r1147], {%f923, %f924};
	st.shared.v2.f32 	[%r1152], {%f925, %f926};
	st.shared.v2.f32 	[%r1147+32], {%f927, %f928};
	st.shared.v2.f32 	[%r1152+32], {%f929, %f930};
	bar.sync 	0;
	ld.shared.v2.f32 	{%f955, %f956}, [%r1160];
	ld.shared.v2.f32 	{%f957, %f958}, [%r1164];
	ld.shared.v2.f32 	{%f959, %f960}, [%r1160+2304];
	ld.shared.v2.f32 	{%f961, %f962}, [%r1164+2304];
	.loc	1 119 38
	bar.sync 	0;
	st.shared.b32 	[%r1170], %r1224;
	st.shared.b32 	[%r1174], %r1225;
	st.shared.b32 	[%r1178], %r1226;
	st.shared.b32 	[%r1181], %r1227;
	st.shared.b32 	[%r1185], %r1228;
	st.shared.b32 	[%r1188], %r1229;
	st.shared.b32 	[%r1192], %r1230;
	st.shared.b32 	[%r1195], %r1231;
	st.shared.b32 	[%r1199], %r1232;
	st.shared.b32 	[%r1202], %r1233;
	st.shared.b32 	[%r1206], %r1234;
	st.shared.b32 	[%r1209], %r1235;
	st.shared.b32 	[%r1213], %r1236;
	st.shared.b32 	[%r1216], %r1237;
	mov.b32 	%r1266, 0;
	st.shared.u32 	[%r1220], %r1266;
	st.shared.b32 	[%r1223], %r1238;
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r875, %r876, %r877, %r878 }, [ %r799 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r923, %r924, %r925, %r926 }, [ %r804 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r971, %r972, %r973, %r974 }, [ %r809 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1019, %r1020, %r1021, %r1022 }, [ %r814 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r899, %r900, %r901, %r902 }, [ %r819 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r947, %r948, %r949, %r950 }, [ %r824 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r995, %r996, %r997, %r998 }, [ %r829 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1043, %r1044, %r1045, %r1046 }, [ %r834 + 0 ];
	// end inline asm
	.loc	1 116 18
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r879, %r880, %r885, %r886 }, [ %r839 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r927, %r928, %r933, %r934 }, [ %r844 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r975, %r976, %r981, %r982 }, [ %r849 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1023, %r1024, %r1029, %r1030 }, [ %r854 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r891, %r892, %r897, %r898 }, [ %r859 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r939, %r940, %r945, %r946 }, [ %r864 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r987, %r988, %r993, %r994 }, [ %r869 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1035, %r1036, %r1041, %r1042 }, [ %r874 + 0 ];
	// end inline asm
	mov.f32 	%f766, 0f00000000;
	.loc	1 119 50
	mov.f32 	%f707, %f766;
	mov.f32 	%f708, %f766;
	mov.f32 	%f709, %f766;
	mov.f32 	%f710, %f766;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f707, %f708, %f709, %f710 }, { %r875, %r876, %r877, %r878 }, { %r879, %r880 }, { %f707, %f708, %f709, %f710 };
	// end inline asm
	mov.f32 	%f715, %f766;
	mov.f32 	%f716, %f766;
	mov.f32 	%f717, %f766;
	mov.f32 	%f718, %f766;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f715, %f716, %f717, %f718 }, { %r875, %r876, %r877, %r878 }, { %r885, %r886 }, { %f715, %f716, %f717, %f718 };
	// end inline asm
	mov.f32 	%f723, %f766;
	mov.f32 	%f724, %f766;
	mov.f32 	%f725, %f766;
	mov.f32 	%f726, %f766;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f723, %f724, %f725, %f726 }, { %r875, %r876, %r877, %r878 }, { %r891, %r892 }, { %f723, %f724, %f725, %f726 };
	// end inline asm
	mov.f32 	%f731, %f766;
	mov.f32 	%f732, %f766;
	mov.f32 	%f733, %f766;
	mov.f32 	%f734, %f766;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f731, %f732, %f733, %f734 }, { %r875, %r876, %r877, %r878 }, { %r897, %r898 }, { %f731, %f732, %f733, %f734 };
	// end inline asm
	mov.f32 	%f739, %f766;
	mov.f32 	%f740, %f766;
	mov.f32 	%f741, %f766;
	mov.f32 	%f742, %f766;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f739, %f740, %f741, %f742 }, { %r899, %r900, %r901, %r902 }, { %r879, %r880 }, { %f739, %f740, %f741, %f742 };
	// end inline asm
	mov.f32 	%f747, %f766;
	mov.f32 	%f748, %f766;
	mov.f32 	%f749, %f766;
	mov.f32 	%f750, %f766;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f747, %f748, %f749, %f750 }, { %r899, %r900, %r901, %r902 }, { %r885, %r886 }, { %f747, %f748, %f749, %f750 };
	// end inline asm
	mov.f32 	%f755, %f766;
	mov.f32 	%f756, %f766;
	mov.f32 	%f757, %f766;
	mov.f32 	%f758, %f766;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f755, %f756, %f757, %f758 }, { %r899, %r900, %r901, %r902 }, { %r891, %r892 }, { %f755, %f756, %f757, %f758 };
	// end inline asm
	mov.f32 	%f763, %f766;
	mov.f32 	%f764, %f766;
	mov.f32 	%f765, %f766;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f763, %f764, %f765, %f766 }, { %r899, %r900, %r901, %r902 }, { %r897, %r898 }, { %f763, %f764, %f765, %f766 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f707, %f708, %f709, %f710 }, { %r923, %r924, %r925, %r926 }, { %r927, %r928 }, { %f707, %f708, %f709, %f710 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f715, %f716, %f717, %f718 }, { %r923, %r924, %r925, %r926 }, { %r933, %r934 }, { %f715, %f716, %f717, %f718 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f723, %f724, %f725, %f726 }, { %r923, %r924, %r925, %r926 }, { %r939, %r940 }, { %f723, %f724, %f725, %f726 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f731, %f732, %f733, %f734 }, { %r923, %r924, %r925, %r926 }, { %r945, %r946 }, { %f731, %f732, %f733, %f734 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f739, %f740, %f741, %f742 }, { %r947, %r948, %r949, %r950 }, { %r927, %r928 }, { %f739, %f740, %f741, %f742 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f747, %f748, %f749, %f750 }, { %r947, %r948, %r949, %r950 }, { %r933, %r934 }, { %f747, %f748, %f749, %f750 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f755, %f756, %f757, %f758 }, { %r947, %r948, %r949, %r950 }, { %r939, %r940 }, { %f755, %f756, %f757, %f758 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f763, %f764, %f765, %f766 }, { %r947, %r948, %r949, %r950 }, { %r945, %r946 }, { %f763, %f764, %f765, %f766 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f707, %f708, %f709, %f710 }, { %r971, %r972, %r973, %r974 }, { %r975, %r976 }, { %f707, %f708, %f709, %f710 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f715, %f716, %f717, %f718 }, { %r971, %r972, %r973, %r974 }, { %r981, %r982 }, { %f715, %f716, %f717, %f718 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f723, %f724, %f725, %f726 }, { %r971, %r972, %r973, %r974 }, { %r987, %r988 }, { %f723, %f724, %f725, %f726 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f731, %f732, %f733, %f734 }, { %r971, %r972, %r973, %r974 }, { %r993, %r994 }, { %f731, %f732, %f733, %f734 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f739, %f740, %f741, %f742 }, { %r995, %r996, %r997, %r998 }, { %r975, %r976 }, { %f739, %f740, %f741, %f742 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f747, %f748, %f749, %f750 }, { %r995, %r996, %r997, %r998 }, { %r981, %r982 }, { %f747, %f748, %f749, %f750 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f755, %f756, %f757, %f758 }, { %r995, %r996, %r997, %r998 }, { %r987, %r988 }, { %f755, %f756, %f757, %f758 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f763, %f764, %f765, %f766 }, { %r995, %r996, %r997, %r998 }, { %r993, %r994 }, { %f763, %f764, %f765, %f766 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f707, %f708, %f709, %f710 }, { %r1019, %r1020, %r1021, %r1022 }, { %r1023, %r1024 }, { %f707, %f708, %f709, %f710 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f715, %f716, %f717, %f718 }, { %r1019, %r1020, %r1021, %r1022 }, { %r1029, %r1030 }, { %f715, %f716, %f717, %f718 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f723, %f724, %f725, %f726 }, { %r1019, %r1020, %r1021, %r1022 }, { %r1035, %r1036 }, { %f723, %f724, %f725, %f726 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f731, %f732, %f733, %f734 }, { %r1019, %r1020, %r1021, %r1022 }, { %r1041, %r1042 }, { %f731, %f732, %f733, %f734 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f739, %f740, %f741, %f742 }, { %r1043, %r1044, %r1045, %r1046 }, { %r1023, %r1024 }, { %f739, %f740, %f741, %f742 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f747, %f748, %f749, %f750 }, { %r1043, %r1044, %r1045, %r1046 }, { %r1029, %r1030 }, { %f747, %f748, %f749, %f750 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f755, %f756, %f757, %f758 }, { %r1043, %r1044, %r1045, %r1046 }, { %r1035, %r1036 }, { %f755, %f756, %f757, %f758 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f763, %f764, %f765, %f766 }, { %r1043, %r1044, %r1045, %r1046 }, { %r1041, %r1042 }, { %f763, %f764, %f765, %f766 };
	// end inline asm
	.loc	1 119 24
	fma.rn.f32 	%f963, %f707, %f129, %f931;
	fma.rn.f32 	%f964, %f708, %f129, %f932;
	fma.rn.f32 	%f965, %f709, %f129, %f933;
	fma.rn.f32 	%f966, %f710, %f129, %f934;
	fma.rn.f32 	%f967, %f715, %f129, %f939;
	fma.rn.f32 	%f968, %f716, %f129, %f940;
	fma.rn.f32 	%f969, %f717, %f129, %f941;
	fma.rn.f32 	%f970, %f718, %f129, %f942;
	fma.rn.f32 	%f971, %f723, %f129, %f947;
	fma.rn.f32 	%f972, %f724, %f129, %f948;
	fma.rn.f32 	%f973, %f725, %f129, %f949;
	fma.rn.f32 	%f974, %f726, %f129, %f950;
	fma.rn.f32 	%f975, %f731, %f129, %f955;
	fma.rn.f32 	%f976, %f732, %f129, %f956;
	fma.rn.f32 	%f977, %f733, %f129, %f957;
	fma.rn.f32 	%f978, %f734, %f129, %f958;
	fma.rn.f32 	%f979, %f739, %f129, %f935;
	fma.rn.f32 	%f980, %f740, %f129, %f936;
	fma.rn.f32 	%f981, %f741, %f129, %f937;
	fma.rn.f32 	%f982, %f742, %f129, %f938;
	fma.rn.f32 	%f983, %f747, %f129, %f943;
	fma.rn.f32 	%f984, %f748, %f129, %f944;
	fma.rn.f32 	%f985, %f749, %f129, %f945;
	fma.rn.f32 	%f986, %f750, %f129, %f946;
	fma.rn.f32 	%f987, %f755, %f129, %f951;
	fma.rn.f32 	%f988, %f756, %f129, %f952;
	fma.rn.f32 	%f989, %f757, %f129, %f953;
	fma.rn.f32 	%f990, %f758, %f129, %f954;
	fma.rn.f32 	%f991, %f763, %f129, %f959;
	fma.rn.f32 	%f992, %f764, %f129, %f960;
	fma.rn.f32 	%f993, %f765, %f129, %f961;
	fma.rn.f32 	%f994, %f766, %f129, %f962;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs119, %f964;
	cvt.rn.f16.f32 	%rs120, %f963;
	mov.b32 	%r1267, {%rs120, %rs119};
	cvt.rn.f16.f32 	%rs121, %f966;
	cvt.rn.f16.f32 	%rs122, %f965;
	mov.b32 	%r1268, {%rs122, %rs121};
	cvt.rn.f16.f32 	%rs123, %f968;
	cvt.rn.f16.f32 	%rs124, %f967;
	mov.b32 	%r1269, {%rs124, %rs123};
	cvt.rn.f16.f32 	%rs125, %f970;
	cvt.rn.f16.f32 	%rs126, %f969;
	mov.b32 	%r1270, {%rs126, %rs125};
	cvt.rn.f16.f32 	%rs127, %f972;
	cvt.rn.f16.f32 	%rs128, %f971;
	mov.b32 	%r1271, {%rs128, %rs127};
	cvt.rn.f16.f32 	%rs129, %f974;
	cvt.rn.f16.f32 	%rs130, %f973;
	mov.b32 	%r1272, {%rs130, %rs129};
	cvt.rn.f16.f32 	%rs131, %f976;
	cvt.rn.f16.f32 	%rs132, %f975;
	mov.b32 	%r1273, {%rs132, %rs131};
	cvt.rn.f16.f32 	%rs133, %f978;
	cvt.rn.f16.f32 	%rs134, %f977;
	mov.b32 	%r1274, {%rs134, %rs133};
	cvt.rn.f16.f32 	%rs135, %f980;
	cvt.rn.f16.f32 	%rs136, %f979;
	mov.b32 	%r1275, {%rs136, %rs135};
	cvt.rn.f16.f32 	%rs137, %f982;
	cvt.rn.f16.f32 	%rs138, %f981;
	mov.b32 	%r1276, {%rs138, %rs137};
	cvt.rn.f16.f32 	%rs139, %f984;
	cvt.rn.f16.f32 	%rs140, %f983;
	mov.b32 	%r1277, {%rs140, %rs139};
	cvt.rn.f16.f32 	%rs141, %f986;
	cvt.rn.f16.f32 	%rs142, %f985;
	mov.b32 	%r1278, {%rs142, %rs141};
	cvt.rn.f16.f32 	%rs143, %f988;
	cvt.rn.f16.f32 	%rs144, %f987;
	mov.b32 	%r1279, {%rs144, %rs143};
	cvt.rn.f16.f32 	%rs145, %f990;
	cvt.rn.f16.f32 	%rs146, %f989;
	mov.b32 	%r1280, {%rs146, %rs145};
	cvt.rn.f16.f32 	%rs147, %f992;
	cvt.rn.f16.f32 	%rs148, %f991;
	mov.b32 	%r1281, {%rs148, %rs147};
	cvt.rn.f16.f32 	%rs149, %f994;
	cvt.rn.f16.f32 	%rs150, %f993;
	mov.b32 	%r1282, {%rs150, %rs149};
	.loc	1 120 18
	add.s64 	%rd114, %rd119, %rd125;
	add.s64 	%rd115, %rd119, %rd126;
	add.s64 	%rd116, %rd119, %rd127;
	add.s64 	%rd117, %rd119, %rd128;
	bar.sync 	0;
	mad.lo.s32 	%r1283, %r1155, 54, %r1158;
	shl.b32 	%r1284, %r1283, 1;
	add.s32 	%r1285, %r1145, %r1284;
	st.shared.b32 	[%r1285], %r1267;
	mad.lo.s32 	%r1286, %r1161, 54, %r1162;
	shl.b32 	%r1287, %r1286, 1;
	add.s32 	%r1288, %r1145, %r1287;
	st.shared.b32 	[%r1288], %r1268;
	mad.lo.s32 	%r1289, %r1155, 72, %r1157;
	shl.b32 	%r1290, %r1289, 1;
	add.s32 	%r1291, %r1145, %r1290;
	st.shared.b32 	[%r1291+32], %r1269;
	mad.lo.s32 	%r1292, %r1161, 72, %r1157;
	shl.b32 	%r1293, %r1292, 1;
	add.s32 	%r1294, %r1145, %r1293;
	st.shared.b32 	[%r1294+32], %r1270;
	st.shared.b32 	[%r1291+64], %r1271;
	st.shared.b32 	[%r1294+64], %r1272;
	st.shared.b32 	[%r1291+96], %r1273;
	st.shared.b32 	[%r1294+96], %r1274;
	bar.sync 	0;
	shl.b32 	%r1295, %r1139, 2;
	or.b32  	%r1296, %r1295, %r1239;
	shl.b32 	%r1297, %r14, 3;
	mad.lo.s32 	%r1298, %r1296, 72, %r1297;
	shl.b32 	%r1299, %r1298, 1;
	add.s32 	%r1300, %r1145, %r1299;
	ld.shared.v4.u32 	{%r1067, %r1068, %r1069, %r1070}, [%r1300];
	ld.shared.v4.u32 	{%r1071, %r1072, %r1073, %r1074}, [%r1300+2304];
	bar.sync 	0;
	st.shared.b32 	[%r1285], %r1275;
	st.shared.b32 	[%r1288], %r1276;
	st.shared.b32 	[%r1291+32], %r1277;
	st.shared.b32 	[%r1294+32], %r1278;
	st.shared.b32 	[%r1291+64], %r1279;
	st.shared.b32 	[%r1294+64], %r1280;
	st.shared.b32 	[%r1291+96], %r1281;
	st.shared.b32 	[%r1294+96], %r1282;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1075, %r1076, %r1077, %r1078}, [%r1300];
	ld.shared.v4.u32 	{%r1079, %r1080, %r1081, %r1082}, [%r1300+2304];
	// begin inline asm
	@%p59 st.global.v4.b32 [ %rd114 + 0 ], { %r1067, %r1068, %r1069, %r1070 };
	// end inline asm
	// begin inline asm
	@%p60 st.global.v4.b32 [ %rd115 + 0 ], { %r1071, %r1072, %r1073, %r1074 };
	// end inline asm
	// begin inline asm
	@%p61 st.global.v4.b32 [ %rd116 + 0 ], { %r1075, %r1076, %r1077, %r1078 };
	// end inline asm
	// begin inline asm
	@%p62 st.global.v4.b32 [ %rd117 + 0 ], { %r1079, %r1080, %r1081, %r1082 };
	// end inline asm
	.loc	1 120 4
	ret;
$L__tmp1:
$L__func_end0:

}
	.file	1 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\fla\\ops\\common\\chunk_o.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 0
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 151
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 104
.b8 117
.b8 110
.b8 107
.b8 95
.b8 111
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 68
.b8 58
.b8 92
.b8 85
.b8 115
.b8 101
.b8 114
.b8 115
.b8 92
.b8 76
.b8 111
.b8 117
.b8 105
.b8 115
.b8 92
.b8 80
.b8 121
.b8 99
.b8 104
.b8 97
.b8 114
.b8 109
.b8 80
.b8 114
.b8 111
.b8 106
.b8 101
.b8 99
.b8 116
.b8 115
.b8 92
.b8 77
.b8 97
.b8 115
.b8 116
.b8 101
.b8 114
.b8 95
.b8 116
.b8 104
.b8 101
.b8 115
.b8 105
.b8 115
.b8 92
.b8 66
.b8 97
.b8 98
.b8 105
.b8 108
.b8 111
.b8 110
.b8 103
.b8 95
.b8 66
.b8 101
.b8 110
.b8 99
.b8 104
.b8 109
.b8 97
.b8 114
.b8 107
.b8 92
.b8 46
.b8 118
.b8 101
.b8 110
.b8 118
.b8 92
.b8 76
.b8 105
.b8 98
.b8 92
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 92
.b8 102
.b8 108
.b8 97
.b8 92
.b8 111
.b8 112
.b8 115
.b8 92
.b8 99
.b8 111
.b8 109
.b8 109
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
	}
	.section	.debug_loc	{	}
