//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_86
.address_size 64

	// .globl	causal_conv1d_fwd_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry causal_conv1d_fwd_kernel(
	.param .u64 causal_conv1d_fwd_kernel_param_0,
	.param .u64 causal_conv1d_fwd_kernel_param_1,
	.param .u64 causal_conv1d_fwd_kernel_param_2,
	.param .u64 causal_conv1d_fwd_kernel_param_3,
	.param .u64 causal_conv1d_fwd_kernel_param_4,
	.param .u32 causal_conv1d_fwd_kernel_param_5
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<26>;
	.reg .b16 	%rs<29>;
	.reg .b32 	%r<63>;
	.reg .f32 	%f<90>;
	.reg .b64 	%rd<45>;
	.loc	1 47 0
$L__func_begin0:
	.loc	1 47 0

	ld.param.u64 	%rd11, [causal_conv1d_fwd_kernel_param_0];
	ld.param.u64 	%rd12, [causal_conv1d_fwd_kernel_param_1];
$L__tmp0:
	.loc	1 71 34
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	ld.param.u64 	%rd13, [causal_conv1d_fwd_kernel_param_2];
	.loc	1 71 52
	// begin inline asm
	mov.u32 %r2, %ctaid.y;
	// end inline asm
	.loc	1 74 49
	shl.b32 	%r33, %r2, 1;
	ld.param.u64 	%rd14, [causal_conv1d_fwd_kernel_param_3];
	ld.param.u64 	%rd15, [causal_conv1d_fwd_kernel_param_4];
	.loc	1 74 43
	mul.wide.s32 	%rd16, %r33, 4;
	add.s64 	%rd1, %rd15, %rd16;
	mov.pred 	%p1, -1;
	.loc	1 74 27
	// begin inline asm
	mov.u32 %r3, 0x0;
	@%p1 ld.global.b32 { %r3 }, [ %rd1 + 0 ];
	// end inline asm
	.loc	1 74 100
	add.s64 	%rd2, %rd1, 4;
	.loc	1 74 74
	// begin inline asm
	mov.u32 %r4, 0x0;
	@%p1 ld.global.b32 { %r4 }, [ %rd2 + 0 ];
	// end inline asm
	.loc	1 75 40
	mul.wide.s32 	%rd17, %r3, 4;
	add.s64 	%rd3, %rd14, %rd17;
	.loc	1 75 27
	// begin inline asm
	mov.u32 %r5, 0x0;
	@%p1 ld.global.b32 { %r5 }, [ %rd3 + 0 ];
	// end inline asm
	.loc	1 75 48
	cvt.s64.s32 	%rd18, %r5;
	.loc	1 75 86
	add.s64 	%rd4, %rd3, 4;
	.loc	1 75 67
	// begin inline asm
	mov.u32 %r6, 0x0;
	@%p1 ld.global.b32 { %r6 }, [ %rd4 + 0 ];
	// end inline asm
	.loc	1 75 92
	cvt.s64.s32 	%rd19, %r6;
	.loc	1 76 18
	sub.s64 	%rd20, %rd19, %rd18;
	.loc	1 81 34
	mov.u32 	%r34, %tid.x;
	and.b32  	%r35, %r34, 127;
	shl.b32 	%r36, %r34, 2;
	.loc	1 93 47
	mul.wide.s32 	%rd21, %r5, 2048;
	.loc	1 93 41
	shl.b64 	%rd22, %rd21, 1;
	add.s64 	%rd23, %rd11, %rd22;
	.loc	1 97 24
	shl.b32 	%r37, %r35, 2;
	mov.u32 	%r38, global_smem;
	add.s32 	%r39, %r38, %r37;
	.loc	1 93 73
	shl.b32 	%r40, %r4, 2;
	shl.b32 	%r41, %r1, 7;
	.loc	1 81 21
	or.b32  	%r42, %r41, %r35;
	.loc	1 82 23
	and.b32  	%r43, %r36, 124;
	bfe.u32 	%r44, %r34, 5, 2;
	cvt.u64.u32 	%rd24, %r44;
	.loc	1 83 16
	setp.lt.s32 	%p5, %r42, 2048;
	.loc	1 88 46
	shl.b32 	%r45, %r42, 2;
	.loc	1 88 31
	mul.wide.s32 	%rd25, %r45, 2;
	add.s64 	%rd5, %rd13, %rd25;
	mov.b32 	%r9, 0;
	.loc	1 88 22
	// begin inline asm
	mov.u32 %r7, 0x0;
	mov.u32 %r8, 0x0;
	@%p5 ld.global.v2.b32 { %r7, %r8 }, [ %rd5 + 0 ];
	@!%p5 mov.u32 %r7, %r9;
	@!%p5 mov.u32 %r8, %r9;
	// end inline asm
	cvt.u16.u32 	%rs1, %r7;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs3}, %r7; }
	cvt.u16.u32 	%rs5, %r8;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs7}, %r8; }
	.loc	1 88 92
	cvt.f32.f16 	%f9, %rs1;
	cvt.f32.f16 	%f10, %rs3;
	cvt.f32.f16 	%f11, %rs5;
	cvt.f32.f16 	%f12, %rs7;
	.loc	1 93 78
	add.s32 	%r46, %r40, -3;
	.loc	1 93 104
	cvt.s64.s32 	%rd26, %r46;
	.loc	1 95 27
	add.s64 	%rd27, %rd26, %rd24;
	shl.b64 	%rd28, %rd27, 12;
	add.s64 	%rd29, %rd23, %rd28;
	or.b32  	%r47, %r40, %r44;
	or.b32  	%r48, %r41, %r43;
	cvt.s64.s32 	%rd30, %r48;
	mul.wide.s32 	%rd31, %r48, 2;
	add.s64 	%rd6, %rd29, %rd31;
	setp.gt.s64 	%p13, %rd27, -1;
	setp.lt.s64 	%p14, %rd27, %rd20;
	and.pred  	%p15, %p13, %p14;
	setp.lt.u32 	%p16, %r48, 2048;
	and.pred  	%p8, %p16, %p15;
	// begin inline asm
	mov.u32 %r11, 0x0;
	mov.u32 %r12, 0x0;
	@%p8 ld.global.v2.b32 { %r11, %r12 }, [ %rd6 + 0 ];
	// end inline asm
	.loc	1 97 38
	mul.f32 	%f13, %f10, 0f00000000;
$L__tmp1:
	.loc	2 256 15
	fma.rn.f32 	%f14, %f10, 0f00000000, %f9;
	fma.rn.f32 	%f15, %f11, 0f00000000, %f14;
	fma.rn.f32 	%f16, %f12, 0f00000000, %f15;
$L__tmp2:
	.loc	1 97 24
	st.shared.f32 	[%r39], %f16;
	bar.sync 	0;
	shl.b32 	%r51, %r43, 2;
	add.s32 	%r52, %r38, %r51;
	.loc	1 93 78
	add.s32 	%r53, %r40, -2;
	.loc	1 93 104
	cvt.s64.s32 	%rd32, %r53;
	.loc	1 95 27
	add.s64 	%rd33, %rd32, %rd24;
	shl.b64 	%rd34, %rd33, 12;
	add.s64 	%rd35, %rd23, %rd34;
	add.s64 	%rd7, %rd35, %rd31;
	setp.gt.s64 	%p17, %rd33, -1;
	setp.lt.s64 	%p18, %rd33, %rd20;
	and.pred  	%p19, %p17, %p18;
	and.pred  	%p9, %p16, %p19;
$L__tmp3:
	.loc	2 256 15
	fma.rn.f32 	%f17, %f9, 0f00000000, %f10;
	fma.rn.f32 	%f18, %f11, 0f00000000, %f17;
	fma.rn.f32 	%f19, %f12, 0f00000000, %f18;
$L__tmp4:
	.loc	1 93 78
	add.s32 	%r54, %r40, -1;
	.loc	1 93 104
	cvt.s64.s32 	%rd36, %r54;
	.loc	1 95 27
	add.s64 	%rd37, %rd36, %rd24;
	shl.b64 	%rd38, %rd37, 12;
	add.s64 	%rd39, %rd23, %rd38;
	add.s64 	%rd8, %rd39, %rd31;
	setp.gt.s64 	%p20, %rd37, -1;
	setp.lt.s64 	%p21, %rd37, %rd20;
	and.pred  	%p22, %p20, %p21;
	and.pred  	%p10, %p16, %p22;
$L__tmp5:
	.loc	2 256 15
	fma.rn.f32 	%f20, %f9, 0f00000000, %f13;
	add.f32 	%f21, %f20, %f11;
	fma.rn.f32 	%f22, %f12, 0f00000000, %f21;
$L__tmp6:
	.loc	1 95 27
	cvt.s64.s32 	%rd40, %r47;
	mul.wide.s32 	%rd41, %r47, 2048;
	add.s64 	%rd42, %rd41, %rd30;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd9, %rd23, %rd43;
	setp.gt.s32 	%p23, %r47, -1;
	setp.gt.s64 	%p24, %rd20, %rd40;
	and.pred  	%p25, %p23, %p24;
	and.pred  	%p11, %p16, %p25;
$L__tmp7:
	.loc	2 256 15
	fma.rn.f32 	%f23, %f11, 0f00000000, %f20;
	add.f32 	%f24, %f23, %f12;
$L__tmp8:
	.loc	1 134 32
	add.s64 	%rd44, %rd12, %rd22;
	.loc	1 95 59
	mov.b32 	{%rs9, %rs10}, %r12;
	cvt.f32.f16 	%f25, %rs10;
	cvt.f32.f16 	%f26, %rs9;
	.loc	1 97 24
	ld.shared.v4.f32 	{%f27, %f28, %f29, %f30}, [%r52];
	.loc	1 98 19
	fma.rn.f32 	%f31, %f29, %f26, 0f00000000;
	fma.rn.f32 	%f32, %f30, %f25, 0f00000000;
	.loc	1 95 59
	mov.b32 	{%rs11, %rs12}, %r11;
	cvt.f32.f16 	%f33, %rs12;
	cvt.f32.f16 	%f34, %rs11;
	.loc	1 98 19
	fma.rn.f32 	%f35, %f27, %f34, 0f00000000;
	fma.rn.f32 	%f36, %f28, %f33, 0f00000000;
	.loc	1 95 27
	// begin inline asm
	mov.u32 %r13, 0x0;
	mov.u32 %r14, 0x0;
	@%p9 ld.global.v2.b32 { %r13, %r14 }, [ %rd7 + 0 ];
	// end inline asm
	.loc	1 95 59
	mov.b32 	{%rs13, %rs14}, %r13;
	cvt.f32.f16 	%f37, %rs13;
	cvt.f32.f16 	%f38, %rs14;
	mov.b32 	{%rs15, %rs16}, %r14;
	cvt.f32.f16 	%f39, %rs15;
	cvt.f32.f16 	%f40, %rs16;
	.loc	1 97 24
	bar.sync 	0;
	st.shared.f32 	[%r39], %f19;
	bar.sync 	0;
	ld.shared.v4.f32 	{%f41, %f42, %f43, %f44}, [%r52];
	.loc	1 98 19
	fma.rn.f32 	%f45, %f42, %f38, %f36;
	fma.rn.f32 	%f46, %f41, %f37, %f35;
	fma.rn.f32 	%f47, %f44, %f40, %f32;
	fma.rn.f32 	%f48, %f43, %f39, %f31;
	.loc	1 95 27
	// begin inline asm
	mov.u32 %r15, 0x0;
	mov.u32 %r16, 0x0;
	@%p10 ld.global.v2.b32 { %r15, %r16 }, [ %rd8 + 0 ];
	// end inline asm
	.loc	1 95 59
	mov.b32 	{%rs17, %rs18}, %r15;
	cvt.f32.f16 	%f49, %rs18;
	cvt.f32.f16 	%f50, %rs17;
	mov.b32 	{%rs19, %rs20}, %r16;
	cvt.f32.f16 	%f51, %rs20;
	cvt.f32.f16 	%f52, %rs19;
	.loc	1 97 24
	bar.sync 	0;
	st.shared.f32 	[%r39], %f22;
	bar.sync 	0;
	ld.shared.v4.f32 	{%f53, %f54, %f55, %f56}, [%r52];
	.loc	1 98 19
	fma.rn.f32 	%f57, %f53, %f50, %f46;
	fma.rn.f32 	%f58, %f54, %f49, %f45;
	fma.rn.f32 	%f59, %f55, %f52, %f48;
	fma.rn.f32 	%f60, %f56, %f51, %f47;
	.loc	1 95 27
	// begin inline asm
	mov.u32 %r17, 0x0;
	mov.u32 %r18, 0x0;
	@%p11 ld.global.v2.b32 { %r17, %r18 }, [ %rd9 + 0 ];
	// end inline asm
	.loc	1 95 59
	mov.b32 	{%rs21, %rs22}, %r17;
	cvt.f32.f16 	%f61, %rs21;
	cvt.f32.f16 	%f62, %rs22;
	mov.b32 	{%rs23, %rs24}, %r18;
	cvt.f32.f16 	%f63, %rs23;
	cvt.f32.f16 	%f64, %rs24;
	.loc	1 97 24
	bar.sync 	0;
	st.shared.f32 	[%r39], %f24;
	bar.sync 	0;
	ld.shared.v4.f32 	{%f65, %f66, %f67, %f68}, [%r52];
	.loc	1 98 19
	fma.rn.f32 	%f69, %f66, %f62, %f58;
	fma.rn.f32 	%f70, %f65, %f61, %f57;
	fma.rn.f32 	%f71, %f68, %f64, %f60;
	fma.rn.f32 	%f72, %f67, %f63, %f59;
	mov.f32 	%f73, 0f00000000;
$L__tmp9:
	.loc	2 51 30
	sub.f32 	%f74, %f73, %f70;
	sub.f32 	%f75, %f73, %f69;
	sub.f32 	%f76, %f73, %f72;
	sub.f32 	%f77, %f73, %f71;
	.loc	2 51 29
	mul.f32 	%f2, %f74, 0f3FB8AA3B;
	// begin inline asm
	ex2.approx.f32 %f1, %f2;
	// end inline asm
	mul.f32 	%f4, %f75, 0f3FB8AA3B;
	// begin inline asm
	ex2.approx.f32 %f3, %f4;
	// end inline asm
	mul.f32 	%f6, %f76, 0f3FB8AA3B;
	// begin inline asm
	ex2.approx.f32 %f5, %f6;
	// end inline asm
	mul.f32 	%f8, %f77, 0f3FB8AA3B;
	// begin inline asm
	ex2.approx.f32 %f7, %f8;
	// end inline asm
	.loc	2 51 20
	add.f32 	%f78, %f1, 0f3F800000;
	add.f32 	%f79, %f3, 0f3F800000;
	add.f32 	%f80, %f5, 0f3F800000;
	add.f32 	%f81, %f7, 0f3F800000;
	.loc	2 51 16
	mov.b32 	%r21, %f78;
	mov.b32 	%r20, 1065353216;
	// begin inline asm
	div.full.f32 %r19, %r20, %r21;
	// end inline asm
	mov.b32 	%f82, %r19;
	mov.b32 	%r24, %f79;
	// begin inline asm
	div.full.f32 %r22, %r20, %r24;
	// end inline asm
	mov.b32 	%f83, %r22;
	mov.b32 	%r27, %f80;
	// begin inline asm
	div.full.f32 %r25, %r20, %r27;
	// end inline asm
	mov.b32 	%f84, %r25;
	mov.b32 	%r30, %f81;
	// begin inline asm
	div.full.f32 %r28, %r20, %r30;
	// end inline asm
	mov.b32 	%f85, %r28;
$L__tmp10:
	.loc	1 127 20
	mul.f32 	%f86, %f70, %f82;
	mul.f32 	%f87, %f69, %f83;
	mul.f32 	%f88, %f72, %f84;
	mul.f32 	%f89, %f71, %f85;
	.loc	1 135 26
	cvt.rn.f16.f32 	%rs25, %f87;
	cvt.rn.f16.f32 	%rs26, %f86;
	mov.b32 	%r61, {%rs26, %rs25};
	cvt.rn.f16.f32 	%rs27, %f89;
	cvt.rn.f16.f32 	%rs28, %f88;
	mov.b32 	%r62, {%rs28, %rs27};
	.loc	1 135 18
	add.s64 	%rd10, %rd44, %rd43;
	// begin inline asm
	@%p11 st.global.v2.b32 [ %rd10 + 0 ], { %r61, %r62 };
	// end inline asm
	.loc	1 135 4
	ret;
$L__tmp11:
$L__func_end0:

}
	.file	1 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\fla\\modules\\convolution.py"
	.file	2 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\triton\\language\\standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 250
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 111
.b8 110
.b8 118
.b8 111
.b8 108
.b8 117
.b8 116
.b8 105
.b8 111
.b8 110
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 68
.b8 58
.b8 92
.b8 85
.b8 115
.b8 101
.b8 114
.b8 115
.b8 92
.b8 76
.b8 111
.b8 117
.b8 105
.b8 115
.b8 92
.b8 80
.b8 121
.b8 99
.b8 104
.b8 97
.b8 114
.b8 109
.b8 80
.b8 114
.b8 111
.b8 106
.b8 101
.b8 99
.b8 116
.b8 115
.b8 92
.b8 77
.b8 97
.b8 115
.b8 116
.b8 101
.b8 114
.b8 95
.b8 116
.b8 104
.b8 101
.b8 115
.b8 105
.b8 115
.b8 92
.b8 66
.b8 97
.b8 98
.b8 105
.b8 108
.b8 111
.b8 110
.b8 103
.b8 95
.b8 66
.b8 101
.b8 110
.b8 99
.b8 104
.b8 109
.b8 97
.b8 114
.b8 107
.b8 92
.b8 46
.b8 118
.b8 101
.b8 110
.b8 118
.b8 92
.b8 76
.b8 105
.b8 98
.b8 92
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 92
.b8 102
.b8 108
.b8 97
.b8 92
.b8 109
.b8 111
.b8 100
.b8 117
.b8 108
.b8 101
.b8 115
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 99
.b8 97
.b8 117
.b8 115
.b8 97
.b8 108
.b8 95
.b8 99
.b8 111
.b8 110
.b8 118
.b8 49
.b8 100
.b8 95
.b8 102
.b8 119
.b8 100
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 156
.b8 4
.b32 156
.b64 $L__tmp1
.b64 $L__tmp8
.b8 1
.b8 97
.b8 61
.b8 4
.b32 156
.b64 $L__tmp9
.b64 $L__tmp10
.b8 1
.b8 127
.b8 31
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
