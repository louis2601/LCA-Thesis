//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_86
.address_size 64

	// .globl	chunk_fwd_kernel_o
.extern .shared .align 16 .b8 global_smem[];

.visible .entry chunk_fwd_kernel_o(
	.param .u64 chunk_fwd_kernel_o_param_0,
	.param .u64 chunk_fwd_kernel_o_param_1,
	.param .u64 chunk_fwd_kernel_o_param_2,
	.param .u64 chunk_fwd_kernel_o_param_3,
	.param .u64 chunk_fwd_kernel_o_param_4,
	.param .u64 chunk_fwd_kernel_o_param_5,
	.param .u64 chunk_fwd_kernel_o_param_6,
	.param .f32 chunk_fwd_kernel_o_param_7,
	.param .u32 chunk_fwd_kernel_o_param_8
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<63>;
	.reg .b16 	%rs<97>;
	.reg .b32 	%r<1515>;
	.reg .f32 	%f<1154>;
	.reg .b64 	%rd<91>;
	.loc	1 33 0
$L__func_begin0:
	.loc	1 33 0

	ld.param.u64 	%rd29, [chunk_fwd_kernel_o_param_0];
$L__tmp0:
	.loc	1 55 35
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	ld.param.u64 	%rd30, [chunk_fwd_kernel_o_param_1];
	ld.param.u64 	%rd31, [chunk_fwd_kernel_o_param_2];
	.loc	1 55 53
	// begin inline asm
	mov.u32 %r2, %ctaid.y;
	// end inline asm
	ld.param.u64 	%rd32, [chunk_fwd_kernel_o_param_3];
	.loc	1 55 71
	// begin inline asm
	mov.u32 %r3, %ctaid.z;
	// end inline asm
	.loc	1 56 33
	shr.s32 	%r1212, %r3, 31;
	shr.u32 	%r1213, %r1212, 28;
	add.s32 	%r1214, %r3, %r1213;
	and.b32  	%r1215, %r1214, -16;
	sub.s32 	%r1216, %r3, %r1215;
	ld.param.u64 	%rd33, [chunk_fwd_kernel_o_param_4];
	.loc	1 60 49
	shl.b32 	%r1217, %r2, 1;
	ld.param.u64 	%rd34, [chunk_fwd_kernel_o_param_5];
	ld.param.u64 	%rd35, [chunk_fwd_kernel_o_param_6];
	.loc	1 60 43
	mul.wide.s32 	%rd36, %r1217, 4;
	add.s64 	%rd1, %rd35, %rd36;
	ld.param.f32 	%f1025, [chunk_fwd_kernel_o_param_7];
	mov.pred 	%p1, -1;
	.loc	1 60 27
	// begin inline asm
	mov.u32 %r4, 0x0;
	@%p1 ld.global.b32 { %r4 }, [ %rd1 + 0 ];
	// end inline asm
	.loc	1 60 100
	add.s64 	%rd2, %rd1, 4;
	.loc	1 60 74
	// begin inline asm
	mov.u32 %r5, 0x0;
	@%p1 ld.global.b32 { %r5 }, [ %rd2 + 0 ];
	// end inline asm
	.loc	1 61 40
	mul.wide.s32 	%rd37, %r4, 4;
	add.s64 	%rd3, %rd34, %rd37;
	.loc	1 61 27
	// begin inline asm
	mov.u32 %r6, 0x0;
	@%p1 ld.global.b32 { %r6 }, [ %rd3 + 0 ];
	// end inline asm
	.loc	1 61 86
	add.s64 	%rd4, %rd3, 4;
	.loc	1 61 67
	// begin inline asm
	mov.u32 %r7, 0x0;
	@%p1 ld.global.b32 { %r7 }, [ %rd4 + 0 ];
	// end inline asm
	.loc	1 62 18
	sub.s32 	%r1218, %r7, %r6;
	.loc	1 70 27
	shl.b32 	%r1219, %r6, 11;
	shl.b32 	%r1220, %r1216, 7;
	add.s32 	%r1221, %r1219, %r1220;
	.loc	1 70 9
	mul.wide.s32 	%rd38, %r1221, 2;
	add.s64 	%rd39, %rd29, %rd38;
	.loc	1 71 9
	add.s64 	%rd40, %rd30, %rd38;
	.loc	1 72 9
	add.s64 	%rd41, %rd31, %rd38;
	.loc	1 73 9
	add.s64 	%rd42, %rd33, %rd38;
	.loc	1 74 17
	shl.b32 	%r1222, %r2, 4;
	.loc	1 74 21
	add.s32 	%r1223, %r1216, %r1222;
	.loc	1 74 0
	mul.wide.s32 	%rd43, %r1223, 16384;
	.loc	1 74 9
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd32, %rd44;
	.loc	1 80 60
	shl.b32 	%r1224, %r5, 6;
	.loc	1 80 85
	cvt.s64.s32 	%rd46, %r1218;
	cvt.s64.s32 	%rd47, %r1224;
	.loc	1 82 68
	shl.b32 	%r1225, %r1, 7;
	.loc	1 82 83
	cvt.s64.s32 	%rd48, %r1225;
	.loc	1 84 22
	mov.u32 	%r1226, %tid.x;
	bfe.u32 	%r1227, %r1226, 4, 4;
	or.b32  	%r1228, %r1227, 16;
	or.b32  	%r1229, %r1227, 32;
	or.b32  	%r1230, %r1227, 48;
	shr.u32 	%r1231, %r1226, 2;
	bfe.u32 	%r1232, %r1226, 2, 3;
	or.b32  	%r1233, %r1232, 8;
	or.b32  	%r1234, %r1232, 16;
	or.b32  	%r1235, %r1232, 24;
	or.b32  	%r1236, %r1232, 32;
	or.b32  	%r1237, %r1232, 40;
	or.b32  	%r1238, %r1232, 48;
	or.b32  	%r1239, %r1232, 56;
	shl.b32 	%r1240, %r1226, 1;
	and.b32  	%r1241, %r1240, 6;
	and.b32  	%r1242, %r1231, 56;
	or.b32  	%r1243, %r1242, %r1241;
	cvt.u64.u32 	%rd49, %r1227;
	cvt.u64.u32 	%rd50, %r1228;
	cvt.u64.u32 	%rd51, %r1229;
	cvt.u64.u32 	%rd52, %r1230;
	or.b64  	%rd53, %rd47, %rd49;
	or.b64  	%rd54, %rd47, %rd50;
	or.b64  	%rd55, %rd47, %rd51;
	or.b64  	%rd56, %rd47, %rd52;
	shl.b64 	%rd57, %rd53, 11;
	shl.b64 	%rd58, %rd54, 11;
	shl.b64 	%rd59, %rd55, 11;
	shl.b64 	%rd60, %rd56, 11;
	shl.b32 	%r1244, %r1226, 3;
	and.b32  	%r1245, %r1244, 120;
	cvt.u64.u32 	%rd61, %r1245;
	or.b64  	%rd62, %rd57, %rd61;
	or.b64  	%rd63, %rd58, %rd61;
	or.b64  	%rd64, %rd59, %rd61;
	or.b64  	%rd65, %rd60, %rd61;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd5, %rd39, %rd66;
	shl.b64 	%rd67, %rd63, 1;
	add.s64 	%rd6, %rd39, %rd67;
	shl.b64 	%rd68, %rd64, 1;
	add.s64 	%rd7, %rd39, %rd68;
	shl.b64 	%rd69, %rd65, 1;
	add.s64 	%rd8, %rd39, %rd69;
	setp.gt.s64 	%p29, %rd53, -1;
	setp.gt.s64 	%p30, %rd54, -1;
	setp.gt.s64 	%p31, %rd55, -1;
	setp.gt.s64 	%p32, %rd56, -1;
	setp.lt.s64 	%p33, %rd53, %rd46;
	setp.lt.s64 	%p34, %rd54, %rd46;
	setp.lt.s64 	%p35, %rd55, %rd46;
	setp.lt.s64 	%p36, %rd56, %rd46;
	and.pred  	%p5, %p29, %p33;
	and.pred  	%p6, %p30, %p34;
	and.pred  	%p7, %p31, %p35;
	and.pred  	%p8, %p32, %p36;
	// begin inline asm
	mov.u32 %r8, 0x0;
	mov.u32 %r9, 0x0;
	mov.u32 %r10, 0x0;
	mov.u32 %r11, 0x0;
	@%p5 ld.global.v4.b32 { %r8, %r9, %r10, %r11 }, [ %rd5 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r12, 0x0;
	mov.u32 %r13, 0x0;
	mov.u32 %r14, 0x0;
	mov.u32 %r15, 0x0;
	@%p6 ld.global.v4.b32 { %r12, %r13, %r14, %r15 }, [ %rd6 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r16, 0x0;
	mov.u32 %r17, 0x0;
	mov.u32 %r18, 0x0;
	mov.u32 %r19, 0x0;
	@%p7 ld.global.v4.b32 { %r16, %r17, %r18, %r19 }, [ %rd7 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r20, 0x0;
	mov.u32 %r21, 0x0;
	mov.u32 %r22, 0x0;
	mov.u32 %r23, 0x0;
	@%p8 ld.global.v4.b32 { %r20, %r21, %r22, %r23 }, [ %rd8 + 0 ];
	// end inline asm
	shl.b32 	%r1258, %r1227, 7;
	shr.u32 	%r1259, %r1226, 1;
	and.b32  	%r1260, %r1259, 56;
	xor.b32  	%r1261, %r1245, %r1260;
	or.b32  	%r1262, %r1258, %r1261;
	shl.b32 	%r1263, %r1262, 1;
	mov.u32 	%r1264, global_smem;
	add.s32 	%r1265, %r1264, %r1263;
	shl.b32 	%r1266, %r1228, 8;
	shl.b32 	%r1267, %r1261, 1;
	or.b32  	%r1268, %r1266, %r1267;
	add.s32 	%r1269, %r1264, %r1268;
	shl.b32 	%r1270, %r1229, 8;
	or.b32  	%r1271, %r1270, %r1267;
	add.s32 	%r1272, %r1264, %r1271;
	shl.b32 	%r1273, %r1230, 8;
	or.b32  	%r1274, %r1273, %r1267;
	add.s32 	%r1275, %r1264, %r1274;
	st.shared.v4.b32 	[%r1265], {%r8, %r9, %r10, %r11};
	st.shared.v4.b32 	[%r1269], {%r12, %r13, %r14, %r15};
	st.shared.v4.b32 	[%r1272], {%r16, %r17, %r18, %r19};
	st.shared.v4.b32 	[%r1275], {%r20, %r21, %r22, %r23};
	.loc	1 86 22
	add.s64 	%rd9, %rd40, %rd66;
	add.s64 	%rd10, %rd40, %rd67;
	add.s64 	%rd11, %rd40, %rd68;
	add.s64 	%rd12, %rd40, %rd69;
	// begin inline asm
	mov.u32 %r24, 0x0;
	mov.u32 %r25, 0x0;
	mov.u32 %r26, 0x0;
	mov.u32 %r27, 0x0;
	@%p5 ld.global.v4.b32 { %r24, %r25, %r26, %r27 }, [ %rd9 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r28, 0x0;
	mov.u32 %r29, 0x0;
	mov.u32 %r30, 0x0;
	mov.u32 %r31, 0x0;
	@%p6 ld.global.v4.b32 { %r28, %r29, %r30, %r31 }, [ %rd10 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r32, 0x0;
	mov.u32 %r33, 0x0;
	mov.u32 %r34, 0x0;
	mov.u32 %r35, 0x0;
	@%p7 ld.global.v4.b32 { %r32, %r33, %r34, %r35 }, [ %rd11 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r36, 0x0;
	mov.u32 %r37, 0x0;
	mov.u32 %r38, 0x0;
	mov.u32 %r39, 0x0;
	@%p8 ld.global.v4.b32 { %r36, %r37, %r38, %r39 }, [ %rd12 + 0 ];
	// end inline asm
	add.s32 	%r1292, %r1264, 16384;
	add.s32 	%r1293, %r1292, %r1263;
	add.s32 	%r1294, %r1292, %r1268;
	add.s32 	%r1295, %r1292, %r1271;
	add.s32 	%r1296, %r1292, %r1274;
	st.shared.v4.b32 	[%r1293], {%r24, %r25, %r26, %r27};
	st.shared.v4.b32 	[%r1294], {%r28, %r29, %r30, %r31};
	st.shared.v4.b32 	[%r1295], {%r32, %r33, %r34, %r35};
	st.shared.v4.b32 	[%r1296], {%r36, %r37, %r38, %r39};
	.loc	1 88 22
	mul.wide.u32 	%rd70, %r1227, 128;
	mul.wide.u32 	%rd71, %r1228, 128;
	mul.wide.u32 	%rd72, %r1229, 128;
	mul.wide.u32 	%rd73, %r1230, 128;
	or.b64  	%rd74, %rd48, %rd61;
	shl.b64 	%rd75, %rd70, 1;
	shl.b64 	%rd76, %rd74, 1;
	add.s64 	%rd77, %rd45, %rd76;
	add.s64 	%rd13, %rd77, %rd75;
	shl.b64 	%rd78, %rd71, 1;
	add.s64 	%rd14, %rd77, %rd78;
	shl.b64 	%rd79, %rd72, 1;
	add.s64 	%rd15, %rd77, %rd79;
	shl.b64 	%rd80, %rd73, 1;
	add.s64 	%rd16, %rd77, %rd80;
	mul.wide.u32 	%rd81, %r1258, 2;
	add.s64 	%rd82, %rd77, %rd81;
	add.s64 	%rd17, %rd82, 16384;
	add.s64 	%rd18, %rd82, 20480;
	add.s64 	%rd19, %rd82, 24576;
	add.s64 	%rd20, %rd82, 28672;
	setp.lt.u64 	%p13, %rd74, 128;
	// begin inline asm
	mov.u32 %r40, 0x0;
	mov.u32 %r41, 0x0;
	mov.u32 %r42, 0x0;
	mov.u32 %r43, 0x0;
	@%p13 ld.global.v4.b32 { %r40, %r41, %r42, %r43 }, [ %rd13 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r44, 0x0;
	mov.u32 %r45, 0x0;
	mov.u32 %r46, 0x0;
	mov.u32 %r47, 0x0;
	@%p13 ld.global.v4.b32 { %r44, %r45, %r46, %r47 }, [ %rd14 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r48, 0x0;
	mov.u32 %r49, 0x0;
	mov.u32 %r50, 0x0;
	mov.u32 %r51, 0x0;
	@%p13 ld.global.v4.b32 { %r48, %r49, %r50, %r51 }, [ %rd15 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r52, 0x0;
	mov.u32 %r53, 0x0;
	mov.u32 %r54, 0x0;
	mov.u32 %r55, 0x0;
	@%p13 ld.global.v4.b32 { %r52, %r53, %r54, %r55 }, [ %rd16 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r56, 0x0;
	mov.u32 %r57, 0x0;
	mov.u32 %r58, 0x0;
	mov.u32 %r59, 0x0;
	@%p13 ld.global.v4.b32 { %r56, %r57, %r58, %r59 }, [ %rd17 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r60, 0x0;
	mov.u32 %r61, 0x0;
	mov.u32 %r62, 0x0;
	mov.u32 %r63, 0x0;
	@%p13 ld.global.v4.b32 { %r60, %r61, %r62, %r63 }, [ %rd18 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r64, 0x0;
	mov.u32 %r65, 0x0;
	mov.u32 %r66, 0x0;
	mov.u32 %r67, 0x0;
	@%p13 ld.global.v4.b32 { %r64, %r65, %r66, %r67 }, [ %rd19 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r68, 0x0;
	mov.u32 %r69, 0x0;
	mov.u32 %r70, 0x0;
	mov.u32 %r71, 0x0;
	@%p13 ld.global.v4.b32 { %r68, %r69, %r70, %r71 }, [ %rd20 + 0 ];
	// end inline asm
	add.s32 	%r1325, %r1264, 32768;
	add.s32 	%r1326, %r1325, %r1263;
	add.s32 	%r1327, %r1325, %r1268;
	add.s32 	%r1328, %r1325, %r1271;
	add.s32 	%r1329, %r1325, %r1274;
	st.shared.v4.b32 	[%r1326], {%r40, %r41, %r42, %r43};
	st.shared.v4.b32 	[%r1327], {%r44, %r45, %r46, %r47};
	st.shared.v4.b32 	[%r1328], {%r48, %r49, %r50, %r51};
	st.shared.v4.b32 	[%r1329], {%r52, %r53, %r54, %r55};
	st.shared.v4.b32 	[%r1326+16384], {%r56, %r57, %r58, %r59};
	st.shared.v4.b32 	[%r1326+20480], {%r60, %r61, %r62, %r63};
	st.shared.v4.b32 	[%r1326+24576], {%r64, %r65, %r66, %r67};
	st.shared.v4.b32 	[%r1326+28672], {%r68, %r69, %r70, %r71};
	.loc	1 84 22
	bar.sync 	0;
	and.b32  	%r1338, %r1226, 7;
	bfe.u32 	%r1339, %r1226, 3, 1;
	bfe.u32 	%r1340, %r1226, 4, 1;
	and.b32  	%r1341, %r1226, 15;
	xor.b32  	%r1342, %r1340, %r1338;
	shl.b32 	%r1343, %r1341, 7;
	shl.b32 	%r1344, %r1342, 3;
	or.b32  	%r1345, %r1344, %r1343;
	shl.b32 	%r1346, %r1345, 1;
	add.s32 	%r76, %r1264, %r1346;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r272, %r273, %r274, %r275 }, [ %r76 + 0 ];
	// end inline asm
	or.b32  	%r1347, %r1340, 2;
	xor.b32  	%r1348, %r1347, %r1338;
	shl.b32 	%r1349, %r1348, 3;
	or.b32  	%r1350, %r1349, %r1343;
	shl.b32 	%r1351, %r1350, 1;
	add.s32 	%r81, %r1264, %r1351;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r320, %r321, %r322, %r323 }, [ %r81 + 0 ];
	// end inline asm
	or.b32  	%r1352, %r1340, 4;
	xor.b32  	%r1353, %r1352, %r1338;
	shl.b32 	%r1354, %r1353, 3;
	or.b32  	%r1355, %r1354, %r1343;
	shl.b32 	%r1356, %r1355, 1;
	add.s32 	%r86, %r1264, %r1356;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r368, %r369, %r370, %r371 }, [ %r86 + 0 ];
	// end inline asm
	or.b32  	%r1357, %r1340, 6;
	xor.b32  	%r1358, %r1357, %r1338;
	shl.b32 	%r1359, %r1358, 3;
	or.b32  	%r1360, %r1359, %r1343;
	shl.b32 	%r1361, %r1360, 1;
	add.s32 	%r91, %r1264, %r1361;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r416, %r417, %r418, %r419 }, [ %r91 + 0 ];
	// end inline asm
	or.b32  	%r1362, %r1340, 8;
	xor.b32  	%r1363, %r1362, %r1338;
	shl.b32 	%r1364, %r1363, 4;
	shl.b32 	%r1365, %r1341, 8;
	or.b32  	%r1366, %r1364, %r1365;
	add.s32 	%r96, %r1264, %r1366;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r464, %r465, %r466, %r467 }, [ %r96 + 0 ];
	// end inline asm
	or.b32  	%r1367, %r1340, 10;
	xor.b32  	%r1368, %r1367, %r1338;
	shl.b32 	%r1369, %r1368, 4;
	or.b32  	%r1370, %r1369, %r1365;
	add.s32 	%r101, %r1264, %r1370;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r512, %r513, %r514, %r515 }, [ %r101 + 0 ];
	// end inline asm
	or.b32  	%r1371, %r1340, 12;
	xor.b32  	%r1372, %r1371, %r1338;
	shl.b32 	%r1373, %r1372, 4;
	or.b32  	%r1374, %r1373, %r1365;
	add.s32 	%r106, %r1264, %r1374;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r560, %r561, %r562, %r563 }, [ %r106 + 0 ];
	// end inline asm
	or.b32  	%r1375, %r1340, 14;
	xor.b32  	%r1376, %r1375, %r1338;
	shl.b32 	%r1377, %r1376, 4;
	or.b32  	%r1378, %r1377, %r1365;
	add.s32 	%r111, %r1264, %r1378;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r608, %r609, %r610, %r611 }, [ %r111 + 0 ];
	// end inline asm
	add.s32 	%r116, %r76, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r284, %r285, %r286, %r287 }, [ %r116 + 0 ];
	// end inline asm
	add.s32 	%r121, %r81, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r332, %r333, %r334, %r335 }, [ %r121 + 0 ];
	// end inline asm
	add.s32 	%r126, %r86, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r380, %r381, %r382, %r383 }, [ %r126 + 0 ];
	// end inline asm
	add.s32 	%r131, %r91, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r428, %r429, %r430, %r431 }, [ %r131 + 0 ];
	// end inline asm
	add.s32 	%r136, %r96, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r476, %r477, %r478, %r479 }, [ %r136 + 0 ];
	// end inline asm
	add.s32 	%r141, %r101, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r524, %r525, %r526, %r527 }, [ %r141 + 0 ];
	// end inline asm
	add.s32 	%r146, %r106, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r572, %r573, %r574, %r575 }, [ %r146 + 0 ];
	// end inline asm
	add.s32 	%r151, %r111, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r620, %r621, %r622, %r623 }, [ %r151 + 0 ];
	// end inline asm
	add.s32 	%r156, %r76, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r296, %r297, %r298, %r299 }, [ %r156 + 0 ];
	// end inline asm
	add.s32 	%r161, %r81, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r344, %r345, %r346, %r347 }, [ %r161 + 0 ];
	// end inline asm
	add.s32 	%r166, %r86, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r392, %r393, %r394, %r395 }, [ %r166 + 0 ];
	// end inline asm
	add.s32 	%r171, %r91, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r440, %r441, %r442, %r443 }, [ %r171 + 0 ];
	// end inline asm
	add.s32 	%r176, %r96, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r488, %r489, %r490, %r491 }, [ %r176 + 0 ];
	// end inline asm
	add.s32 	%r181, %r101, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r536, %r537, %r538, %r539 }, [ %r181 + 0 ];
	// end inline asm
	add.s32 	%r186, %r106, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r584, %r585, %r586, %r587 }, [ %r186 + 0 ];
	// end inline asm
	add.s32 	%r191, %r111, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r632, %r633, %r634, %r635 }, [ %r191 + 0 ];
	// end inline asm
	add.s32 	%r196, %r76, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r308, %r309, %r310, %r311 }, [ %r196 + 0 ];
	// end inline asm
	add.s32 	%r201, %r81, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r356, %r357, %r358, %r359 }, [ %r201 + 0 ];
	// end inline asm
	add.s32 	%r206, %r86, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r404, %r405, %r406, %r407 }, [ %r206 + 0 ];
	// end inline asm
	add.s32 	%r211, %r91, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r452, %r453, %r454, %r455 }, [ %r211 + 0 ];
	// end inline asm
	add.s32 	%r216, %r96, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r500, %r501, %r502, %r503 }, [ %r216 + 0 ];
	// end inline asm
	add.s32 	%r221, %r101, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r548, %r549, %r550, %r551 }, [ %r221 + 0 ];
	// end inline asm
	add.s32 	%r226, %r106, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r596, %r597, %r598, %r599 }, [ %r226 + 0 ];
	// end inline asm
	add.s32 	%r231, %r111, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r644, %r645, %r646, %r647 }, [ %r231 + 0 ];
	// end inline asm
	.loc	1 88 22
	bfe.u32 	%r1379, %r1226, 5, 3;
	shl.b32 	%r1380, %r1340, 3;
	or.b32  	%r1381, %r1380, %r1379;
	xor.b32  	%r1382, %r1381, %r1338;
	shl.b32 	%r1383, %r1382, 4;
	or.b32  	%r1384, %r1383, %r1365;
	add.s32 	%r236, %r1325, %r1384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r276, %r277, %r282, %r283 }, [ %r236 + 0 ];
	// end inline asm
	add.s32 	%r241, %r236, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r324, %r325, %r330, %r331 }, [ %r241 + 0 ];
	// end inline asm
	add.s32 	%r246, %r236, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r372, %r373, %r378, %r379 }, [ %r246 + 0 ];
	// end inline asm
	add.s32 	%r251, %r236, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r420, %r421, %r426, %r427 }, [ %r251 + 0 ];
	// end inline asm
	add.s32 	%r256, %r236, 16384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r468, %r469, %r474, %r475 }, [ %r256 + 0 ];
	// end inline asm
	add.s32 	%r261, %r236, 20480;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r516, %r517, %r522, %r523 }, [ %r261 + 0 ];
	// end inline asm
	add.s32 	%r266, %r236, 24576;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r564, %r565, %r570, %r571 }, [ %r266 + 0 ];
	// end inline asm
	add.s32 	%r271, %r236, 28672;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r612, %r613, %r618, %r619 }, [ %r271 + 0 ];
	// end inline asm
	mov.f32 	%f889, 0f00000000;
	.loc	1 91 27
	mov.f32 	%f65, %f889;
	mov.f32 	%f66, %f889;
	mov.f32 	%f67, %f889;
	mov.f32 	%f68, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 }, { %r272, %r273, %r274, %r275 }, { %r276, %r277 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	mov.f32 	%f73, %f889;
	mov.f32 	%f74, %f889;
	mov.f32 	%f75, %f889;
	mov.f32 	%f76, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r272, %r273, %r274, %r275 }, { %r282, %r283 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	mov.f32 	%f81, %f889;
	mov.f32 	%f82, %f889;
	mov.f32 	%f83, %f889;
	mov.f32 	%f84, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f81, %f82, %f83, %f84 }, { %r284, %r285, %r286, %r287 }, { %r276, %r277 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	mov.f32 	%f89, %f889;
	mov.f32 	%f90, %f889;
	mov.f32 	%f91, %f889;
	mov.f32 	%f92, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f89, %f90, %f91, %f92 }, { %r284, %r285, %r286, %r287 }, { %r282, %r283 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	mov.f32 	%f97, %f889;
	mov.f32 	%f98, %f889;
	mov.f32 	%f99, %f889;
	mov.f32 	%f100, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f97, %f98, %f99, %f100 }, { %r296, %r297, %r298, %r299 }, { %r276, %r277 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	mov.f32 	%f105, %f889;
	mov.f32 	%f106, %f889;
	mov.f32 	%f107, %f889;
	mov.f32 	%f108, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f105, %f106, %f107, %f108 }, { %r296, %r297, %r298, %r299 }, { %r282, %r283 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	mov.f32 	%f113, %f889;
	mov.f32 	%f114, %f889;
	mov.f32 	%f115, %f889;
	mov.f32 	%f116, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f113, %f114, %f115, %f116 }, { %r308, %r309, %r310, %r311 }, { %r276, %r277 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	mov.f32 	%f121, %f889;
	mov.f32 	%f122, %f889;
	mov.f32 	%f123, %f889;
	mov.f32 	%f124, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f121, %f122, %f123, %f124 }, { %r308, %r309, %r310, %r311 }, { %r282, %r283 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 }, { %r320, %r321, %r322, %r323 }, { %r324, %r325 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r320, %r321, %r322, %r323 }, { %r330, %r331 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f81, %f82, %f83, %f84 }, { %r332, %r333, %r334, %r335 }, { %r324, %r325 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f89, %f90, %f91, %f92 }, { %r332, %r333, %r334, %r335 }, { %r330, %r331 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f97, %f98, %f99, %f100 }, { %r344, %r345, %r346, %r347 }, { %r324, %r325 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f105, %f106, %f107, %f108 }, { %r344, %r345, %r346, %r347 }, { %r330, %r331 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f113, %f114, %f115, %f116 }, { %r356, %r357, %r358, %r359 }, { %r324, %r325 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f121, %f122, %f123, %f124 }, { %r356, %r357, %r358, %r359 }, { %r330, %r331 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 }, { %r368, %r369, %r370, %r371 }, { %r372, %r373 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r368, %r369, %r370, %r371 }, { %r378, %r379 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f81, %f82, %f83, %f84 }, { %r380, %r381, %r382, %r383 }, { %r372, %r373 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f89, %f90, %f91, %f92 }, { %r380, %r381, %r382, %r383 }, { %r378, %r379 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f97, %f98, %f99, %f100 }, { %r392, %r393, %r394, %r395 }, { %r372, %r373 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f105, %f106, %f107, %f108 }, { %r392, %r393, %r394, %r395 }, { %r378, %r379 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f113, %f114, %f115, %f116 }, { %r404, %r405, %r406, %r407 }, { %r372, %r373 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f121, %f122, %f123, %f124 }, { %r404, %r405, %r406, %r407 }, { %r378, %r379 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 }, { %r416, %r417, %r418, %r419 }, { %r420, %r421 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r416, %r417, %r418, %r419 }, { %r426, %r427 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f81, %f82, %f83, %f84 }, { %r428, %r429, %r430, %r431 }, { %r420, %r421 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f89, %f90, %f91, %f92 }, { %r428, %r429, %r430, %r431 }, { %r426, %r427 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f97, %f98, %f99, %f100 }, { %r440, %r441, %r442, %r443 }, { %r420, %r421 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f105, %f106, %f107, %f108 }, { %r440, %r441, %r442, %r443 }, { %r426, %r427 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f113, %f114, %f115, %f116 }, { %r452, %r453, %r454, %r455 }, { %r420, %r421 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f121, %f122, %f123, %f124 }, { %r452, %r453, %r454, %r455 }, { %r426, %r427 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 }, { %r464, %r465, %r466, %r467 }, { %r468, %r469 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r464, %r465, %r466, %r467 }, { %r474, %r475 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f81, %f82, %f83, %f84 }, { %r476, %r477, %r478, %r479 }, { %r468, %r469 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f89, %f90, %f91, %f92 }, { %r476, %r477, %r478, %r479 }, { %r474, %r475 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f97, %f98, %f99, %f100 }, { %r488, %r489, %r490, %r491 }, { %r468, %r469 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f105, %f106, %f107, %f108 }, { %r488, %r489, %r490, %r491 }, { %r474, %r475 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f113, %f114, %f115, %f116 }, { %r500, %r501, %r502, %r503 }, { %r468, %r469 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f121, %f122, %f123, %f124 }, { %r500, %r501, %r502, %r503 }, { %r474, %r475 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 }, { %r512, %r513, %r514, %r515 }, { %r516, %r517 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r512, %r513, %r514, %r515 }, { %r522, %r523 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f81, %f82, %f83, %f84 }, { %r524, %r525, %r526, %r527 }, { %r516, %r517 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f89, %f90, %f91, %f92 }, { %r524, %r525, %r526, %r527 }, { %r522, %r523 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f97, %f98, %f99, %f100 }, { %r536, %r537, %r538, %r539 }, { %r516, %r517 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f105, %f106, %f107, %f108 }, { %r536, %r537, %r538, %r539 }, { %r522, %r523 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f113, %f114, %f115, %f116 }, { %r548, %r549, %r550, %r551 }, { %r516, %r517 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f121, %f122, %f123, %f124 }, { %r548, %r549, %r550, %r551 }, { %r522, %r523 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 }, { %r560, %r561, %r562, %r563 }, { %r564, %r565 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r560, %r561, %r562, %r563 }, { %r570, %r571 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f81, %f82, %f83, %f84 }, { %r572, %r573, %r574, %r575 }, { %r564, %r565 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f89, %f90, %f91, %f92 }, { %r572, %r573, %r574, %r575 }, { %r570, %r571 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f97, %f98, %f99, %f100 }, { %r584, %r585, %r586, %r587 }, { %r564, %r565 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f105, %f106, %f107, %f108 }, { %r584, %r585, %r586, %r587 }, { %r570, %r571 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f113, %f114, %f115, %f116 }, { %r596, %r597, %r598, %r599 }, { %r564, %r565 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f121, %f122, %f123, %f124 }, { %r596, %r597, %r598, %r599 }, { %r570, %r571 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f65, %f66, %f67, %f68 }, { %r608, %r609, %r610, %r611 }, { %r612, %r613 }, { %f65, %f66, %f67, %f68 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r608, %r609, %r610, %r611 }, { %r618, %r619 }, { %f73, %f74, %f75, %f76 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f81, %f82, %f83, %f84 }, { %r620, %r621, %r622, %r623 }, { %r612, %r613 }, { %f81, %f82, %f83, %f84 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f89, %f90, %f91, %f92 }, { %r620, %r621, %r622, %r623 }, { %r618, %r619 }, { %f89, %f90, %f91, %f92 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f97, %f98, %f99, %f100 }, { %r632, %r633, %r634, %r635 }, { %r612, %r613 }, { %f97, %f98, %f99, %f100 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f105, %f106, %f107, %f108 }, { %r632, %r633, %r634, %r635 }, { %r618, %r619 }, { %f105, %f106, %f107, %f108 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f113, %f114, %f115, %f116 }, { %r644, %r645, %r646, %r647 }, { %r612, %r613 }, { %f113, %f114, %f115, %f116 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f121, %f122, %f123, %f124 }, { %r644, %r645, %r646, %r647 }, { %r618, %r619 }, { %f121, %f122, %f123, %f124 };
	// end inline asm
	bar.sync 	0;
	shl.b32 	%r1385, %r1379, 3;
	or.b32  	%r1386, %r1385, %r1241;
	mad.lo.s32 	%r1387, %r1232, 136, %r1386;
	shl.b32 	%r1388, %r1387, 2;
	add.s32 	%r1389, %r1264, %r1388;
	st.shared.v2.f32 	[%r1389], {%f65, %f66};
	st.shared.v2.f32 	[%r1389+4352], {%f67, %f68};
	st.shared.v2.f32 	[%r1389+256], {%f73, %f74};
	st.shared.v2.f32 	[%r1389+4608], {%f75, %f76};
	bar.sync 	0;
	shl.b32 	%r1390, %r1379, 1;
	or.b32  	%r1391, %r1390, %r1340;
	mad.lo.s32 	%r1392, %r1391, 136, %r1245;
	shl.b32 	%r1393, %r1392, 2;
	add.s32 	%r1394, %r1264, %r1393;
	ld.shared.v4.f32 	{%f1026, %f1027, %f1028, %f1029}, [%r1394];
	ld.shared.v4.f32 	{%f1030, %f1031, %f1032, %f1033}, [%r1394+16];
	bar.sync 	0;
	st.shared.v2.f32 	[%r1389], {%f81, %f82};
	st.shared.v2.f32 	[%r1389+4352], {%f83, %f84};
	st.shared.v2.f32 	[%r1389+256], {%f89, %f90};
	st.shared.v2.f32 	[%r1389+4608], {%f91, %f92};
	bar.sync 	0;
	ld.shared.v4.f32 	{%f1034, %f1035, %f1036, %f1037}, [%r1394];
	ld.shared.v4.f32 	{%f1038, %f1039, %f1040, %f1041}, [%r1394+16];
	bar.sync 	0;
	st.shared.v2.f32 	[%r1389], {%f97, %f98};
	st.shared.v2.f32 	[%r1389+4352], {%f99, %f100};
	st.shared.v2.f32 	[%r1389+256], {%f105, %f106};
	st.shared.v2.f32 	[%r1389+4608], {%f107, %f108};
	bar.sync 	0;
	ld.shared.v4.f32 	{%f1042, %f1043, %f1044, %f1045}, [%r1394];
	ld.shared.v4.f32 	{%f1046, %f1047, %f1048, %f1049}, [%r1394+16];
	bar.sync 	0;
	st.shared.v2.f32 	[%r1389], {%f113, %f114};
	st.shared.v2.f32 	[%r1389+4352], {%f115, %f116};
	st.shared.v2.f32 	[%r1389+256], {%f121, %f122};
	st.shared.v2.f32 	[%r1389+4608], {%f123, %f124};
	bar.sync 	0;
	ld.shared.v4.f32 	{%f1050, %f1051, %f1052, %f1053}, [%r1394];
	ld.shared.v4.f32 	{%f1054, %f1055, %f1056, %f1057}, [%r1394+16];
	.loc	1 86 22
	or.b32  	%r1395, %r1385, %r1338;
	xor.b32  	%r1396, %r1339, %r1338;
	shl.b32 	%r1397, %r1396, 4;
	shl.b32 	%r1398, %r1395, 8;
	or.b32  	%r1399, %r1398, %r1397;
	add.s32 	%r660, %r1292, %r1399;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r700, %r701, %r658, %r659 }, [ %r660 + 0 ];
	// end inline asm
	or.b32  	%r1400, %r1339, 2;
	xor.b32  	%r1401, %r1400, %r1338;
	shl.b32 	%r1402, %r1401, 4;
	or.b32  	%r1403, %r1402, %r1398;
	add.s32 	%r665, %r1292, %r1403;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r724, %r725, %r663, %r664 }, [ %r665 + 0 ];
	// end inline asm
	or.b32  	%r1404, %r1339, 4;
	xor.b32  	%r1405, %r1404, %r1338;
	shl.b32 	%r1406, %r1405, 4;
	or.b32  	%r1407, %r1406, %r1398;
	add.s32 	%r670, %r1292, %r1407;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r748, %r749, %r668, %r669 }, [ %r670 + 0 ];
	// end inline asm
	or.b32  	%r1408, %r1339, 6;
	xor.b32  	%r1409, %r1408, %r1338;
	shl.b32 	%r1410, %r1409, 4;
	or.b32  	%r1411, %r1410, %r1398;
	add.s32 	%r675, %r1292, %r1411;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r772, %r773, %r673, %r674 }, [ %r675 + 0 ];
	// end inline asm
	or.b32  	%r1412, %r1339, 8;
	xor.b32  	%r1413, %r1412, %r1338;
	shl.b32 	%r1414, %r1413, 4;
	or.b32  	%r1415, %r1414, %r1398;
	add.s32 	%r680, %r1292, %r1415;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r796, %r797, %r678, %r679 }, [ %r680 + 0 ];
	// end inline asm
	or.b32  	%r1416, %r1339, 10;
	xor.b32  	%r1417, %r1416, %r1338;
	shl.b32 	%r1418, %r1417, 4;
	or.b32  	%r1419, %r1418, %r1398;
	add.s32 	%r685, %r1292, %r1419;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r820, %r821, %r683, %r684 }, [ %r685 + 0 ];
	// end inline asm
	or.b32  	%r1420, %r1339, 12;
	xor.b32  	%r1421, %r1420, %r1338;
	shl.b32 	%r1422, %r1421, 4;
	or.b32  	%r1423, %r1422, %r1398;
	add.s32 	%r690, %r1292, %r1423;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r844, %r845, %r688, %r689 }, [ %r690 + 0 ];
	// end inline asm
	or.b32  	%r1424, %r1339, 14;
	xor.b32  	%r1425, %r1424, %r1338;
	shl.b32 	%r1426, %r1425, 4;
	or.b32  	%r1427, %r1426, %r1398;
	add.s32 	%r695, %r1292, %r1427;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r868, %r869, %r693, %r694 }, [ %r695 + 0 ];
	// end inline asm
	.loc	1 93 27
	mov.f32 	%f545, %f889;
	mov.f32 	%f546, %f889;
	mov.f32 	%f547, %f889;
	mov.f32 	%f548, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f545, %f546, %f547, %f548 }, { %r272, %r273, %r274, %r275 }, { %r700, %r701 }, { %f545, %f546, %f547, %f548 };
	// end inline asm
	mov.f32 	%f553, %f889;
	mov.f32 	%f554, %f889;
	mov.f32 	%f555, %f889;
	mov.f32 	%f556, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f553, %f554, %f555, %f556 }, { %r284, %r285, %r286, %r287 }, { %r700, %r701 }, { %f553, %f554, %f555, %f556 };
	// end inline asm
	mov.f32 	%f561, %f889;
	mov.f32 	%f562, %f889;
	mov.f32 	%f563, %f889;
	mov.f32 	%f564, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f561, %f562, %f563, %f564 }, { %r296, %r297, %r298, %r299 }, { %r700, %r701 }, { %f561, %f562, %f563, %f564 };
	// end inline asm
	mov.f32 	%f569, %f889;
	mov.f32 	%f570, %f889;
	mov.f32 	%f571, %f889;
	mov.f32 	%f572, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f569, %f570, %f571, %f572 }, { %r308, %r309, %r310, %r311 }, { %r700, %r701 }, { %f569, %f570, %f571, %f572 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f545, %f546, %f547, %f548 }, { %r320, %r321, %r322, %r323 }, { %r724, %r725 }, { %f545, %f546, %f547, %f548 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f553, %f554, %f555, %f556 }, { %r332, %r333, %r334, %r335 }, { %r724, %r725 }, { %f553, %f554, %f555, %f556 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f561, %f562, %f563, %f564 }, { %r344, %r345, %r346, %r347 }, { %r724, %r725 }, { %f561, %f562, %f563, %f564 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f569, %f570, %f571, %f572 }, { %r356, %r357, %r358, %r359 }, { %r724, %r725 }, { %f569, %f570, %f571, %f572 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f545, %f546, %f547, %f548 }, { %r368, %r369, %r370, %r371 }, { %r748, %r749 }, { %f545, %f546, %f547, %f548 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f553, %f554, %f555, %f556 }, { %r380, %r381, %r382, %r383 }, { %r748, %r749 }, { %f553, %f554, %f555, %f556 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f561, %f562, %f563, %f564 }, { %r392, %r393, %r394, %r395 }, { %r748, %r749 }, { %f561, %f562, %f563, %f564 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f569, %f570, %f571, %f572 }, { %r404, %r405, %r406, %r407 }, { %r748, %r749 }, { %f569, %f570, %f571, %f572 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f545, %f546, %f547, %f548 }, { %r416, %r417, %r418, %r419 }, { %r772, %r773 }, { %f545, %f546, %f547, %f548 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f553, %f554, %f555, %f556 }, { %r428, %r429, %r430, %r431 }, { %r772, %r773 }, { %f553, %f554, %f555, %f556 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f561, %f562, %f563, %f564 }, { %r440, %r441, %r442, %r443 }, { %r772, %r773 }, { %f561, %f562, %f563, %f564 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f569, %f570, %f571, %f572 }, { %r452, %r453, %r454, %r455 }, { %r772, %r773 }, { %f569, %f570, %f571, %f572 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f545, %f546, %f547, %f548 }, { %r464, %r465, %r466, %r467 }, { %r796, %r797 }, { %f545, %f546, %f547, %f548 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f553, %f554, %f555, %f556 }, { %r476, %r477, %r478, %r479 }, { %r796, %r797 }, { %f553, %f554, %f555, %f556 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f561, %f562, %f563, %f564 }, { %r488, %r489, %r490, %r491 }, { %r796, %r797 }, { %f561, %f562, %f563, %f564 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f569, %f570, %f571, %f572 }, { %r500, %r501, %r502, %r503 }, { %r796, %r797 }, { %f569, %f570, %f571, %f572 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f545, %f546, %f547, %f548 }, { %r512, %r513, %r514, %r515 }, { %r820, %r821 }, { %f545, %f546, %f547, %f548 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f553, %f554, %f555, %f556 }, { %r524, %r525, %r526, %r527 }, { %r820, %r821 }, { %f553, %f554, %f555, %f556 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f561, %f562, %f563, %f564 }, { %r536, %r537, %r538, %r539 }, { %r820, %r821 }, { %f561, %f562, %f563, %f564 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f569, %f570, %f571, %f572 }, { %r548, %r549, %r550, %r551 }, { %r820, %r821 }, { %f569, %f570, %f571, %f572 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f545, %f546, %f547, %f548 }, { %r560, %r561, %r562, %r563 }, { %r844, %r845 }, { %f545, %f546, %f547, %f548 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f553, %f554, %f555, %f556 }, { %r572, %r573, %r574, %r575 }, { %r844, %r845 }, { %f553, %f554, %f555, %f556 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f561, %f562, %f563, %f564 }, { %r584, %r585, %r586, %r587 }, { %r844, %r845 }, { %f561, %f562, %f563, %f564 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f569, %f570, %f571, %f572 }, { %r596, %r597, %r598, %r599 }, { %r844, %r845 }, { %f569, %f570, %f571, %f572 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f545, %f546, %f547, %f548 }, { %r608, %r609, %r610, %r611 }, { %r868, %r869 }, { %f545, %f546, %f547, %f548 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f553, %f554, %f555, %f556 }, { %r620, %r621, %r622, %r623 }, { %r868, %r869 }, { %f553, %f554, %f555, %f556 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f561, %f562, %f563, %f564 }, { %r632, %r633, %r634, %r635 }, { %r868, %r869 }, { %f561, %f562, %f563, %f564 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f569, %f570, %f571, %f572 }, { %r644, %r645, %r646, %r647 }, { %r868, %r869 }, { %f569, %f570, %f571, %f572 };
	// end inline asm
	.loc	1 108 21
	or.b32  	%r1428, %r1224, %r1232;
	.loc	1 109 16
	setp.lt.s32 	%p37, %r1428, %r1218;
	.loc	1 108 21
	or.b32  	%r1429, %r1224, %r1243;
	or.b32  	%r1430, %r1224, %r1233;
	or.b32  	%r1431, %r1224, %r1234;
	or.b32  	%r1432, %r1224, %r1235;
	or.b32  	%r1433, %r1224, %r1236;
	or.b32  	%r1434, %r1224, %r1237;
	or.b32  	%r1435, %r1224, %r1238;
	or.b32  	%r1436, %r1224, %r1239;
	or.b32  	%r1437, %r1429, 1;
	.loc	1 109 16
	setp.lt.s32 	%p38, %r1429, %r1218;
	setp.lt.s32 	%p39, %r1436, %r1218;
	setp.lt.s32 	%p40, %r1435, %r1218;
	setp.lt.s32 	%p41, %r1434, %r1218;
	setp.lt.s32 	%p42, %r1433, %r1218;
	setp.lt.s32 	%p43, %r1432, %r1218;
	setp.lt.s32 	%p44, %r1431, %r1218;
	setp.lt.s32 	%p45, %r1430, %r1218;
	setp.lt.s32 	%p46, %r1437, %r1218;
	.loc	1 110 27
	setp.ge.u32 	%p47, %r1232, %r1243;
	setp.gt.u32 	%p48, %r1232, %r1243;
	setp.ge.u32 	%p49, %r1233, %r1243;
	setp.gt.u32 	%p50, %r1233, %r1243;
	setp.ge.u32 	%p51, %r1234, %r1243;
	setp.gt.u32 	%p52, %r1234, %r1243;
	setp.ge.u32 	%p53, %r1235, %r1243;
	setp.gt.u32 	%p54, %r1235, %r1243;
	setp.ge.u32 	%p55, %r1236, %r1243;
	setp.gt.u32 	%p56, %r1236, %r1243;
	setp.ge.u32 	%p57, %r1237, %r1243;
	setp.gt.u32 	%p58, %r1237, %r1243;
	setp.ge.u32 	%p59, %r1238, %r1243;
	setp.gt.u32 	%p60, %r1238, %r1243;
	setp.ge.u32 	%p61, %r1239, %r1243;
	setp.gt.u32 	%p62, %r1239, %r1243;
	.loc	1 119 38
	cvt.rn.f16.f32 	%rs1, %f545;
	cvt.rn.f16.f32 	%rs2, %f546;
	cvt.rn.f16.f32 	%rs3, %f547;
	cvt.rn.f16.f32 	%rs4, %f548;
	cvt.rn.f16.f32 	%rs5, %f553;
	cvt.rn.f16.f32 	%rs6, %f554;
	cvt.rn.f16.f32 	%rs7, %f555;
	cvt.rn.f16.f32 	%rs8, %f556;
	cvt.rn.f16.f32 	%rs9, %f561;
	cvt.rn.f16.f32 	%rs10, %f562;
	cvt.rn.f16.f32 	%rs11, %f563;
	cvt.rn.f16.f32 	%rs12, %f564;
	cvt.rn.f16.f32 	%rs13, %f569;
	cvt.rn.f16.f32 	%rs14, %f570;
	cvt.rn.f16.f32 	%rs15, %f571;
	cvt.rn.f16.f32 	%rs16, %f572;
	.loc	1 116 18
	add.s64 	%rd83, %rd57, %rd74;
	add.s64 	%rd84, %rd58, %rd74;
	add.s64 	%rd85, %rd59, %rd74;
	add.s64 	%rd86, %rd60, %rd74;
	shl.b64 	%rd87, %rd83, 1;
	add.s64 	%rd21, %rd41, %rd87;
	shl.b64 	%rd88, %rd84, 1;
	add.s64 	%rd22, %rd41, %rd88;
	shl.b64 	%rd89, %rd85, 1;
	add.s64 	%rd23, %rd41, %rd89;
	shl.b64 	%rd90, %rd86, 1;
	add.s64 	%rd24, %rd41, %rd90;
	and.pred  	%p21, %p13, %p5;
	and.pred  	%p22, %p13, %p6;
	and.pred  	%p23, %p13, %p7;
	and.pred  	%p24, %p13, %p8;
	// begin inline asm
	mov.u32 %r888, 0x0;
	mov.u32 %r889, 0x0;
	mov.u32 %r890, 0x0;
	mov.u32 %r891, 0x0;
	@%p21 ld.global.v4.b32 { %r888, %r889, %r890, %r891 }, [ %rd21 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r892, 0x0;
	mov.u32 %r893, 0x0;
	mov.u32 %r894, 0x0;
	mov.u32 %r895, 0x0;
	@%p22 ld.global.v4.b32 { %r892, %r893, %r894, %r895 }, [ %rd22 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r896, 0x0;
	mov.u32 %r897, 0x0;
	mov.u32 %r898, 0x0;
	mov.u32 %r899, 0x0;
	@%p23 ld.global.v4.b32 { %r896, %r897, %r898, %r899 }, [ %rd23 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r900, 0x0;
	mov.u32 %r901, 0x0;
	mov.u32 %r902, 0x0;
	mov.u32 %r903, 0x0;
	@%p24 ld.global.v4.b32 { %r900, %r901, %r902, %r903 }, [ %rd24 + 0 ];
	// end inline asm
	bar.sync 	0;
	st.shared.v4.b32 	[%r1265], {%r888, %r889, %r890, %r891};
	st.shared.v4.b32 	[%r1269], {%r892, %r893, %r894, %r895};
	st.shared.v4.b32 	[%r1272], {%r896, %r897, %r898, %r899};
	st.shared.v4.b32 	[%r1275], {%r900, %r901, %r902, %r903};
	.loc	1 119 38
	selp.b16 	%rs17, %rs2, 0x0000, %p48;
	selp.b16 	%rs18, %rs17, 0x0000, %p37;
	selp.b16 	%rs19, %rs18, 0x0000, %p46;
	selp.b16 	%rs20, %rs1, 0x0000, %p38;
	selp.b16 	%rs21, %rs20, 0x0000, %p47;
	selp.b16 	%rs22, %rs21, 0x0000, %p37;
	mov.b32 	%r1454, {%rs22, %rs19};
	selp.b16 	%rs23, %rs4, 0x0000, %p46;
	selp.b16 	%rs24, %rs23, 0x0000, %p50;
	selp.b16 	%rs25, %rs24, 0x0000, %p45;
	selp.b16 	%rs26, %rs3, 0x0000, %p38;
	selp.b16 	%rs27, %rs26, 0x0000, %p49;
	selp.b16 	%rs28, %rs27, 0x0000, %p45;
	mov.b32 	%r1455, {%rs28, %rs25};
	selp.b16 	%rs29, %rs6, 0x0000, %p46;
	selp.b16 	%rs30, %rs29, 0x0000, %p52;
	selp.b16 	%rs31, %rs30, 0x0000, %p44;
	selp.b16 	%rs32, %rs5, 0x0000, %p38;
	selp.b16 	%rs33, %rs32, 0x0000, %p51;
	selp.b16 	%rs34, %rs33, 0x0000, %p44;
	mov.b32 	%r1456, {%rs34, %rs31};
	selp.b16 	%rs35, %rs8, 0x0000, %p46;
	selp.b16 	%rs36, %rs35, 0x0000, %p54;
	selp.b16 	%rs37, %rs36, 0x0000, %p43;
	selp.b16 	%rs38, %rs7, 0x0000, %p38;
	selp.b16 	%rs39, %rs38, 0x0000, %p53;
	selp.b16 	%rs40, %rs39, 0x0000, %p43;
	mov.b32 	%r1457, {%rs40, %rs37};
	selp.b16 	%rs41, %rs10, 0x0000, %p46;
	selp.b16 	%rs42, %rs41, 0x0000, %p56;
	selp.b16 	%rs43, %rs42, 0x0000, %p42;
	selp.b16 	%rs44, %rs9, 0x0000, %p38;
	selp.b16 	%rs45, %rs44, 0x0000, %p55;
	selp.b16 	%rs46, %rs45, 0x0000, %p42;
	mov.b32 	%r1458, {%rs46, %rs43};
	selp.b16 	%rs47, %rs12, 0x0000, %p46;
	selp.b16 	%rs48, %rs47, 0x0000, %p58;
	selp.b16 	%rs49, %rs48, 0x0000, %p41;
	selp.b16 	%rs50, %rs11, 0x0000, %p38;
	selp.b16 	%rs51, %rs50, 0x0000, %p57;
	selp.b16 	%rs52, %rs51, 0x0000, %p41;
	mov.b32 	%r1459, {%rs52, %rs49};
	selp.b16 	%rs53, %rs14, 0x0000, %p46;
	selp.b16 	%rs54, %rs53, 0x0000, %p60;
	selp.b16 	%rs55, %rs54, 0x0000, %p40;
	selp.b16 	%rs56, %rs13, 0x0000, %p38;
	selp.b16 	%rs57, %rs56, 0x0000, %p59;
	selp.b16 	%rs58, %rs57, 0x0000, %p40;
	mov.b32 	%r1460, {%rs58, %rs55};
	selp.b16 	%rs59, %rs16, 0x0000, %p46;
	selp.b16 	%rs60, %rs59, 0x0000, %p62;
	selp.b16 	%rs61, %rs60, 0x0000, %p39;
	selp.b16 	%rs62, %rs15, 0x0000, %p38;
	selp.b16 	%rs63, %rs62, 0x0000, %p61;
	selp.b16 	%rs64, %rs63, 0x0000, %p39;
	mov.b32 	%r1461, {%rs64, %rs61};
	shl.b32 	%r1462, %r1232, 3;
	xor.b32  	%r1463, %r1242, %r1462;
	or.b32  	%r1464, %r1463, %r1241;
	shl.b32 	%r1465, %r1232, 7;
	shl.b32 	%r1466, %r1464, 1;
	or.b32  	%r1467, %r1465, %r1466;
	add.s32 	%r1468, %r1292, %r1467;
	shl.b32 	%r1469, %r1233, 7;
	or.b32  	%r1470, %r1469, %r1466;
	add.s32 	%r1471, %r1292, %r1470;
	shl.b32 	%r1472, %r1234, 7;
	or.b32  	%r1473, %r1472, %r1466;
	add.s32 	%r1474, %r1292, %r1473;
	shl.b32 	%r1475, %r1235, 7;
	or.b32  	%r1476, %r1475, %r1466;
	add.s32 	%r1477, %r1292, %r1476;
	shl.b32 	%r1478, %r1236, 7;
	or.b32  	%r1479, %r1478, %r1466;
	add.s32 	%r1480, %r1292, %r1479;
	shl.b32 	%r1481, %r1237, 7;
	or.b32  	%r1482, %r1481, %r1466;
	add.s32 	%r1483, %r1292, %r1482;
	shl.b32 	%r1484, %r1238, 7;
	or.b32  	%r1485, %r1484, %r1466;
	add.s32 	%r1486, %r1292, %r1485;
	shl.b32 	%r1487, %r1239, 7;
	or.b32  	%r1488, %r1487, %r1466;
	add.s32 	%r1489, %r1292, %r1488;
	st.shared.b32 	[%r1468], %r1454;
	st.shared.b32 	[%r1471], %r1455;
	st.shared.b32 	[%r1474], %r1456;
	st.shared.b32 	[%r1477], %r1457;
	st.shared.b32 	[%r1480], %r1458;
	st.shared.b32 	[%r1483], %r1459;
	st.shared.b32 	[%r1486], %r1460;
	st.shared.b32 	[%r1489], %r1461;
	bar.sync 	0;
	shl.b32 	%r1490, %r1341, 6;
	or.b32  	%r1491, %r1344, %r1490;
	shl.b32 	%r1492, %r1491, 1;
	add.s32 	%r908, %r1292, %r1492;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1004, %r1005, %r1006, %r1007 }, [ %r908 + 0 ];
	// end inline asm
	or.b32  	%r1493, %r1349, %r1490;
	shl.b32 	%r1494, %r1493, 1;
	add.s32 	%r913, %r1292, %r1494;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1052, %r1053, %r1054, %r1055 }, [ %r913 + 0 ];
	// end inline asm
	or.b32  	%r1495, %r1354, %r1490;
	shl.b32 	%r1496, %r1495, 1;
	add.s32 	%r918, %r1292, %r1496;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1100, %r1101, %r1102, %r1103 }, [ %r918 + 0 ];
	// end inline asm
	or.b32  	%r1497, %r1359, %r1490;
	shl.b32 	%r1498, %r1497, 1;
	add.s32 	%r923, %r1292, %r1498;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1148, %r1149, %r1150, %r1151 }, [ %r923 + 0 ];
	// end inline asm
	add.s32 	%r928, %r908, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1016, %r1017, %r1018, %r1019 }, [ %r928 + 0 ];
	// end inline asm
	add.s32 	%r933, %r913, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1064, %r1065, %r1066, %r1067 }, [ %r933 + 0 ];
	// end inline asm
	add.s32 	%r938, %r918, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1112, %r1113, %r1114, %r1115 }, [ %r938 + 0 ];
	// end inline asm
	add.s32 	%r943, %r923, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1160, %r1161, %r1162, %r1163 }, [ %r943 + 0 ];
	// end inline asm
	add.s32 	%r948, %r908, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1028, %r1029, %r1030, %r1031 }, [ %r948 + 0 ];
	// end inline asm
	add.s32 	%r953, %r913, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1076, %r1077, %r1078, %r1079 }, [ %r953 + 0 ];
	// end inline asm
	add.s32 	%r958, %r918, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1124, %r1125, %r1126, %r1127 }, [ %r958 + 0 ];
	// end inline asm
	add.s32 	%r963, %r923, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1172, %r1173, %r1174, %r1175 }, [ %r963 + 0 ];
	// end inline asm
	add.s32 	%r968, %r908, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1040, %r1041, %r1042, %r1043 }, [ %r968 + 0 ];
	// end inline asm
	add.s32 	%r973, %r913, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1088, %r1089, %r1090, %r1091 }, [ %r973 + 0 ];
	// end inline asm
	add.s32 	%r978, %r918, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1136, %r1137, %r1138, %r1139 }, [ %r978 + 0 ];
	// end inline asm
	add.s32 	%r983, %r923, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r1184, %r1185, %r1186, %r1187 }, [ %r983 + 0 ];
	// end inline asm
	.loc	1 116 18
	add.s32 	%r988, %r1264, %r1384;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1008, %r1009, %r1014, %r1015 }, [ %r988 + 0 ];
	// end inline asm
	add.s32 	%r993, %r988, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1056, %r1057, %r1062, %r1063 }, [ %r993 + 0 ];
	// end inline asm
	add.s32 	%r998, %r988, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1104, %r1105, %r1110, %r1111 }, [ %r998 + 0 ];
	// end inline asm
	add.s32 	%r1003, %r988, 12288;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1152, %r1153, %r1158, %r1159 }, [ %r1003 + 0 ];
	// end inline asm
	.loc	1 119 50
	mov.f32 	%f833, %f889;
	mov.f32 	%f834, %f889;
	mov.f32 	%f835, %f889;
	mov.f32 	%f836, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f833, %f834, %f835, %f836 }, { %r1004, %r1005, %r1006, %r1007 }, { %r1008, %r1009 }, { %f833, %f834, %f835, %f836 };
	// end inline asm
	mov.f32 	%f841, %f889;
	mov.f32 	%f842, %f889;
	mov.f32 	%f843, %f889;
	mov.f32 	%f844, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f841, %f842, %f843, %f844 }, { %r1004, %r1005, %r1006, %r1007 }, { %r1014, %r1015 }, { %f841, %f842, %f843, %f844 };
	// end inline asm
	mov.f32 	%f849, %f889;
	mov.f32 	%f850, %f889;
	mov.f32 	%f851, %f889;
	mov.f32 	%f852, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f849, %f850, %f851, %f852 }, { %r1016, %r1017, %r1018, %r1019 }, { %r1008, %r1009 }, { %f849, %f850, %f851, %f852 };
	// end inline asm
	mov.f32 	%f857, %f889;
	mov.f32 	%f858, %f889;
	mov.f32 	%f859, %f889;
	mov.f32 	%f860, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f857, %f858, %f859, %f860 }, { %r1016, %r1017, %r1018, %r1019 }, { %r1014, %r1015 }, { %f857, %f858, %f859, %f860 };
	// end inline asm
	mov.f32 	%f865, %f889;
	mov.f32 	%f866, %f889;
	mov.f32 	%f867, %f889;
	mov.f32 	%f868, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f865, %f866, %f867, %f868 }, { %r1028, %r1029, %r1030, %r1031 }, { %r1008, %r1009 }, { %f865, %f866, %f867, %f868 };
	// end inline asm
	mov.f32 	%f873, %f889;
	mov.f32 	%f874, %f889;
	mov.f32 	%f875, %f889;
	mov.f32 	%f876, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f873, %f874, %f875, %f876 }, { %r1028, %r1029, %r1030, %r1031 }, { %r1014, %r1015 }, { %f873, %f874, %f875, %f876 };
	// end inline asm
	mov.f32 	%f881, %f889;
	mov.f32 	%f882, %f889;
	mov.f32 	%f883, %f889;
	mov.f32 	%f884, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f881, %f882, %f883, %f884 }, { %r1040, %r1041, %r1042, %r1043 }, { %r1008, %r1009 }, { %f881, %f882, %f883, %f884 };
	// end inline asm
	mov.f32 	%f890, %f889;
	mov.f32 	%f891, %f889;
	mov.f32 	%f892, %f889;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f889, %f890, %f891, %f892 }, { %r1040, %r1041, %r1042, %r1043 }, { %r1014, %r1015 }, { %f889, %f890, %f891, %f892 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f833, %f834, %f835, %f836 }, { %r1052, %r1053, %r1054, %r1055 }, { %r1056, %r1057 }, { %f833, %f834, %f835, %f836 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f841, %f842, %f843, %f844 }, { %r1052, %r1053, %r1054, %r1055 }, { %r1062, %r1063 }, { %f841, %f842, %f843, %f844 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f849, %f850, %f851, %f852 }, { %r1064, %r1065, %r1066, %r1067 }, { %r1056, %r1057 }, { %f849, %f850, %f851, %f852 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f857, %f858, %f859, %f860 }, { %r1064, %r1065, %r1066, %r1067 }, { %r1062, %r1063 }, { %f857, %f858, %f859, %f860 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f865, %f866, %f867, %f868 }, { %r1076, %r1077, %r1078, %r1079 }, { %r1056, %r1057 }, { %f865, %f866, %f867, %f868 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f873, %f874, %f875, %f876 }, { %r1076, %r1077, %r1078, %r1079 }, { %r1062, %r1063 }, { %f873, %f874, %f875, %f876 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f881, %f882, %f883, %f884 }, { %r1088, %r1089, %r1090, %r1091 }, { %r1056, %r1057 }, { %f881, %f882, %f883, %f884 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f889, %f890, %f891, %f892 }, { %r1088, %r1089, %r1090, %r1091 }, { %r1062, %r1063 }, { %f889, %f890, %f891, %f892 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f833, %f834, %f835, %f836 }, { %r1100, %r1101, %r1102, %r1103 }, { %r1104, %r1105 }, { %f833, %f834, %f835, %f836 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f841, %f842, %f843, %f844 }, { %r1100, %r1101, %r1102, %r1103 }, { %r1110, %r1111 }, { %f841, %f842, %f843, %f844 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f849, %f850, %f851, %f852 }, { %r1112, %r1113, %r1114, %r1115 }, { %r1104, %r1105 }, { %f849, %f850, %f851, %f852 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f857, %f858, %f859, %f860 }, { %r1112, %r1113, %r1114, %r1115 }, { %r1110, %r1111 }, { %f857, %f858, %f859, %f860 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f865, %f866, %f867, %f868 }, { %r1124, %r1125, %r1126, %r1127 }, { %r1104, %r1105 }, { %f865, %f866, %f867, %f868 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f873, %f874, %f875, %f876 }, { %r1124, %r1125, %r1126, %r1127 }, { %r1110, %r1111 }, { %f873, %f874, %f875, %f876 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f881, %f882, %f883, %f884 }, { %r1136, %r1137, %r1138, %r1139 }, { %r1104, %r1105 }, { %f881, %f882, %f883, %f884 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f889, %f890, %f891, %f892 }, { %r1136, %r1137, %r1138, %r1139 }, { %r1110, %r1111 }, { %f889, %f890, %f891, %f892 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f833, %f834, %f835, %f836 }, { %r1148, %r1149, %r1150, %r1151 }, { %r1152, %r1153 }, { %f833, %f834, %f835, %f836 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f841, %f842, %f843, %f844 }, { %r1148, %r1149, %r1150, %r1151 }, { %r1158, %r1159 }, { %f841, %f842, %f843, %f844 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f849, %f850, %f851, %f852 }, { %r1160, %r1161, %r1162, %r1163 }, { %r1152, %r1153 }, { %f849, %f850, %f851, %f852 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f857, %f858, %f859, %f860 }, { %r1160, %r1161, %r1162, %r1163 }, { %r1158, %r1159 }, { %f857, %f858, %f859, %f860 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f865, %f866, %f867, %f868 }, { %r1172, %r1173, %r1174, %r1175 }, { %r1152, %r1153 }, { %f865, %f866, %f867, %f868 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f873, %f874, %f875, %f876 }, { %r1172, %r1173, %r1174, %r1175 }, { %r1158, %r1159 }, { %f873, %f874, %f875, %f876 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f881, %f882, %f883, %f884 }, { %r1184, %r1185, %r1186, %r1187 }, { %r1152, %r1153 }, { %f881, %f882, %f883, %f884 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f889, %f890, %f891, %f892 }, { %r1184, %r1185, %r1186, %r1187 }, { %r1158, %r1159 }, { %f889, %f890, %f891, %f892 };
	// end inline asm
	bar.sync 	0;
	st.shared.v2.f32 	[%r1389], {%f833, %f834};
	st.shared.v2.f32 	[%r1389+4352], {%f835, %f836};
	st.shared.v2.f32 	[%r1389+256], {%f841, %f842};
	st.shared.v2.f32 	[%r1389+4608], {%f843, %f844};
	bar.sync 	0;
	ld.shared.v4.f32 	{%f1058, %f1059, %f1060, %f1061}, [%r1394];
	ld.shared.v4.f32 	{%f1062, %f1063, %f1064, %f1065}, [%r1394+16];
	bar.sync 	0;
	st.shared.v2.f32 	[%r1389], {%f849, %f850};
	st.shared.v2.f32 	[%r1389+4352], {%f851, %f852};
	st.shared.v2.f32 	[%r1389+256], {%f857, %f858};
	st.shared.v2.f32 	[%r1389+4608], {%f859, %f860};
	bar.sync 	0;
	ld.shared.v4.f32 	{%f1066, %f1067, %f1068, %f1069}, [%r1394];
	ld.shared.v4.f32 	{%f1070, %f1071, %f1072, %f1073}, [%r1394+16];
	bar.sync 	0;
	st.shared.v2.f32 	[%r1389], {%f865, %f866};
	st.shared.v2.f32 	[%r1389+4352], {%f867, %f868};
	st.shared.v2.f32 	[%r1389+256], {%f873, %f874};
	st.shared.v2.f32 	[%r1389+4608], {%f875, %f876};
	bar.sync 	0;
	ld.shared.v4.f32 	{%f1074, %f1075, %f1076, %f1077}, [%r1394];
	ld.shared.v4.f32 	{%f1078, %f1079, %f1080, %f1081}, [%r1394+16];
	bar.sync 	0;
	st.shared.v2.f32 	[%r1389], {%f881, %f882};
	st.shared.v2.f32 	[%r1389+4352], {%f883, %f884};
	st.shared.v2.f32 	[%r1389+256], {%f889, %f890};
	st.shared.v2.f32 	[%r1389+4608], {%f891, %f892};
	bar.sync 	0;
	ld.shared.v4.f32 	{%f1082, %f1083, %f1084, %f1085}, [%r1394];
	ld.shared.v4.f32 	{%f1086, %f1087, %f1088, %f1089}, [%r1394+16];
	.loc	1 119 57
	mul.f32 	%f1090, %f1058, %f1025;
	mul.f32 	%f1091, %f1059, %f1025;
	mul.f32 	%f1092, %f1060, %f1025;
	mul.f32 	%f1093, %f1061, %f1025;
	mul.f32 	%f1094, %f1062, %f1025;
	mul.f32 	%f1095, %f1063, %f1025;
	mul.f32 	%f1096, %f1064, %f1025;
	mul.f32 	%f1097, %f1065, %f1025;
	mul.f32 	%f1098, %f1066, %f1025;
	mul.f32 	%f1099, %f1067, %f1025;
	mul.f32 	%f1100, %f1068, %f1025;
	mul.f32 	%f1101, %f1069, %f1025;
	mul.f32 	%f1102, %f1070, %f1025;
	mul.f32 	%f1103, %f1071, %f1025;
	mul.f32 	%f1104, %f1072, %f1025;
	mul.f32 	%f1105, %f1073, %f1025;
	mul.f32 	%f1106, %f1074, %f1025;
	mul.f32 	%f1107, %f1075, %f1025;
	mul.f32 	%f1108, %f1076, %f1025;
	mul.f32 	%f1109, %f1077, %f1025;
	mul.f32 	%f1110, %f1078, %f1025;
	mul.f32 	%f1111, %f1079, %f1025;
	mul.f32 	%f1112, %f1080, %f1025;
	mul.f32 	%f1113, %f1081, %f1025;
	mul.f32 	%f1114, %f1082, %f1025;
	mul.f32 	%f1115, %f1083, %f1025;
	mul.f32 	%f1116, %f1084, %f1025;
	mul.f32 	%f1117, %f1085, %f1025;
	mul.f32 	%f1118, %f1086, %f1025;
	mul.f32 	%f1119, %f1087, %f1025;
	mul.f32 	%f1120, %f1088, %f1025;
	mul.f32 	%f1121, %f1089, %f1025;
	.loc	1 119 24
	fma.rn.f32 	%f1122, %f1026, %f1025, %f1090;
	fma.rn.f32 	%f1123, %f1027, %f1025, %f1091;
	fma.rn.f32 	%f1124, %f1028, %f1025, %f1092;
	fma.rn.f32 	%f1125, %f1029, %f1025, %f1093;
	fma.rn.f32 	%f1126, %f1030, %f1025, %f1094;
	fma.rn.f32 	%f1127, %f1031, %f1025, %f1095;
	fma.rn.f32 	%f1128, %f1032, %f1025, %f1096;
	fma.rn.f32 	%f1129, %f1033, %f1025, %f1097;
	fma.rn.f32 	%f1130, %f1034, %f1025, %f1098;
	fma.rn.f32 	%f1131, %f1035, %f1025, %f1099;
	fma.rn.f32 	%f1132, %f1036, %f1025, %f1100;
	fma.rn.f32 	%f1133, %f1037, %f1025, %f1101;
	fma.rn.f32 	%f1134, %f1038, %f1025, %f1102;
	fma.rn.f32 	%f1135, %f1039, %f1025, %f1103;
	fma.rn.f32 	%f1136, %f1040, %f1025, %f1104;
	fma.rn.f32 	%f1137, %f1041, %f1025, %f1105;
	fma.rn.f32 	%f1138, %f1042, %f1025, %f1106;
	fma.rn.f32 	%f1139, %f1043, %f1025, %f1107;
	fma.rn.f32 	%f1140, %f1044, %f1025, %f1108;
	fma.rn.f32 	%f1141, %f1045, %f1025, %f1109;
	fma.rn.f32 	%f1142, %f1046, %f1025, %f1110;
	fma.rn.f32 	%f1143, %f1047, %f1025, %f1111;
	fma.rn.f32 	%f1144, %f1048, %f1025, %f1112;
	fma.rn.f32 	%f1145, %f1049, %f1025, %f1113;
	fma.rn.f32 	%f1146, %f1050, %f1025, %f1114;
	fma.rn.f32 	%f1147, %f1051, %f1025, %f1115;
	fma.rn.f32 	%f1148, %f1052, %f1025, %f1116;
	fma.rn.f32 	%f1149, %f1053, %f1025, %f1117;
	fma.rn.f32 	%f1150, %f1054, %f1025, %f1118;
	fma.rn.f32 	%f1151, %f1055, %f1025, %f1119;
	fma.rn.f32 	%f1152, %f1056, %f1025, %f1120;
	fma.rn.f32 	%f1153, %f1057, %f1025, %f1121;
	.loc	1 120 25
	cvt.rn.f16.f32 	%rs65, %f1123;
	cvt.rn.f16.f32 	%rs66, %f1122;
	mov.b32 	%r1499, {%rs66, %rs65};
	cvt.rn.f16.f32 	%rs67, %f1125;
	cvt.rn.f16.f32 	%rs68, %f1124;
	mov.b32 	%r1500, {%rs68, %rs67};
	cvt.rn.f16.f32 	%rs69, %f1127;
	cvt.rn.f16.f32 	%rs70, %f1126;
	mov.b32 	%r1501, {%rs70, %rs69};
	cvt.rn.f16.f32 	%rs71, %f1129;
	cvt.rn.f16.f32 	%rs72, %f1128;
	mov.b32 	%r1502, {%rs72, %rs71};
	cvt.rn.f16.f32 	%rs73, %f1131;
	cvt.rn.f16.f32 	%rs74, %f1130;
	mov.b32 	%r1503, {%rs74, %rs73};
	cvt.rn.f16.f32 	%rs75, %f1133;
	cvt.rn.f16.f32 	%rs76, %f1132;
	mov.b32 	%r1504, {%rs76, %rs75};
	cvt.rn.f16.f32 	%rs77, %f1135;
	cvt.rn.f16.f32 	%rs78, %f1134;
	mov.b32 	%r1505, {%rs78, %rs77};
	cvt.rn.f16.f32 	%rs79, %f1137;
	cvt.rn.f16.f32 	%rs80, %f1136;
	mov.b32 	%r1506, {%rs80, %rs79};
	cvt.rn.f16.f32 	%rs81, %f1139;
	cvt.rn.f16.f32 	%rs82, %f1138;
	mov.b32 	%r1507, {%rs82, %rs81};
	cvt.rn.f16.f32 	%rs83, %f1141;
	cvt.rn.f16.f32 	%rs84, %f1140;
	mov.b32 	%r1508, {%rs84, %rs83};
	cvt.rn.f16.f32 	%rs85, %f1143;
	cvt.rn.f16.f32 	%rs86, %f1142;
	mov.b32 	%r1509, {%rs86, %rs85};
	cvt.rn.f16.f32 	%rs87, %f1145;
	cvt.rn.f16.f32 	%rs88, %f1144;
	mov.b32 	%r1510, {%rs88, %rs87};
	cvt.rn.f16.f32 	%rs89, %f1147;
	cvt.rn.f16.f32 	%rs90, %f1146;
	mov.b32 	%r1511, {%rs90, %rs89};
	cvt.rn.f16.f32 	%rs91, %f1149;
	cvt.rn.f16.f32 	%rs92, %f1148;
	mov.b32 	%r1512, {%rs92, %rs91};
	cvt.rn.f16.f32 	%rs93, %f1151;
	cvt.rn.f16.f32 	%rs94, %f1150;
	mov.b32 	%r1513, {%rs94, %rs93};
	cvt.rn.f16.f32 	%rs95, %f1153;
	cvt.rn.f16.f32 	%rs96, %f1152;
	mov.b32 	%r1514, {%rs96, %rs95};
	.loc	1 120 18
	add.s64 	%rd25, %rd42, %rd87;
	add.s64 	%rd26, %rd42, %rd88;
	add.s64 	%rd27, %rd42, %rd89;
	add.s64 	%rd28, %rd42, %rd90;
	// begin inline asm
	@%p21 st.global.v4.b32 [ %rd25 + 0 ], { %r1499, %r1500, %r1501, %r1502 };
	// end inline asm
	// begin inline asm
	@%p22 st.global.v4.b32 [ %rd26 + 0 ], { %r1503, %r1504, %r1505, %r1506 };
	// end inline asm
	// begin inline asm
	@%p23 st.global.v4.b32 [ %rd27 + 0 ], { %r1507, %r1508, %r1509, %r1510 };
	// end inline asm
	// begin inline asm
	@%p24 st.global.v4.b32 [ %rd28 + 0 ], { %r1511, %r1512, %r1513, %r1514 };
	// end inline asm
	.loc	1 120 4
	ret;
$L__tmp1:
$L__func_end0:

}
	.file	1 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\fla\\ops\\common\\chunk_o.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 0
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 151
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 104
.b8 117
.b8 110
.b8 107
.b8 95
.b8 111
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 68
.b8 58
.b8 92
.b8 85
.b8 115
.b8 101
.b8 114
.b8 115
.b8 92
.b8 76
.b8 111
.b8 117
.b8 105
.b8 115
.b8 92
.b8 80
.b8 121
.b8 99
.b8 104
.b8 97
.b8 114
.b8 109
.b8 80
.b8 114
.b8 111
.b8 106
.b8 101
.b8 99
.b8 116
.b8 115
.b8 92
.b8 77
.b8 97
.b8 115
.b8 116
.b8 101
.b8 114
.b8 95
.b8 116
.b8 104
.b8 101
.b8 115
.b8 105
.b8 115
.b8 92
.b8 66
.b8 97
.b8 98
.b8 105
.b8 108
.b8 111
.b8 110
.b8 103
.b8 95
.b8 66
.b8 101
.b8 110
.b8 99
.b8 104
.b8 109
.b8 97
.b8 114
.b8 107
.b8 92
.b8 46
.b8 118
.b8 101
.b8 110
.b8 118
.b8 92
.b8 76
.b8 105
.b8 98
.b8 92
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 92
.b8 102
.b8 108
.b8 97
.b8 92
.b8 111
.b8 112
.b8 115
.b8 92
.b8 99
.b8 111
.b8 109
.b8 109
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
	}
	.section	.debug_loc	{	}
