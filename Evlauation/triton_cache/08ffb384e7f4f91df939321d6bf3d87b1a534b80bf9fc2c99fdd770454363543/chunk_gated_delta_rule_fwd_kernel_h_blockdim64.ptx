//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_86
.address_size 64

	// .globl	chunk_gated_delta_rule_fwd_kernel_h_blockdim64
.extern .shared .align 16 .b8 global_smem[];

.visible .entry chunk_gated_delta_rule_fwd_kernel_h_blockdim64(
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_0,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_1,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_2,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_3,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_4,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_5,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_6,
	.param .u64 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_7,
	.param .u32 chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<100>;
	.reg .b16 	%rs<129>;
	.reg .b32 	%r<1783>;
	.reg .f32 	%f<1411>;
	.reg .b64 	%rd<165>;
	.loc	1 37 0
$L__func_begin0:
	.loc	1 37 0

	ld.param.u64 	%rd24, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_5];
	ld.param.u64 	%rd22, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_2];
	ld.param.u64 	%rd21, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_1];
	ld.param.u64 	%rd20, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_0];
$L__tmp0:
	.loc	1 62 30
	// begin inline asm
	mov.u32 %r119, %ctaid.x;
	// end inline asm
	.loc	1 62 48
	// begin inline asm
	mov.u32 %r120, %ctaid.y;
	// end inline asm
	.loc	1 63 23
	shr.s32 	%r163, %r120, 31;
	shr.u32 	%r164, %r163, 28;
	add.s32 	%r165, %r120, %r164;
	shr.s32 	%r166, %r165, 4;
	and.b32  	%r167, %r165, -16;
	sub.s32 	%r168, %r120, %r167;
	ld.param.u64 	%rd50, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_6];
	ld.param.u64 	%rd51, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_7];
	.loc	1 65 40
	mul.wide.s32 	%rd52, %r166, 4;
	add.s64 	%rd25, %rd50, %rd52;
	mov.pred 	%p35, -1;
	.loc	1 65 27
	// begin inline asm
	mov.u32 %r121, 0x0;
	@%p35 ld.global.b32 { %r121 }, [ %rd25 + 0 ];
	// end inline asm
	.loc	1 65 86
	add.s64 	%rd26, %rd25, 4;
	.loc	1 65 67
	// begin inline asm
	mov.u32 %r122, 0x0;
	@%p35 ld.global.b32 { %r122 }, [ %rd26 + 0 ];
	// end inline asm
	.loc	1 66 18
	sub.s32 	%r169, %r122, %r121;
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r170, %r169, 63;
$L__tmp2:
	.loc	1 68 38
	mul.wide.s32 	%rd53, %r166, 8;
	add.s64 	%rd28, %rd51, %rd53;
	.loc	1 68 22
	// begin inline asm
	mov.u64 %rd27, 0x0;
	@%p35 ld.global.b64 { %rd27 }, [ %rd28 + 0 ];
	// end inline asm
	.loc	1 85 28
	shl.b32 	%r178, %r121, 11;
	shl.b32 	%r179, %r168, 7;
	add.s32 	%r180, %r178, %r179;
	.loc	1 85 9
	mul.wide.s32 	%rd55, %r180, 2;
	add.s64 	%rd56, %rd21, %rd55;
	.loc	1 86 9
	add.s64 	%rd57, %rd20, %rd55;
	.loc	1 87 9
	add.s64 	%rd58, %rd22, %rd55;
	.loc	1 114 79
	shl.b32 	%r181, %r119, 6;
	.loc	1 114 94
	cvt.s64.s32 	%rd3, %r181;
	.loc	1 115 23
	mov.u32 	%r3, %tid.x;
	and.b32  	%r4, %r3, 31;
	bfe.u32 	%r182, %r3, 3, 4;
	or.b32  	%r183, %r182, 16;
	or.b32  	%r184, %r182, 32;
	or.b32  	%r185, %r182, 48;
	shl.b32 	%r186, %r3, 3;
	and.b32  	%r187, %r186, 56;
	cvt.u64.u32 	%rd59, %r187;
	or.b64  	%rd9, %rd3, %rd59;
	setp.lt.u64 	%p36, %rd9, 128;
	.loc	1 113 21
	setp.gt.s32 	%p37, %r170, 63;
	.loc	1 127 22
	mul.wide.u32 	%rd60, %r182, 2048;
	mul.wide.u32 	%rd61, %r183, 2048;
	mul.wide.u32 	%rd62, %r184, 2048;
	mul.wide.u32 	%rd63, %r185, 2048;
	or.b64  	%rd64, %rd60, %rd59;
	or.b64  	%rd65, %rd61, %rd59;
	or.b64  	%rd66, %rd62, %rd59;
	or.b64  	%rd67, %rd63, %rd59;
	shl.b64 	%rd68, %rd64, 1;
	add.s64 	%rd29, %rd58, %rd68;
	shl.b64 	%rd69, %rd65, 1;
	add.s64 	%rd30, %rd58, %rd69;
	shl.b64 	%rd70, %rd66, 1;
	add.s64 	%rd31, %rd58, %rd70;
	shl.b64 	%rd71, %rd67, 1;
	add.s64 	%rd32, %rd58, %rd71;
	setp.lt.s32 	%p38, %r182, %r169;
	setp.lt.s32 	%p39, %r183, %r169;
	setp.lt.s32 	%p40, %r184, %r169;
	setp.lt.s32 	%p41, %r185, %r169;
	shl.b32 	%r188, %r182, 6;
	xor.b32  	%r189, %r186, %r3;
	and.b32  	%r190, %r189, 56;
	or.b32  	%r191, %r188, %r190;
	shl.b32 	%r192, %r191, 1;
	mov.u32 	%r193, global_smem;
	add.s32 	%r123, %r193, %r192;
	shl.b32 	%r194, %r183, 6;
	or.b32  	%r195, %r194, %r190;
	shl.b32 	%r196, %r195, 1;
	add.s32 	%r125, %r193, %r196;
	shl.b32 	%r197, %r184, 6;
	or.b32  	%r198, %r197, %r190;
	shl.b32 	%r199, %r198, 1;
	add.s32 	%r127, %r193, %r199;
	shl.b32 	%r200, %r185, 6;
	or.b32  	%r201, %r200, %r190;
	shl.b32 	%r202, %r201, 1;
	add.s32 	%r129, %r193, %r202;
	selp.b32 	%r203, 16, 0, %p37;
	selp.b32 	%r132, %r203, 0, %p38;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r123 + 0 ], [ %rd29 + 0 ], 0x10, %r132;
	// end inline asm
	selp.b32 	%r134, %r203, 0, %p39;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r125 + 0 ], [ %rd30 + 0 ], 0x10, %r134;
	// end inline asm
	selp.b32 	%r136, %r203, 0, %p40;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r127 + 0 ], [ %rd31 + 0 ], 0x10, %r136;
	// end inline asm
	selp.b32 	%r138, %r203, 0, %p41;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r129 + 0 ], [ %rd32 + 0 ], 0x10, %r138;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 131 26
	add.s64 	%rd33, %rd29, 128;
	add.s64 	%rd34, %rd30, 128;
	add.s64 	%rd35, %rd31, 128;
	add.s64 	%rd36, %rd32, 128;
	add.s32 	%r204, %r193, 8192;
	add.s32 	%r131, %r204, %r192;
	add.s32 	%r133, %r204, %r196;
	add.s32 	%r135, %r204, %r199;
	add.s32 	%r137, %r204, %r202;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r131 + 0 ], [ %rd33 + 0 ], 0x10, %r132;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r133 + 0 ], [ %rd34 + 0 ], 0x10, %r134;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r135 + 0 ], [ %rd35 + 0 ], 0x10, %r136;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r137 + 0 ], [ %rd36 + 0 ], 0x10, %r138;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 142 22
	add.s64 	%rd72, %rd9, %rd60;
	add.s64 	%rd73, %rd9, %rd61;
	add.s64 	%rd74, %rd9, %rd62;
	add.s64 	%rd75, %rd9, %rd63;
	shl.b64 	%rd76, %rd72, 1;
	add.s64 	%rd37, %rd56, %rd76;
	shl.b64 	%rd77, %rd73, 1;
	add.s64 	%rd38, %rd56, %rd77;
	shl.b64 	%rd78, %rd74, 1;
	add.s64 	%rd39, %rd56, %rd78;
	shl.b64 	%rd79, %rd75, 1;
	add.s64 	%rd40, %rd56, %rd79;
	and.pred  	%p99, %p36, %p38;
	and.pred  	%p98, %p36, %p39;
	and.pred  	%p97, %p36, %p40;
	and.pred  	%p96, %p36, %p41;
	or.b32  	%r205, %r188, %r187;
	shl.b32 	%r206, %r205, 1;
	add.s32 	%r207, %r193, 24576;
	add.s32 	%r139, %r207, %r206;
	or.b32  	%r208, %r194, %r187;
	shl.b32 	%r209, %r208, 1;
	add.s32 	%r141, %r207, %r209;
	or.b32  	%r210, %r197, %r187;
	shl.b32 	%r211, %r210, 1;
	add.s32 	%r143, %r207, %r211;
	or.b32  	%r212, %r200, %r187;
	shl.b32 	%r213, %r212, 1;
	add.s32 	%r145, %r207, %r213;
	selp.b32 	%r214, 16, 0, %p99;
	selp.b32 	%r140, %r214, 0, %p37;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r139 + 0 ], [ %rd37 + 0 ], 0x10, %r140;
	// end inline asm
	selp.b32 	%r215, 16, 0, %p98;
	selp.b32 	%r142, %r215, 0, %p37;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r141 + 0 ], [ %rd38 + 0 ], 0x10, %r142;
	// end inline asm
	selp.b32 	%r216, 16, 0, %p97;
	selp.b32 	%r144, %r216, 0, %p37;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r143 + 0 ], [ %rd39 + 0 ], 0x10, %r144;
	// end inline asm
	selp.b32 	%r217, 16, 0, %p96;
	selp.b32 	%r146, %r217, 0, %p37;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r145 + 0 ], [ %rd40 + 0 ], 0x10, %r146;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 183 22
	add.s64 	%rd41, %rd57, %rd68;
	add.s64 	%rd42, %rd57, %rd69;
	add.s64 	%rd43, %rd57, %rd70;
	add.s64 	%rd44, %rd57, %rd71;
	add.s32 	%r218, %r193, 16384;
	add.s32 	%r147, %r218, %r192;
	add.s32 	%r149, %r218, %r196;
	add.s32 	%r151, %r218, %r199;
	add.s32 	%r153, %r218, %r202;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r147 + 0 ], [ %rd41 + 0 ], 0x10, %r132;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r149 + 0 ], [ %rd42 + 0 ], 0x10, %r134;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r151 + 0 ], [ %rd43 + 0 ], 0x10, %r136;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r153 + 0 ], [ %rd44 + 0 ], 0x10, %r138;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 187 26
	add.s64 	%rd45, %rd41, 128;
	add.s64 	%rd46, %rd42, 128;
	add.s64 	%rd47, %rd43, 128;
	add.s64 	%rd48, %rd44, 128;
	add.s32 	%r219, %r193, 32768;
	add.s32 	%r155, %r219, %r192;
	add.s32 	%r157, %r219, %r196;
	add.s32 	%r159, %r219, %r199;
	add.s32 	%r161, %r219, %r202;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r155 + 0 ], [ %rd45 + 0 ], 0x10, %r132;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r157 + 0 ], [ %rd46 + 0 ], 0x10, %r134;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r159 + 0 ], [ %rd47 + 0 ], 0x10, %r136;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r161 + 0 ], [ %rd48 + 0 ], 0x10, %r138;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 127 22
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	bfe.u32 	%r25, %r3, 5, 2;
	bfe.u32 	%r220, %r3, 2, 3;
	shl.b32 	%r221, %r3, 1;
	and.b32  	%r26, %r221, 6;
	shl.b32 	%r222, %r25, 4;
	or.b32  	%r27, %r222, %r220;
	.loc	1 113 21
	@%p37 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	.loc	1 0 21
	ld.param.u64 	%rd23, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_3];
	ld.param.u64 	%rd49, [chunk_gated_delta_rule_fwd_kernel_h_blockdim64_param_4];
	shr.s32 	%r171, %r170, 31;
	shr.u32 	%r172, %r171, 26;
	add.s32 	%r173, %r170, %r172;
	shr.s32 	%r2, %r173, 6;
	cvt.u32.u64 	%r174, %rd27;
	shl.b32 	%r175, %r174, 18;
	shl.b32 	%r176, %r168, 14;
	add.s32 	%r177, %r175, %r176;
	mul.wide.s32 	%rd54, %r177, 2;
	add.s64 	%rd1, %rd49, %rd54;
	cvt.s64.s32 	%rd2, %r180;
	cvt.u64.u32 	%rd4, %r182;
	mul.wide.u32 	%rd5, %r182, 128;
	mul.wide.u32 	%rd6, %r183, 128;
	mul.wide.u32 	%rd7, %r184, 128;
	mul.wide.u32 	%rd8, %r185, 128;
	cvt.s64.s32 	%rd10, %r169;
	add.s32 	%r38, %r2, -1;
	mul.lo.s32 	%r224, %r27, 72;
	or.b32  	%r225, %r224, %r26;
	shl.b32 	%r226, %r225, 1;
	add.s32 	%r228, %r193, 40960;
	add.s32 	%r39, %r228, %r226;
	add.s32 	%r43, %r39, 1152;
	or.b32  	%r1781, %r26, 8;
	or.b32  	%r1780, %r26, 16;
	or.b32  	%r1779, %r26, 24;
	or.b32  	%r1778, %r26, 32;
	or.b32  	%r1777, %r26, 40;
	or.b32  	%r1776, %r26, 48;
	or.b32  	%r1775, %r26, 56;
	and.b32  	%r229, %r3, 7;
	shr.u32 	%r230, %r4, 3;
	shl.b32 	%r231, %r25, 2;
	or.b32  	%r232, %r231, %r230;
	shl.b32 	%r233, %r229, 3;
	mad.lo.s32 	%r234, %r232, 72, %r233;
	shl.b32 	%r235, %r234, 1;
	add.s32 	%r50, %r228, %r235;
	bfe.u32 	%r236, %r4, 3, 1;
	shr.u32 	%r1774, %r4, 4;
	shl.b32 	%r1773, %r25, 1;
	or.b32  	%r237, %r1773, %r236;
	xor.b32  	%r238, %r1774, %r229;
	shl.b32 	%r239, %r237, 9;
	shl.b32 	%r240, %r229, 6;
	or.b32  	%r241, %r239, %r240;
	shl.b32 	%r242, %r238, 3;
	or.b32  	%r243, %r241, %r242;
	shl.b32 	%r244, %r243, 1;
	add.s32 	%r412, %r193, %r244;
	or.b32  	%r245, %r1774, 2;
	xor.b32  	%r246, %r245, %r229;
	shl.b32 	%r247, %r246, 3;
	or.b32  	%r248, %r241, %r247;
	shl.b32 	%r249, %r248, 1;
	add.s32 	%r417, %r193, %r249;
	or.b32  	%r250, %r1774, 4;
	xor.b32  	%r251, %r250, %r229;
	shl.b32 	%r252, %r251, 3;
	or.b32  	%r253, %r241, %r252;
	shl.b32 	%r254, %r253, 1;
	add.s32 	%r422, %r193, %r254;
	or.b32  	%r255, %r1774, 6;
	xor.b32  	%r256, %r255, %r229;
	shl.b32 	%r257, %r256, 3;
	or.b32  	%r258, %r241, %r257;
	shl.b32 	%r259, %r258, 1;
	add.s32 	%r427, %r193, %r259;
	mad.lo.s32 	%r260, %r27, 66, %r26;
	shl.b32 	%r261, %r260, 1;
	add.s32 	%r57, %r228, %r261;
	add.s32 	%r58, %r57, 1056;
	shl.b32 	%r262, %r4, 1;
	mad.lo.s32 	%r263, %r25, 132, %r262;
	shl.b32 	%r264, %r263, 1;
	add.s32 	%r59, %r228, %r264;
	shr.u32 	%r1782, %r3, 4;
	and.b32  	%r265, %r1782, 6;
	or.b32  	%r266, %r265, 1;
	or.b32  	%r267, %r265, 9;
	or.b32  	%r268, %r265, 17;
	or.b32  	%r269, %r265, 25;
	or.b32  	%r270, %r265, 33;
	or.b32  	%r271, %r265, 41;
	or.b32  	%r272, %r265, 49;
	or.b32  	%r273, %r265, 57;
	shl.b32 	%r274, %r265, 6;
	shr.u32 	%r275, %r3, 2;
	xor.b32  	%r277, %r220, %r265;
	shl.b32 	%r278, %r277, 3;
	or.b32  	%r279, %r278, %r274;
	or.b32  	%r280, %r279, %r26;
	shl.b32 	%r281, %r280, 1;
	add.s32 	%r61, %r228, %r281;
	shl.b32 	%r282, %r266, 6;
	xor.b32  	%r283, %r266, %r220;
	shl.b32 	%r284, %r283, 3;
	or.b32  	%r285, %r284, %r282;
	or.b32  	%r286, %r285, %r26;
	shl.b32 	%r287, %r286, 1;
	add.s32 	%r62, %r228, %r287;
	shl.b32 	%r288, %r267, 6;
	xor.b32  	%r289, %r267, %r275;
	shl.b32 	%r290, %r289, 3;
	and.b32  	%r291, %r290, 56;
	or.b32  	%r292, %r291, %r288;
	or.b32  	%r293, %r292, %r26;
	shl.b32 	%r294, %r293, 1;
	add.s32 	%r63, %r228, %r294;
	shl.b32 	%r295, %r268, 6;
	xor.b32  	%r296, %r268, %r275;
	shl.b32 	%r297, %r296, 3;
	and.b32  	%r298, %r297, 56;
	or.b32  	%r299, %r298, %r295;
	or.b32  	%r300, %r299, %r26;
	shl.b32 	%r301, %r300, 1;
	add.s32 	%r64, %r228, %r301;
	shl.b32 	%r302, %r269, 6;
	xor.b32  	%r303, %r269, %r275;
	shl.b32 	%r304, %r303, 3;
	and.b32  	%r305, %r304, 56;
	or.b32  	%r306, %r305, %r302;
	or.b32  	%r307, %r306, %r26;
	shl.b32 	%r308, %r307, 1;
	add.s32 	%r65, %r228, %r308;
	shl.b32 	%r309, %r270, 6;
	xor.b32  	%r310, %r270, %r275;
	shl.b32 	%r311, %r310, 3;
	and.b32  	%r312, %r311, 56;
	or.b32  	%r313, %r312, %r309;
	or.b32  	%r314, %r313, %r26;
	shl.b32 	%r315, %r314, 1;
	add.s32 	%r66, %r228, %r315;
	shl.b32 	%r316, %r271, 6;
	xor.b32  	%r317, %r271, %r275;
	shl.b32 	%r318, %r317, 3;
	and.b32  	%r319, %r318, 56;
	or.b32  	%r320, %r319, %r316;
	or.b32  	%r321, %r320, %r26;
	shl.b32 	%r322, %r321, 1;
	add.s32 	%r67, %r228, %r322;
	shl.b32 	%r323, %r272, 6;
	xor.b32  	%r324, %r272, %r275;
	shl.b32 	%r325, %r324, 3;
	and.b32  	%r326, %r325, 56;
	or.b32  	%r327, %r326, %r323;
	or.b32  	%r328, %r327, %r26;
	shl.b32 	%r329, %r328, 1;
	add.s32 	%r68, %r228, %r329;
	shl.b32 	%r330, %r273, 6;
	xor.b32  	%r331, %r273, %r275;
	shl.b32 	%r332, %r331, 3;
	and.b32  	%r333, %r332, 56;
	or.b32  	%r334, %r333, %r330;
	or.b32  	%r335, %r334, %r26;
	shl.b32 	%r336, %r335, 1;
	add.s32 	%r69, %r228, %r336;
	shl.b32 	%r337, %r236, 9;
	or.b32  	%r338, %r337, %r240;
	or.b32  	%r339, %r338, %r242;
	shl.b32 	%r340, %r339, 1;
	add.s32 	%r432, %r228, %r340;
	add.s32 	%r437, %r432, 2048;
	add.s32 	%r442, %r432, 4096;
	add.s32 	%r447, %r432, 6144;
	or.b32  	%r341, %r247, %r338;
	shl.b32 	%r342, %r341, 1;
	add.s32 	%r452, %r228, %r342;
	add.s32 	%r457, %r452, 2048;
	add.s32 	%r462, %r452, 4096;
	add.s32 	%r467, %r452, 6144;
	or.b32  	%r343, %r252, %r338;
	shl.b32 	%r344, %r343, 1;
	add.s32 	%r472, %r228, %r344;
	add.s32 	%r477, %r472, 2048;
	add.s32 	%r482, %r472, 4096;
	add.s32 	%r487, %r472, 6144;
	or.b32  	%r345, %r257, %r338;
	shl.b32 	%r346, %r345, 1;
	add.s32 	%r492, %r228, %r346;
	add.s32 	%r497, %r492, 2048;
	add.s32 	%r502, %r492, 4096;
	add.s32 	%r507, %r492, 6144;
	shr.u32 	%r347, %r3, 1;
	and.b32  	%r348, %r347, 48;
	or.b32  	%r349, %r220, %r348;
	add.s32 	%r704, %r204, %r244;
	add.s32 	%r709, %r204, %r249;
	add.s32 	%r714, %r204, %r254;
	add.s32 	%r719, %r204, %r259;
	shl.b32 	%r351, %r349, 6;
	or.b32  	%r352, %r351, %r26;
	shl.b32 	%r353, %r352, 1;
	add.s32 	%r90, %r207, %r353;
	or.b32  	%r355, %r351, %r1781;
	shl.b32 	%r356, %r355, 1;
	add.s32 	%r92, %r207, %r356;
	or.b32  	%r357, %r351, %r1780;
	shl.b32 	%r358, %r357, 1;
	add.s32 	%r93, %r207, %r358;
	or.b32  	%r359, %r351, %r1779;
	shl.b32 	%r360, %r359, 1;
	add.s32 	%r94, %r207, %r360;
	or.b32  	%r361, %r351, %r1778;
	shl.b32 	%r362, %r361, 1;
	add.s32 	%r95, %r207, %r362;
	or.b32  	%r363, %r351, %r1777;
	shl.b32 	%r364, %r363, 1;
	add.s32 	%r96, %r207, %r364;
	or.b32  	%r365, %r351, %r1776;
	shl.b32 	%r366, %r365, 1;
	add.s32 	%r97, %r207, %r366;
	or.b32  	%r367, %r351, %r1775;
	shl.b32 	%r368, %r367, 1;
	add.s32 	%r98, %r207, %r368;
	xor.b32  	%r369, %r237, %r229;
	shl.b32 	%r370, %r229, 7;
	shl.b32 	%r371, %r1774, 10;
	or.b32  	%r372, %r371, %r370;
	shl.b32 	%r373, %r369, 4;
	or.b32  	%r374, %r373, %r372;
	add.s32 	%r375, %r193, %r374;
	add.s32 	%r1012, %r375, 16384;
	add.s32 	%r1017, %r375, 18432;
	add.s32 	%r1022, %r375, 20480;
	add.s32 	%r1027, %r375, 22528;
	add.s32 	%r1304, %r375, 32768;
	add.s32 	%r1309, %r375, 34816;
	add.s32 	%r1314, %r375, 36864;
	add.s32 	%r1319, %r375, 38912;
	.loc	1 113 21
	shl.b64 	%rd81, %rd4, 12;
	mul.wide.u32 	%rd82, %r229, 16;
	or.b64  	%rd83, %rd81, %rd82;
	shl.b64 	%rd84, %rd3, 1;
	shl.b64 	%rd85, %rd2, 1;
	add.s64 	%rd86, %rd83, %rd85;
	add.s64 	%rd87, %rd86, %rd84;
	add.s64 	%rd11, %rd23, %rd87;
	or.b64  	%rd163, %rd4, 112;
	add.s64 	%rd13, %rd20, %rd86;
	add.s64 	%rd14, %rd22, %rd86;
	add.s64 	%rd88, %rd3, %rd2;
	shl.b64 	%rd89, %rd88, 1;
	add.s64 	%rd90, %rd83, %rd89;
	add.s64 	%rd15, %rd21, %rd90;
	mov.b32 	%r1772, 0;
	mov.f32 	%f199, 0f00000000;
	mov.u64 	%rd164, 0;
	shl.b64 	%rd125, %rd5, 1;
	shl.b64 	%rd128, %rd6, 1;
	shl.b64 	%rd130, %rd7, 1;
	shl.b64 	%rd132, %rd8, 1;
	mov.f32 	%f1283, %f199;
	mov.f32 	%f1284, %f199;
	mov.f32 	%f1285, %f199;
	mov.f32 	%f1286, %f199;
	mov.f32 	%f1287, %f199;
	mov.f32 	%f1288, %f199;
	mov.f32 	%f1289, %f199;
	mov.f32 	%f1290, %f199;
	mov.f32 	%f1291, %f199;
	mov.f32 	%f1292, %f199;
	mov.f32 	%f1293, %f199;
	mov.f32 	%f1294, %f199;
	mov.f32 	%f1295, %f199;
	mov.f32 	%f1296, %f199;
	mov.f32 	%f1297, %f199;
	mov.f32 	%f1298, %f199;
	mov.f32 	%f1299, %f199;
	mov.f32 	%f1300, %f199;
	mov.f32 	%f1301, %f199;
	mov.f32 	%f1302, %f199;
	mov.f32 	%f1303, %f199;
	mov.f32 	%f1304, %f199;
	mov.f32 	%f1305, %f199;
	mov.f32 	%f1306, %f199;
	mov.f32 	%f1307, %f199;
	mov.f32 	%f1308, %f199;
	mov.f32 	%f1309, %f199;
	mov.f32 	%f1310, %f199;
	mov.f32 	%f1311, %f199;
	mov.f32 	%f1312, %f199;
	mov.f32 	%f1313, %f199;
	mov.f32 	%f1314, %f199;
	mov.f32 	%f1315, %f199;
	mov.f32 	%f1316, %f199;
	mov.f32 	%f1317, %f199;
	mov.f32 	%f1318, %f199;
	mov.f32 	%f1319, %f199;
	mov.f32 	%f1320, %f199;
	mov.f32 	%f1321, %f199;
	mov.f32 	%f1322, %f199;
	mov.f32 	%f1323, %f199;
	mov.f32 	%f1324, %f199;
	mov.f32 	%f1325, %f199;
	mov.f32 	%f1326, %f199;
	mov.f32 	%f1327, %f199;
	mov.f32 	%f1328, %f199;
	mov.f32 	%f1329, %f199;
	mov.f32 	%f1330, %f199;
	mov.f32 	%f1331, %f199;
	mov.f32 	%f1332, %f199;
	mov.f32 	%f1333, %f199;
	mov.f32 	%f1334, %f199;
	mov.f32 	%f1335, %f199;
	mov.f32 	%f1336, %f199;
	mov.f32 	%f1337, %f199;
	mov.f32 	%f1338, %f199;
	mov.f32 	%f1339, %f199;
	mov.f32 	%f1340, %f199;
	mov.f32 	%f1341, %f199;
	mov.f32 	%f1342, %f199;
	mov.f32 	%f1343, %f199;
	mov.f32 	%f1344, %f199;
	mov.f32 	%f1345, %f199;
	mov.f32 	%f1346, %f199;
$L__BB0_3:
	setp.lt.s32 	%p74, %r1772, %r38;
	cvt.u32.u64 	%r1552, %rd164;
	.loc	1 114 37
	mul.wide.s32 	%rd123, %r1552, 2;
	add.s64 	%rd124, %rd1, %rd123;
	.loc	1 115 31
	cvt.rn.f16.f32 	%rs1, %f1315;
	cvt.rn.f16.f32 	%rs2, %f1316;
	cvt.rn.f16.f32 	%rs3, %f1317;
	cvt.rn.f16.f32 	%rs4, %f1318;
	cvt.rn.f16.f32 	%rs5, %f1319;
	cvt.rn.f16.f32 	%rs6, %f1320;
	cvt.rn.f16.f32 	%rs7, %f1321;
	cvt.rn.f16.f32 	%rs8, %f1322;
	cvt.rn.f16.f32 	%rs9, %f1323;
	cvt.rn.f16.f32 	%rs10, %f1324;
	cvt.rn.f16.f32 	%rs11, %f1325;
	cvt.rn.f16.f32 	%rs12, %f1326;
	cvt.rn.f16.f32 	%rs13, %f1327;
	cvt.rn.f16.f32 	%rs14, %f1328;
	cvt.rn.f16.f32 	%rs15, %f1329;
	cvt.rn.f16.f32 	%rs16, %f1330;
	cvt.rn.f16.f32 	%rs17, %f1331;
	cvt.rn.f16.f32 	%rs18, %f1332;
	cvt.rn.f16.f32 	%rs19, %f1333;
	cvt.rn.f16.f32 	%rs20, %f1334;
	cvt.rn.f16.f32 	%rs21, %f1335;
	cvt.rn.f16.f32 	%rs22, %f1336;
	cvt.rn.f16.f32 	%rs23, %f1337;
	cvt.rn.f16.f32 	%rs24, %f1338;
	cvt.rn.f16.f32 	%rs25, %f1339;
	cvt.rn.f16.f32 	%rs26, %f1340;
	cvt.rn.f16.f32 	%rs27, %f1341;
	cvt.rn.f16.f32 	%rs28, %f1342;
	cvt.rn.f16.f32 	%rs29, %f1343;
	cvt.rn.f16.f32 	%rs30, %f1344;
	cvt.rn.f16.f32 	%rs31, %f1345;
	cvt.rn.f16.f32 	%rs32, %f1346;
	.loc	1 115 23
	add.s64 	%rd126, %rd124, %rd125;
	shl.b64 	%rd127, %rd9, 1;
	add.s64 	%rd91, %rd126, %rd127;
	add.s64 	%rd129, %rd124, %rd128;
	add.s64 	%rd92, %rd129, %rd127;
	add.s64 	%rd131, %rd124, %rd130;
	add.s64 	%rd93, %rd131, %rd127;
	add.s64 	%rd133, %rd124, %rd132;
	add.s64 	%rd94, %rd133, %rd127;
	mov.b32 	%r1553, {%rs1, %rs2};
	st.shared.b32 	[%r39], %r1553;
	mov.b32 	%r1554, {%rs3, %rs4};
	st.shared.b32 	[%r43], %r1554;
	mov.b32 	%r1555, {%rs5, %rs6};
	st.shared.b32 	[%r39+16], %r1555;
	mov.b32 	%r1556, {%rs7, %rs8};
	st.shared.b32 	[%r43+16], %r1556;
	mov.b32 	%r1557, {%rs9, %rs10};
	st.shared.b32 	[%r39+32], %r1557;
	mov.b32 	%r1558, {%rs11, %rs12};
	st.shared.b32 	[%r43+32], %r1558;
	mov.b32 	%r1559, {%rs13, %rs14};
	st.shared.b32 	[%r39+48], %r1559;
	mov.b32 	%r1560, {%rs15, %rs16};
	st.shared.b32 	[%r43+48], %r1560;
	mov.b32 	%r1561, {%rs17, %rs18};
	st.shared.b32 	[%r39+64], %r1561;
	mov.b32 	%r1562, {%rs19, %rs20};
	st.shared.b32 	[%r43+64], %r1562;
	mov.b32 	%r1563, {%rs21, %rs22};
	st.shared.b32 	[%r39+80], %r1563;
	mov.b32 	%r1564, {%rs23, %rs24};
	st.shared.b32 	[%r43+80], %r1564;
	mov.b32 	%r1565, {%rs25, %rs26};
	st.shared.b32 	[%r39+96], %r1565;
	mov.b32 	%r1566, {%rs27, %rs28};
	st.shared.b32 	[%r43+96], %r1566;
	mov.b32 	%r1567, {%rs29, %rs30};
	st.shared.b32 	[%r39+112], %r1567;
	mov.b32 	%r1568, {%rs31, %rs32};
	st.shared.b32 	[%r43+112], %r1568;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r380, %r381, %r382, %r383}, [%r50+2304];
	ld.shared.v4.u32 	{%r384, %r385, %r386, %r387}, [%r50+4608];
	ld.shared.v4.u32 	{%r388, %r389, %r390, %r391}, [%r50+6912];
	ld.shared.v4.u32 	{%r376, %r377, %r378, %r379}, [%r50];
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd91 + 0 ], { %r376, %r377, %r378, %r379 };
	// end inline asm
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd92 + 0 ], { %r380, %r381, %r382, %r383 };
	// end inline asm
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd93 + 0 ], { %r384, %r385, %r386, %r387 };
	// end inline asm
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd94 + 0 ], { %r388, %r389, %r390, %r391 };
	// end inline asm
	.loc	1 118 35
	cvt.rn.f16.f32 	%rs33, %f1283;
	cvt.rn.f16.f32 	%rs34, %f1284;
	cvt.rn.f16.f32 	%rs35, %f1285;
	cvt.rn.f16.f32 	%rs36, %f1286;
	cvt.rn.f16.f32 	%rs37, %f1287;
	cvt.rn.f16.f32 	%rs38, %f1288;
	cvt.rn.f16.f32 	%rs39, %f1289;
	cvt.rn.f16.f32 	%rs40, %f1290;
	cvt.rn.f16.f32 	%rs41, %f1291;
	cvt.rn.f16.f32 	%rs42, %f1292;
	cvt.rn.f16.f32 	%rs43, %f1293;
	cvt.rn.f16.f32 	%rs44, %f1294;
	cvt.rn.f16.f32 	%rs45, %f1295;
	cvt.rn.f16.f32 	%rs46, %f1296;
	cvt.rn.f16.f32 	%rs47, %f1297;
	cvt.rn.f16.f32 	%rs48, %f1298;
	cvt.rn.f16.f32 	%rs49, %f1299;
	cvt.rn.f16.f32 	%rs50, %f1300;
	cvt.rn.f16.f32 	%rs51, %f1301;
	cvt.rn.f16.f32 	%rs52, %f1302;
	cvt.rn.f16.f32 	%rs53, %f1303;
	cvt.rn.f16.f32 	%rs54, %f1304;
	cvt.rn.f16.f32 	%rs55, %f1305;
	cvt.rn.f16.f32 	%rs56, %f1306;
	cvt.rn.f16.f32 	%rs57, %f1307;
	cvt.rn.f16.f32 	%rs58, %f1308;
	cvt.rn.f16.f32 	%rs59, %f1309;
	cvt.rn.f16.f32 	%rs60, %f1310;
	cvt.rn.f16.f32 	%rs61, %f1311;
	cvt.rn.f16.f32 	%rs62, %f1312;
	cvt.rn.f16.f32 	%rs63, %f1313;
	cvt.rn.f16.f32 	%rs64, %f1314;
	.loc	1 118 27
	add.s64 	%rd95, %rd91, 16384;
	add.s64 	%rd96, %rd92, 16384;
	add.s64 	%rd97, %rd93, 16384;
	add.s64 	%rd98, %rd94, 16384;
	bar.sync 	0;
	mov.b32 	%r1569, {%rs33, %rs34};
	st.shared.b32 	[%r39], %r1569;
	mov.b32 	%r1570, {%rs35, %rs36};
	st.shared.b32 	[%r43], %r1570;
	mov.b32 	%r1571, {%rs37, %rs38};
	st.shared.b32 	[%r39+16], %r1571;
	mov.b32 	%r1572, {%rs39, %rs40};
	st.shared.b32 	[%r43+16], %r1572;
	mov.b32 	%r1573, {%rs41, %rs42};
	st.shared.b32 	[%r39+32], %r1573;
	mov.b32 	%r1574, {%rs43, %rs44};
	st.shared.b32 	[%r43+32], %r1574;
	mov.b32 	%r1575, {%rs45, %rs46};
	st.shared.b32 	[%r39+48], %r1575;
	mov.b32 	%r1576, {%rs47, %rs48};
	st.shared.b32 	[%r43+48], %r1576;
	mov.b32 	%r1577, {%rs49, %rs50};
	st.shared.b32 	[%r39+64], %r1577;
	mov.b32 	%r1578, {%rs51, %rs52};
	st.shared.b32 	[%r43+64], %r1578;
	mov.b32 	%r1579, {%rs53, %rs54};
	st.shared.b32 	[%r39+80], %r1579;
	mov.b32 	%r1580, {%rs55, %rs56};
	st.shared.b32 	[%r43+80], %r1580;
	mov.b32 	%r1581, {%rs57, %rs58};
	st.shared.b32 	[%r39+96], %r1581;
	mov.b32 	%r1582, {%rs59, %rs60};
	st.shared.b32 	[%r43+96], %r1582;
	mov.b32 	%r1583, {%rs61, %rs62};
	st.shared.b32 	[%r39+112], %r1583;
	mov.b32 	%r1584, {%rs63, %rs64};
	st.shared.b32 	[%r43+112], %r1584;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r396, %r397, %r398, %r399}, [%r50+2304];
	ld.shared.v4.u32 	{%r400, %r401, %r402, %r403}, [%r50+4608];
	ld.shared.v4.u32 	{%r404, %r405, %r406, %r407}, [%r50+6912];
	ld.shared.v4.u32 	{%r392, %r393, %r394, %r395}, [%r50];
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd95 + 0 ], { %r392, %r393, %r394, %r395 };
	// end inline asm
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd96 + 0 ], { %r396, %r397, %r398, %r399 };
	// end inline asm
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd97 + 0 ], { %r400, %r401, %r402, %r403 };
	// end inline asm
	// begin inline asm
	@%p36 st.global.v4.b32 [ %rd98 + 0 ], { %r404, %r405, %r406, %r407 };
	// end inline asm
	.loc	1 127 22
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r508, %r509, %r510, %r511 }, [ %r412 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r556, %r557, %r558, %r559 }, [ %r417 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r604, %r605, %r606, %r607 }, [ %r422 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r652, %r653, %r654, %r655 }, [ %r427 + 0 ];
	// end inline asm
	.loc	1 115 31
	bar.sync 	0;
	st.shared.b32 	[%r57], %r1553;
	st.shared.b32 	[%r58], %r1554;
	st.shared.b32 	[%r57+16], %r1555;
	st.shared.b32 	[%r58+16], %r1556;
	st.shared.b32 	[%r57+32], %r1557;
	st.shared.b32 	[%r58+32], %r1558;
	st.shared.b32 	[%r57+48], %r1559;
	st.shared.b32 	[%r58+48], %r1560;
	st.shared.b32 	[%r57+64], %r1561;
	st.shared.b32 	[%r58+64], %r1562;
	st.shared.b32 	[%r57+80], %r1563;
	st.shared.b32 	[%r58+80], %r1564;
	st.shared.b32 	[%r57+96], %r1565;
	st.shared.b32 	[%r58+96], %r1566;
	st.shared.b32 	[%r57+112], %r1567;
	st.shared.b32 	[%r58+112], %r1568;
	bar.sync 	0;
	ld.shared.b32 	%r1585, [%r59];
	ld.shared.b32 	%r1586, [%r59+132];
	ld.shared.b32 	%r1587, [%r59+1056];
	ld.shared.b32 	%r1588, [%r59+1188];
	ld.shared.b32 	%r1589, [%r59+2112];
	ld.shared.b32 	%r1590, [%r59+2244];
	ld.shared.b32 	%r1591, [%r59+3168];
	ld.shared.b32 	%r1592, [%r59+3300];
	ld.shared.b32 	%r1593, [%r59+4224];
	ld.shared.b32 	%r1594, [%r59+4356];
	ld.shared.b32 	%r1595, [%r59+5280];
	ld.shared.b32 	%r1596, [%r59+5412];
	ld.shared.b32 	%r1597, [%r59+6336];
	ld.shared.b32 	%r1598, [%r59+6468];
	ld.shared.b32 	%r1599, [%r59+7392];
	ld.shared.b32 	%r1600, [%r59+7524];
	bar.sync 	0;
	st.shared.b32 	[%r61], %r1585;
	st.shared.b32 	[%r62], %r1586;
	st.shared.b32 	[%r61+1024], %r1587;
	st.shared.b32 	[%r63], %r1588;
	st.shared.b32 	[%r61+2048], %r1589;
	st.shared.b32 	[%r64], %r1590;
	st.shared.b32 	[%r61+3072], %r1591;
	st.shared.b32 	[%r65], %r1592;
	st.shared.b32 	[%r61+4096], %r1593;
	st.shared.b32 	[%r66], %r1594;
	st.shared.b32 	[%r61+5120], %r1595;
	st.shared.b32 	[%r67], %r1596;
	st.shared.b32 	[%r61+6144], %r1597;
	st.shared.b32 	[%r68], %r1598;
	st.shared.b32 	[%r61+7168], %r1599;
	st.shared.b32 	[%r69], %r1600;
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r428, %r429, %r430, %r431 }, [ %r432 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r433, %r434, %r435, %r436 }, [ %r437 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r438, %r439, %r440, %r441 }, [ %r442 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r443, %r444, %r445, %r446 }, [ %r447 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r448, %r449, %r450, %r451 }, [ %r452 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r453, %r454, %r455, %r456 }, [ %r457 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r458, %r459, %r460, %r461 }, [ %r462 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r463, %r464, %r465, %r466 }, [ %r467 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r468, %r469, %r470, %r471 }, [ %r472 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r473, %r474, %r475, %r476 }, [ %r477 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r478, %r479, %r480, %r481 }, [ %r482 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r483, %r484, %r485, %r486 }, [ %r487 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r488, %r489, %r490, %r491 }, [ %r492 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r493, %r494, %r495, %r496 }, [ %r497 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r498, %r499, %r500, %r501 }, [ %r502 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r503, %r504, %r505, %r506 }, [ %r507 + 0 ];
	// end inline asm
	.loc	1 128 26
	mov.f32 	%f259, %f199;
	mov.f32 	%f260, %f199;
	mov.f32 	%f261, %f199;
	mov.f32 	%f262, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r508, %r509, %r510, %r511 }, { %r428, %r429 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	mov.f32 	%f267, %f199;
	mov.f32 	%f268, %f199;
	mov.f32 	%f269, %f199;
	mov.f32 	%f270, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r508, %r509, %r510, %r511 }, { %r430, %r431 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	mov.f32 	%f275, %f199;
	mov.f32 	%f276, %f199;
	mov.f32 	%f277, %f199;
	mov.f32 	%f278, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r508, %r509, %r510, %r511 }, { %r448, %r449 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	mov.f32 	%f283, %f199;
	mov.f32 	%f284, %f199;
	mov.f32 	%f285, %f199;
	mov.f32 	%f286, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r508, %r509, %r510, %r511 }, { %r450, %r451 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	mov.f32 	%f291, %f199;
	mov.f32 	%f292, %f199;
	mov.f32 	%f293, %f199;
	mov.f32 	%f294, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r508, %r509, %r510, %r511 }, { %r468, %r469 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	mov.f32 	%f299, %f199;
	mov.f32 	%f300, %f199;
	mov.f32 	%f301, %f199;
	mov.f32 	%f302, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r508, %r509, %r510, %r511 }, { %r470, %r471 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	mov.f32 	%f307, %f199;
	mov.f32 	%f308, %f199;
	mov.f32 	%f309, %f199;
	mov.f32 	%f310, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r508, %r509, %r510, %r511 }, { %r488, %r489 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	mov.f32 	%f315, %f199;
	mov.f32 	%f316, %f199;
	mov.f32 	%f317, %f199;
	mov.f32 	%f318, %f199;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r508, %r509, %r510, %r511 }, { %r490, %r491 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r556, %r557, %r558, %r559 }, { %r433, %r434 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r556, %r557, %r558, %r559 }, { %r435, %r436 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r556, %r557, %r558, %r559 }, { %r453, %r454 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r556, %r557, %r558, %r559 }, { %r455, %r456 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r556, %r557, %r558, %r559 }, { %r473, %r474 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r556, %r557, %r558, %r559 }, { %r475, %r476 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r556, %r557, %r558, %r559 }, { %r493, %r494 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r556, %r557, %r558, %r559 }, { %r495, %r496 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r604, %r605, %r606, %r607 }, { %r438, %r439 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r604, %r605, %r606, %r607 }, { %r440, %r441 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r604, %r605, %r606, %r607 }, { %r458, %r459 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r604, %r605, %r606, %r607 }, { %r460, %r461 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r604, %r605, %r606, %r607 }, { %r478, %r479 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r604, %r605, %r606, %r607 }, { %r480, %r481 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r604, %r605, %r606, %r607 }, { %r498, %r499 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r604, %r605, %r606, %r607 }, { %r500, %r501 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r652, %r653, %r654, %r655 }, { %r443, %r444 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r652, %r653, %r654, %r655 }, { %r445, %r446 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r652, %r653, %r654, %r655 }, { %r463, %r464 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r652, %r653, %r654, %r655 }, { %r465, %r466 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r652, %r653, %r654, %r655 }, { %r483, %r484 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r652, %r653, %r654, %r655 }, { %r485, %r486 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r652, %r653, %r654, %r655 }, { %r503, %r504 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r652, %r653, %r654, %r655 }, { %r505, %r506 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	.loc	1 131 26
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r800, %r801, %r802, %r803 }, [ %r704 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r848, %r849, %r850, %r851 }, [ %r709 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r896, %r897, %r898, %r899 }, [ %r714 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r944, %r945, %r946, %r947 }, [ %r719 + 0 ];
	// end inline asm
	.loc	1 118 35
	bar.sync 	0;
	st.shared.b32 	[%r57], %r1569;
	st.shared.b32 	[%r58], %r1570;
	st.shared.b32 	[%r57+16], %r1571;
	st.shared.b32 	[%r58+16], %r1572;
	st.shared.b32 	[%r57+32], %r1573;
	st.shared.b32 	[%r58+32], %r1574;
	st.shared.b32 	[%r57+48], %r1575;
	st.shared.b32 	[%r58+48], %r1576;
	st.shared.b32 	[%r57+64], %r1577;
	st.shared.b32 	[%r58+64], %r1578;
	st.shared.b32 	[%r57+80], %r1579;
	st.shared.b32 	[%r58+80], %r1580;
	st.shared.b32 	[%r57+96], %r1581;
	st.shared.b32 	[%r58+96], %r1582;
	st.shared.b32 	[%r57+112], %r1583;
	st.shared.b32 	[%r58+112], %r1584;
	bar.sync 	0;
	ld.shared.b32 	%r1601, [%r59];
	ld.shared.b32 	%r1602, [%r59+132];
	ld.shared.b32 	%r1603, [%r59+1056];
	ld.shared.b32 	%r1604, [%r59+1188];
	ld.shared.b32 	%r1605, [%r59+2112];
	ld.shared.b32 	%r1606, [%r59+2244];
	ld.shared.b32 	%r1607, [%r59+3168];
	ld.shared.b32 	%r1608, [%r59+3300];
	ld.shared.b32 	%r1609, [%r59+4224];
	ld.shared.b32 	%r1610, [%r59+4356];
	ld.shared.b32 	%r1611, [%r59+5280];
	ld.shared.b32 	%r1612, [%r59+5412];
	ld.shared.b32 	%r1613, [%r59+6336];
	ld.shared.b32 	%r1614, [%r59+6468];
	ld.shared.b32 	%r1615, [%r59+7392];
	ld.shared.b32 	%r1616, [%r59+7524];
	bar.sync 	0;
	st.shared.b32 	[%r61], %r1601;
	st.shared.b32 	[%r62], %r1602;
	st.shared.b32 	[%r61+1024], %r1603;
	st.shared.b32 	[%r63], %r1604;
	st.shared.b32 	[%r61+2048], %r1605;
	st.shared.b32 	[%r64], %r1606;
	st.shared.b32 	[%r61+3072], %r1607;
	st.shared.b32 	[%r65], %r1608;
	st.shared.b32 	[%r61+4096], %r1609;
	st.shared.b32 	[%r66], %r1610;
	st.shared.b32 	[%r61+5120], %r1611;
	st.shared.b32 	[%r67], %r1612;
	st.shared.b32 	[%r61+6144], %r1613;
	st.shared.b32 	[%r68], %r1614;
	st.shared.b32 	[%r61+7168], %r1615;
	st.shared.b32 	[%r69], %r1616;
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r720, %r721, %r722, %r723 }, [ %r432 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r725, %r726, %r727, %r728 }, [ %r437 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r730, %r731, %r732, %r733 }, [ %r442 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r735, %r736, %r737, %r738 }, [ %r447 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r740, %r741, %r742, %r743 }, [ %r452 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r745, %r746, %r747, %r748 }, [ %r457 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r750, %r751, %r752, %r753 }, [ %r462 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r755, %r756, %r757, %r758 }, [ %r467 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r760, %r761, %r762, %r763 }, [ %r472 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r765, %r766, %r767, %r768 }, [ %r477 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r770, %r771, %r772, %r773 }, [ %r482 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r775, %r776, %r777, %r778 }, [ %r487 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r780, %r781, %r782, %r783 }, [ %r492 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r785, %r786, %r787, %r788 }, [ %r497 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r790, %r791, %r792, %r793 }, [ %r502 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r795, %r796, %r797, %r798 }, [ %r507 + 0 ];
	// end inline asm
	.loc	1 132 31
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r800, %r801, %r802, %r803 }, { %r720, %r721 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r800, %r801, %r802, %r803 }, { %r722, %r723 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r800, %r801, %r802, %r803 }, { %r740, %r741 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r800, %r801, %r802, %r803 }, { %r742, %r743 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r800, %r801, %r802, %r803 }, { %r760, %r761 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r800, %r801, %r802, %r803 }, { %r762, %r763 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r800, %r801, %r802, %r803 }, { %r780, %r781 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r800, %r801, %r802, %r803 }, { %r782, %r783 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r848, %r849, %r850, %r851 }, { %r725, %r726 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r848, %r849, %r850, %r851 }, { %r727, %r728 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r848, %r849, %r850, %r851 }, { %r745, %r746 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r848, %r849, %r850, %r851 }, { %r747, %r748 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r848, %r849, %r850, %r851 }, { %r765, %r766 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r848, %r849, %r850, %r851 }, { %r767, %r768 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r848, %r849, %r850, %r851 }, { %r785, %r786 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r848, %r849, %r850, %r851 }, { %r787, %r788 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r896, %r897, %r898, %r899 }, { %r730, %r731 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r896, %r897, %r898, %r899 }, { %r732, %r733 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r896, %r897, %r898, %r899 }, { %r750, %r751 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r896, %r897, %r898, %r899 }, { %r752, %r753 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r896, %r897, %r898, %r899 }, { %r770, %r771 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r896, %r897, %r898, %r899 }, { %r772, %r773 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r896, %r897, %r898, %r899 }, { %r790, %r791 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r896, %r897, %r898, %r899 }, { %r792, %r793 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f259, %f260, %f261, %f262 }, { %r944, %r945, %r946, %r947 }, { %r735, %r736 }, { %f259, %f260, %f261, %f262 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f267, %f268, %f269, %f270 }, { %r944, %r945, %r946, %r947 }, { %r737, %r738 }, { %f267, %f268, %f269, %f270 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f275, %f276, %f277, %f278 }, { %r944, %r945, %r946, %r947 }, { %r755, %r756 }, { %f275, %f276, %f277, %f278 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f283, %f284, %f285, %f286 }, { %r944, %r945, %r946, %r947 }, { %r757, %r758 }, { %f283, %f284, %f285, %f286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f291, %f292, %f293, %f294 }, { %r944, %r945, %r946, %r947 }, { %r775, %r776 }, { %f291, %f292, %f293, %f294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f299, %f300, %f301, %f302 }, { %r944, %r945, %r946, %r947 }, { %r777, %r778 }, { %f299, %f300, %f301, %f302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f307, %f308, %f309, %f310 }, { %r944, %r945, %r946, %r947 }, { %r795, %r796 }, { %f307, %f308, %f309, %f310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f315, %f316, %f317, %f318 }, { %r944, %r945, %r946, %r947 }, { %r797, %r798 }, { %f315, %f316, %f317, %f318 };
	// end inline asm
	.loc	1 142 52
	ld.shared.b32 	%r1617, [%r90];
	mov.b32 	{%rs65, %rs66}, %r1617;
	cvt.f32.f16 	%f1219, %rs66;
	cvt.f32.f16 	%f1220, %rs65;
	sub.f32 	%f1221, %f1220, %f259;
	sub.f32 	%f1222, %f1219, %f260;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs67, %f1222;
	cvt.rn.f16.f32 	%rs68, %f1221;
	mov.b32 	%r1618, {%rs68, %rs67};
	.loc	1 142 52
	ld.shared.b32 	%r1619, [%r90+1024];
	mov.b32 	{%rs69, %rs70}, %r1619;
	cvt.f32.f16 	%f1223, %rs70;
	cvt.f32.f16 	%f1224, %rs69;
	sub.f32 	%f1225, %f1224, %f261;
	sub.f32 	%f1226, %f1223, %f262;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs71, %f1226;
	cvt.rn.f16.f32 	%rs72, %f1225;
	mov.b32 	%r1620, {%rs72, %rs71};
	.loc	1 142 52
	ld.shared.b32 	%r1621, [%r90+16];
	mov.b32 	{%rs73, %rs74}, %r1621;
	cvt.f32.f16 	%f1227, %rs74;
	cvt.f32.f16 	%f1228, %rs73;
	sub.f32 	%f1229, %f1228, %f267;
	sub.f32 	%f1230, %f1227, %f268;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs75, %f1230;
	cvt.rn.f16.f32 	%rs76, %f1229;
	mov.b32 	%r1622, {%rs76, %rs75};
	.loc	1 142 52
	ld.shared.b32 	%r1623, [%r92+1024];
	mov.b32 	{%rs77, %rs78}, %r1623;
	cvt.f32.f16 	%f1231, %rs78;
	cvt.f32.f16 	%f1232, %rs77;
	sub.f32 	%f1233, %f1232, %f269;
	sub.f32 	%f1234, %f1231, %f270;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs79, %f1234;
	cvt.rn.f16.f32 	%rs80, %f1233;
	mov.b32 	%r1624, {%rs80, %rs79};
	.loc	1 142 52
	ld.shared.b32 	%r1625, [%r90+32];
	mov.b32 	{%rs81, %rs82}, %r1625;
	cvt.f32.f16 	%f1235, %rs82;
	cvt.f32.f16 	%f1236, %rs81;
	sub.f32 	%f1237, %f1236, %f275;
	sub.f32 	%f1238, %f1235, %f276;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs83, %f1238;
	cvt.rn.f16.f32 	%rs84, %f1237;
	mov.b32 	%r1626, {%rs84, %rs83};
	.loc	1 142 52
	ld.shared.b32 	%r1627, [%r93+1024];
	mov.b32 	{%rs85, %rs86}, %r1627;
	cvt.f32.f16 	%f1239, %rs86;
	cvt.f32.f16 	%f1240, %rs85;
	sub.f32 	%f1241, %f1240, %f277;
	sub.f32 	%f1242, %f1239, %f278;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs87, %f1242;
	cvt.rn.f16.f32 	%rs88, %f1241;
	mov.b32 	%r1628, {%rs88, %rs87};
	.loc	1 142 52
	ld.shared.b32 	%r1629, [%r90+48];
	mov.b32 	{%rs89, %rs90}, %r1629;
	cvt.f32.f16 	%f1243, %rs90;
	cvt.f32.f16 	%f1244, %rs89;
	sub.f32 	%f1245, %f1244, %f283;
	sub.f32 	%f1246, %f1243, %f284;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs91, %f1246;
	cvt.rn.f16.f32 	%rs92, %f1245;
	mov.b32 	%r1630, {%rs92, %rs91};
	.loc	1 142 52
	ld.shared.b32 	%r1631, [%r94+1024];
	mov.b32 	{%rs93, %rs94}, %r1631;
	cvt.f32.f16 	%f1247, %rs94;
	cvt.f32.f16 	%f1248, %rs93;
	sub.f32 	%f1249, %f1248, %f285;
	sub.f32 	%f1250, %f1247, %f286;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs95, %f1250;
	cvt.rn.f16.f32 	%rs96, %f1249;
	mov.b32 	%r1632, {%rs96, %rs95};
	.loc	1 142 52
	ld.shared.b32 	%r1633, [%r90+64];
	mov.b32 	{%rs97, %rs98}, %r1633;
	cvt.f32.f16 	%f1251, %rs98;
	cvt.f32.f16 	%f1252, %rs97;
	sub.f32 	%f1253, %f1252, %f291;
	sub.f32 	%f1254, %f1251, %f292;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs99, %f1254;
	cvt.rn.f16.f32 	%rs100, %f1253;
	mov.b32 	%r1634, {%rs100, %rs99};
	.loc	1 142 52
	ld.shared.b32 	%r1635, [%r95+1024];
	mov.b32 	{%rs101, %rs102}, %r1635;
	cvt.f32.f16 	%f1255, %rs102;
	cvt.f32.f16 	%f1256, %rs101;
	sub.f32 	%f1257, %f1256, %f293;
	sub.f32 	%f1258, %f1255, %f294;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs103, %f1258;
	cvt.rn.f16.f32 	%rs104, %f1257;
	mov.b32 	%r1636, {%rs104, %rs103};
	.loc	1 142 52
	ld.shared.b32 	%r1637, [%r90+80];
	mov.b32 	{%rs105, %rs106}, %r1637;
	cvt.f32.f16 	%f1259, %rs106;
	cvt.f32.f16 	%f1260, %rs105;
	sub.f32 	%f1261, %f1260, %f299;
	sub.f32 	%f1262, %f1259, %f300;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs107, %f1262;
	cvt.rn.f16.f32 	%rs108, %f1261;
	mov.b32 	%r1638, {%rs108, %rs107};
	.loc	1 142 52
	ld.shared.b32 	%r1639, [%r96+1024];
	mov.b32 	{%rs109, %rs110}, %r1639;
	cvt.f32.f16 	%f1263, %rs110;
	cvt.f32.f16 	%f1264, %rs109;
	sub.f32 	%f1265, %f1264, %f301;
	sub.f32 	%f1266, %f1263, %f302;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs111, %f1266;
	cvt.rn.f16.f32 	%rs112, %f1265;
	mov.b32 	%r1640, {%rs112, %rs111};
	.loc	1 142 52
	ld.shared.b32 	%r1641, [%r90+96];
	mov.b32 	{%rs113, %rs114}, %r1641;
	cvt.f32.f16 	%f1267, %rs114;
	cvt.f32.f16 	%f1268, %rs113;
	sub.f32 	%f1269, %f1268, %f307;
	sub.f32 	%f1270, %f1267, %f308;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs115, %f1270;
	cvt.rn.f16.f32 	%rs116, %f1269;
	mov.b32 	%r1642, {%rs116, %rs115};
	.loc	1 142 52
	ld.shared.b32 	%r1643, [%r97+1024];
	mov.b32 	{%rs117, %rs118}, %r1643;
	cvt.f32.f16 	%f1271, %rs118;
	cvt.f32.f16 	%f1272, %rs117;
	sub.f32 	%f1273, %f1272, %f309;
	sub.f32 	%f1274, %f1271, %f310;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs119, %f1274;
	cvt.rn.f16.f32 	%rs120, %f1273;
	mov.b32 	%r1644, {%rs120, %rs119};
	.loc	1 142 52
	ld.shared.b32 	%r1645, [%r90+112];
	mov.b32 	{%rs121, %rs122}, %r1645;
	cvt.f32.f16 	%f1275, %rs122;
	cvt.f32.f16 	%f1276, %rs121;
	sub.f32 	%f1277, %f1276, %f315;
	sub.f32 	%f1278, %f1275, %f316;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs123, %f1278;
	cvt.rn.f16.f32 	%rs124, %f1277;
	mov.b32 	%r1646, {%rs124, %rs123};
	.loc	1 142 52
	ld.shared.b32 	%r1647, [%r98+1024];
	mov.b32 	{%rs125, %rs126}, %r1647;
	cvt.f32.f16 	%f1279, %rs126;
	cvt.f32.f16 	%f1280, %rs125;
	sub.f32 	%f1281, %f1280, %f317;
	sub.f32 	%f1282, %f1279, %f318;
	.loc	1 146 33
	cvt.rn.f16.f32 	%rs127, %f1282;
	cvt.rn.f16.f32 	%rs128, %f1281;
	mov.b32 	%r1648, {%rs128, %rs127};
	bar.sync 	0;
	st.shared.b32 	[%r39], %r1618;
	st.shared.b32 	[%r43], %r1620;
	st.shared.b32 	[%r39+16], %r1622;
	st.shared.b32 	[%r43+16], %r1624;
	st.shared.b32 	[%r39+32], %r1626;
	st.shared.b32 	[%r43+32], %r1628;
	st.shared.b32 	[%r39+48], %r1630;
	st.shared.b32 	[%r43+48], %r1632;
	st.shared.b32 	[%r39+64], %r1634;
	st.shared.b32 	[%r43+64], %r1636;
	st.shared.b32 	[%r39+80], %r1638;
	st.shared.b32 	[%r43+80], %r1640;
	st.shared.b32 	[%r39+96], %r1642;
	st.shared.b32 	[%r43+96], %r1644;
	st.shared.b32 	[%r39+112], %r1646;
	st.shared.b32 	[%r43+112], %r1648;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r992, %r993, %r994, %r995}, [%r50];
	ld.shared.v4.u32 	{%r996, %r997, %r998, %r999}, [%r50+2304];
	ld.shared.v4.u32 	{%r1000, %r1001, %r1002, %r1003}, [%r50+4608];
	ld.shared.v4.u32 	{%r1004, %r1005, %r1006, %r1007}, [%r50+6912];
	.loc	1 146 26
	add.s64 	%rd99, %rd11, %rd164;
	add.s64 	%rd100, %rd99, 65536;
	add.s64 	%rd101, %rd99, 131072;
	add.s64 	%rd102, %rd99, 196608;
	// begin inline asm
	@%p99 st.global.v4.b32 [ %rd99 + 0 ], { %r992, %r993, %r994, %r995 };
	// end inline asm
	// begin inline asm
	@%p98 st.global.v4.b32 [ %rd100 + 0 ], { %r996, %r997, %r998, %r999 };
	// end inline asm
	// begin inline asm
	@%p97 st.global.v4.b32 [ %rd101 + 0 ], { %r1000, %r1001, %r1002, %r1003 };
	// end inline asm
	// begin inline asm
	@%p96 st.global.v4.b32 [ %rd102 + 0 ], { %r1004, %r1005, %r1006, %r1007 };
	// end inline asm
	.loc	1 183 22
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1108, %r1109, %r1110, %r1111 }, [ %r1012 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1156, %r1157, %r1158, %r1159 }, [ %r1017 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1204, %r1205, %r1206, %r1207 }, [ %r1022 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1252, %r1253, %r1254, %r1255 }, [ %r1027 + 0 ];
	// end inline asm
	.loc	1 146 33
	bar.sync 	0;
	st.shared.b32 	[%r57], %r1618;
	st.shared.b32 	[%r58], %r1620;
	st.shared.b32 	[%r57+16], %r1622;
	st.shared.b32 	[%r58+16], %r1624;
	st.shared.b32 	[%r57+32], %r1626;
	st.shared.b32 	[%r58+32], %r1628;
	st.shared.b32 	[%r57+48], %r1630;
	st.shared.b32 	[%r58+48], %r1632;
	st.shared.b32 	[%r57+64], %r1634;
	st.shared.b32 	[%r58+64], %r1636;
	st.shared.b32 	[%r57+80], %r1638;
	st.shared.b32 	[%r58+80], %r1640;
	st.shared.b32 	[%r57+96], %r1642;
	st.shared.b32 	[%r58+96], %r1644;
	st.shared.b32 	[%r57+112], %r1646;
	st.shared.b32 	[%r58+112], %r1648;
	bar.sync 	0;
	ld.shared.b32 	%r1649, [%r59];
	ld.shared.b32 	%r1650, [%r59+132];
	ld.shared.b32 	%r1651, [%r59+1056];
	ld.shared.b32 	%r1652, [%r59+1188];
	ld.shared.b32 	%r1653, [%r59+2112];
	ld.shared.b32 	%r1654, [%r59+2244];
	ld.shared.b32 	%r1655, [%r59+3168];
	ld.shared.b32 	%r1656, [%r59+3300];
	ld.shared.b32 	%r1657, [%r59+4224];
	ld.shared.b32 	%r1658, [%r59+4356];
	ld.shared.b32 	%r1659, [%r59+5280];
	ld.shared.b32 	%r1660, [%r59+5412];
	ld.shared.b32 	%r1661, [%r59+6336];
	ld.shared.b32 	%r1662, [%r59+6468];
	ld.shared.b32 	%r1663, [%r59+7392];
	ld.shared.b32 	%r1664, [%r59+7524];
	bar.sync 	0;
	st.shared.b32 	[%r61], %r1649;
	st.shared.b32 	[%r62], %r1650;
	st.shared.b32 	[%r61+1024], %r1651;
	st.shared.b32 	[%r63], %r1652;
	st.shared.b32 	[%r61+2048], %r1653;
	st.shared.b32 	[%r64], %r1654;
	st.shared.b32 	[%r61+3072], %r1655;
	st.shared.b32 	[%r65], %r1656;
	st.shared.b32 	[%r61+4096], %r1657;
	st.shared.b32 	[%r66], %r1658;
	st.shared.b32 	[%r61+5120], %r1659;
	st.shared.b32 	[%r67], %r1660;
	st.shared.b32 	[%r61+6144], %r1661;
	st.shared.b32 	[%r68], %r1662;
	st.shared.b32 	[%r61+7168], %r1663;
	st.shared.b32 	[%r69], %r1664;
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1112, %r1113, %r1118, %r1119 }, [ %r432 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1160, %r1161, %r1166, %r1167 }, [ %r437 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1208, %r1209, %r1214, %r1215 }, [ %r442 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1256, %r1257, %r1262, %r1263 }, [ %r447 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1124, %r1125, %r1130, %r1131 }, [ %r452 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1172, %r1173, %r1178, %r1179 }, [ %r457 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1220, %r1221, %r1226, %r1227 }, [ %r462 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1268, %r1269, %r1274, %r1275 }, [ %r467 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1136, %r1137, %r1142, %r1143 }, [ %r472 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1184, %r1185, %r1190, %r1191 }, [ %r477 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1232, %r1233, %r1238, %r1239 }, [ %r482 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1280, %r1281, %r1286, %r1287 }, [ %r487 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1148, %r1149, %r1154, %r1155 }, [ %r492 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1196, %r1197, %r1202, %r1203 }, [ %r497 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1244, %r1245, %r1250, %r1251 }, [ %r502 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1292, %r1293, %r1298, %r1299 }, [ %r507 + 0 ];
	// end inline asm
	.loc	1 184 28
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1315, %f1316, %f1317, %f1318 }, { %r1108, %r1109, %r1110, %r1111 }, { %r1112, %r1113 }, { %f1315, %f1316, %f1317, %f1318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1319, %f1320, %f1321, %f1322 }, { %r1108, %r1109, %r1110, %r1111 }, { %r1118, %r1119 }, { %f1319, %f1320, %f1321, %f1322 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1323, %f1324, %f1325, %f1326 }, { %r1108, %r1109, %r1110, %r1111 }, { %r1124, %r1125 }, { %f1323, %f1324, %f1325, %f1326 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1327, %f1328, %f1329, %f1330 }, { %r1108, %r1109, %r1110, %r1111 }, { %r1130, %r1131 }, { %f1327, %f1328, %f1329, %f1330 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1331, %f1332, %f1333, %f1334 }, { %r1108, %r1109, %r1110, %r1111 }, { %r1136, %r1137 }, { %f1331, %f1332, %f1333, %f1334 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1335, %f1336, %f1337, %f1338 }, { %r1108, %r1109, %r1110, %r1111 }, { %r1142, %r1143 }, { %f1335, %f1336, %f1337, %f1338 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1339, %f1340, %f1341, %f1342 }, { %r1108, %r1109, %r1110, %r1111 }, { %r1148, %r1149 }, { %f1339, %f1340, %f1341, %f1342 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1343, %f1344, %f1345, %f1346 }, { %r1108, %r1109, %r1110, %r1111 }, { %r1154, %r1155 }, { %f1343, %f1344, %f1345, %f1346 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1315, %f1316, %f1317, %f1318 }, { %r1156, %r1157, %r1158, %r1159 }, { %r1160, %r1161 }, { %f1315, %f1316, %f1317, %f1318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1319, %f1320, %f1321, %f1322 }, { %r1156, %r1157, %r1158, %r1159 }, { %r1166, %r1167 }, { %f1319, %f1320, %f1321, %f1322 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1323, %f1324, %f1325, %f1326 }, { %r1156, %r1157, %r1158, %r1159 }, { %r1172, %r1173 }, { %f1323, %f1324, %f1325, %f1326 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1327, %f1328, %f1329, %f1330 }, { %r1156, %r1157, %r1158, %r1159 }, { %r1178, %r1179 }, { %f1327, %f1328, %f1329, %f1330 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1331, %f1332, %f1333, %f1334 }, { %r1156, %r1157, %r1158, %r1159 }, { %r1184, %r1185 }, { %f1331, %f1332, %f1333, %f1334 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1335, %f1336, %f1337, %f1338 }, { %r1156, %r1157, %r1158, %r1159 }, { %r1190, %r1191 }, { %f1335, %f1336, %f1337, %f1338 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1339, %f1340, %f1341, %f1342 }, { %r1156, %r1157, %r1158, %r1159 }, { %r1196, %r1197 }, { %f1339, %f1340, %f1341, %f1342 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1343, %f1344, %f1345, %f1346 }, { %r1156, %r1157, %r1158, %r1159 }, { %r1202, %r1203 }, { %f1343, %f1344, %f1345, %f1346 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1315, %f1316, %f1317, %f1318 }, { %r1204, %r1205, %r1206, %r1207 }, { %r1208, %r1209 }, { %f1315, %f1316, %f1317, %f1318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1319, %f1320, %f1321, %f1322 }, { %r1204, %r1205, %r1206, %r1207 }, { %r1214, %r1215 }, { %f1319, %f1320, %f1321, %f1322 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1323, %f1324, %f1325, %f1326 }, { %r1204, %r1205, %r1206, %r1207 }, { %r1220, %r1221 }, { %f1323, %f1324, %f1325, %f1326 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1327, %f1328, %f1329, %f1330 }, { %r1204, %r1205, %r1206, %r1207 }, { %r1226, %r1227 }, { %f1327, %f1328, %f1329, %f1330 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1331, %f1332, %f1333, %f1334 }, { %r1204, %r1205, %r1206, %r1207 }, { %r1232, %r1233 }, { %f1331, %f1332, %f1333, %f1334 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1335, %f1336, %f1337, %f1338 }, { %r1204, %r1205, %r1206, %r1207 }, { %r1238, %r1239 }, { %f1335, %f1336, %f1337, %f1338 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1339, %f1340, %f1341, %f1342 }, { %r1204, %r1205, %r1206, %r1207 }, { %r1244, %r1245 }, { %f1339, %f1340, %f1341, %f1342 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1343, %f1344, %f1345, %f1346 }, { %r1204, %r1205, %r1206, %r1207 }, { %r1250, %r1251 }, { %f1343, %f1344, %f1345, %f1346 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1315, %f1316, %f1317, %f1318 }, { %r1252, %r1253, %r1254, %r1255 }, { %r1256, %r1257 }, { %f1315, %f1316, %f1317, %f1318 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1319, %f1320, %f1321, %f1322 }, { %r1252, %r1253, %r1254, %r1255 }, { %r1262, %r1263 }, { %f1319, %f1320, %f1321, %f1322 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1323, %f1324, %f1325, %f1326 }, { %r1252, %r1253, %r1254, %r1255 }, { %r1268, %r1269 }, { %f1323, %f1324, %f1325, %f1326 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1327, %f1328, %f1329, %f1330 }, { %r1252, %r1253, %r1254, %r1255 }, { %r1274, %r1275 }, { %f1327, %f1328, %f1329, %f1330 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1331, %f1332, %f1333, %f1334 }, { %r1252, %r1253, %r1254, %r1255 }, { %r1280, %r1281 }, { %f1331, %f1332, %f1333, %f1334 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1335, %f1336, %f1337, %f1338 }, { %r1252, %r1253, %r1254, %r1255 }, { %r1286, %r1287 }, { %f1335, %f1336, %f1337, %f1338 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1339, %f1340, %f1341, %f1342 }, { %r1252, %r1253, %r1254, %r1255 }, { %r1292, %r1293 }, { %f1339, %f1340, %f1341, %f1342 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1343, %f1344, %f1345, %f1346 }, { %r1252, %r1253, %r1254, %r1255 }, { %r1298, %r1299 }, { %f1343, %f1344, %f1345, %f1346 };
	// end inline asm
	.loc	1 187 26
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1320, %r1321, %r1322, %r1323 }, [ %r1304 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1368, %r1369, %r1370, %r1371 }, [ %r1309 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1416, %r1417, %r1418, %r1419 }, [ %r1314 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1464, %r1465, %r1466, %r1467 }, [ %r1319 + 0 ];
	// end inline asm
	.loc	1 188 32
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1283, %f1284, %f1285, %f1286 }, { %r1320, %r1321, %r1322, %r1323 }, { %r1112, %r1113 }, { %f1283, %f1284, %f1285, %f1286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1287, %f1288, %f1289, %f1290 }, { %r1320, %r1321, %r1322, %r1323 }, { %r1118, %r1119 }, { %f1287, %f1288, %f1289, %f1290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1291, %f1292, %f1293, %f1294 }, { %r1320, %r1321, %r1322, %r1323 }, { %r1124, %r1125 }, { %f1291, %f1292, %f1293, %f1294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1295, %f1296, %f1297, %f1298 }, { %r1320, %r1321, %r1322, %r1323 }, { %r1130, %r1131 }, { %f1295, %f1296, %f1297, %f1298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1299, %f1300, %f1301, %f1302 }, { %r1320, %r1321, %r1322, %r1323 }, { %r1136, %r1137 }, { %f1299, %f1300, %f1301, %f1302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1303, %f1304, %f1305, %f1306 }, { %r1320, %r1321, %r1322, %r1323 }, { %r1142, %r1143 }, { %f1303, %f1304, %f1305, %f1306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1307, %f1308, %f1309, %f1310 }, { %r1320, %r1321, %r1322, %r1323 }, { %r1148, %r1149 }, { %f1307, %f1308, %f1309, %f1310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1311, %f1312, %f1313, %f1314 }, { %r1320, %r1321, %r1322, %r1323 }, { %r1154, %r1155 }, { %f1311, %f1312, %f1313, %f1314 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1283, %f1284, %f1285, %f1286 }, { %r1368, %r1369, %r1370, %r1371 }, { %r1160, %r1161 }, { %f1283, %f1284, %f1285, %f1286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1287, %f1288, %f1289, %f1290 }, { %r1368, %r1369, %r1370, %r1371 }, { %r1166, %r1167 }, { %f1287, %f1288, %f1289, %f1290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1291, %f1292, %f1293, %f1294 }, { %r1368, %r1369, %r1370, %r1371 }, { %r1172, %r1173 }, { %f1291, %f1292, %f1293, %f1294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1295, %f1296, %f1297, %f1298 }, { %r1368, %r1369, %r1370, %r1371 }, { %r1178, %r1179 }, { %f1295, %f1296, %f1297, %f1298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1299, %f1300, %f1301, %f1302 }, { %r1368, %r1369, %r1370, %r1371 }, { %r1184, %r1185 }, { %f1299, %f1300, %f1301, %f1302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1303, %f1304, %f1305, %f1306 }, { %r1368, %r1369, %r1370, %r1371 }, { %r1190, %r1191 }, { %f1303, %f1304, %f1305, %f1306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1307, %f1308, %f1309, %f1310 }, { %r1368, %r1369, %r1370, %r1371 }, { %r1196, %r1197 }, { %f1307, %f1308, %f1309, %f1310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1311, %f1312, %f1313, %f1314 }, { %r1368, %r1369, %r1370, %r1371 }, { %r1202, %r1203 }, { %f1311, %f1312, %f1313, %f1314 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1283, %f1284, %f1285, %f1286 }, { %r1416, %r1417, %r1418, %r1419 }, { %r1208, %r1209 }, { %f1283, %f1284, %f1285, %f1286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1287, %f1288, %f1289, %f1290 }, { %r1416, %r1417, %r1418, %r1419 }, { %r1214, %r1215 }, { %f1287, %f1288, %f1289, %f1290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1291, %f1292, %f1293, %f1294 }, { %r1416, %r1417, %r1418, %r1419 }, { %r1220, %r1221 }, { %f1291, %f1292, %f1293, %f1294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1295, %f1296, %f1297, %f1298 }, { %r1416, %r1417, %r1418, %r1419 }, { %r1226, %r1227 }, { %f1295, %f1296, %f1297, %f1298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1299, %f1300, %f1301, %f1302 }, { %r1416, %r1417, %r1418, %r1419 }, { %r1232, %r1233 }, { %f1299, %f1300, %f1301, %f1302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1303, %f1304, %f1305, %f1306 }, { %r1416, %r1417, %r1418, %r1419 }, { %r1238, %r1239 }, { %f1303, %f1304, %f1305, %f1306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1307, %f1308, %f1309, %f1310 }, { %r1416, %r1417, %r1418, %r1419 }, { %r1244, %r1245 }, { %f1307, %f1308, %f1309, %f1310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1311, %f1312, %f1313, %f1314 }, { %r1416, %r1417, %r1418, %r1419 }, { %r1250, %r1251 }, { %f1311, %f1312, %f1313, %f1314 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1283, %f1284, %f1285, %f1286 }, { %r1464, %r1465, %r1466, %r1467 }, { %r1256, %r1257 }, { %f1283, %f1284, %f1285, %f1286 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1287, %f1288, %f1289, %f1290 }, { %r1464, %r1465, %r1466, %r1467 }, { %r1262, %r1263 }, { %f1287, %f1288, %f1289, %f1290 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1291, %f1292, %f1293, %f1294 }, { %r1464, %r1465, %r1466, %r1467 }, { %r1268, %r1269 }, { %f1291, %f1292, %f1293, %f1294 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1295, %f1296, %f1297, %f1298 }, { %r1464, %r1465, %r1466, %r1467 }, { %r1274, %r1275 }, { %f1295, %f1296, %f1297, %f1298 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1299, %f1300, %f1301, %f1302 }, { %r1464, %r1465, %r1466, %r1467 }, { %r1280, %r1281 }, { %f1299, %f1300, %f1301, %f1302 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1303, %f1304, %f1305, %f1306 }, { %r1464, %r1465, %r1466, %r1467 }, { %r1286, %r1287 }, { %f1303, %f1304, %f1305, %f1306 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1307, %f1308, %f1309, %f1310 }, { %r1464, %r1465, %r1466, %r1467 }, { %r1292, %r1293 }, { %f1307, %f1308, %f1309, %f1310 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f1311, %f1312, %f1313, %f1314 }, { %r1464, %r1465, %r1466, %r1467 }, { %r1298, %r1299 }, { %f1311, %f1312, %f1313, %f1314 };
	// end inline asm
	.loc	1 113 21
	add.s32 	%r1772, %r1772, 1;
	.loc	1 127 22
	add.s64 	%rd134, %rd163, -48;
	add.s64 	%rd135, %rd163, -32;
	add.s64 	%rd136, %rd163, -16;
	add.s64 	%rd137, %rd14, %rd164;
	add.s64 	%rd103, %rd137, 262144;
	add.s64 	%rd104, %rd137, 327680;
	add.s64 	%rd105, %rd137, 393216;
	add.s64 	%rd106, %rd137, 458752;
	setp.lt.u64 	%p75, %rd134, %rd10;
	setp.lt.u64 	%p76, %rd135, %rd10;
	setp.lt.u64 	%p77, %rd136, %rd10;
	setp.lt.u64 	%p78, %rd163, %rd10;
	selp.b32 	%r1665, 16, 0, %p75;
	selp.b32 	%r1521, %r1665, 0, %p74;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r123 + 0 ], [ %rd103 + 0 ], 0x10, %r1521;
	// end inline asm
	selp.b32 	%r1666, 16, 0, %p76;
	selp.b32 	%r1523, %r1666, 0, %p74;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r125 + 0 ], [ %rd104 + 0 ], 0x10, %r1523;
	// end inline asm
	selp.b32 	%r1667, 16, 0, %p77;
	selp.b32 	%r1525, %r1667, 0, %p74;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r127 + 0 ], [ %rd105 + 0 ], 0x10, %r1525;
	// end inline asm
	selp.b32 	%r1668, 16, 0, %p78;
	selp.b32 	%r1527, %r1668, 0, %p74;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r129 + 0 ], [ %rd106 + 0 ], 0x10, %r1527;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	add.s64 	%rd107, %rd137, 262272;
	add.s64 	%rd108, %rd137, 327808;
	add.s64 	%rd109, %rd137, 393344;
	.loc	1 131 26
	add.s64 	%rd110, %rd137, 458880;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r131 + 0 ], [ %rd107 + 0 ], 0x10, %r1521;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r133 + 0 ], [ %rd108 + 0 ], 0x10, %r1523;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r135 + 0 ], [ %rd109 + 0 ], 0x10, %r1525;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r137 + 0 ], [ %rd110 + 0 ], 0x10, %r1527;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 142 22
	add.s64 	%rd138, %rd15, %rd164;
	add.s64 	%rd111, %rd138, 262144;
	add.s64 	%rd112, %rd138, 327680;
	add.s64 	%rd113, %rd138, 393216;
	add.s64 	%rd114, %rd138, 458752;
	and.pred  	%p99, %p36, %p75;
	and.pred  	%p98, %p36, %p76;
	and.pred  	%p97, %p36, %p77;
	and.pred  	%p96, %p36, %p78;
	selp.b32 	%r1669, 16, 0, %p99;
	selp.b32 	%r1529, %r1669, 0, %p74;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r139 + 0 ], [ %rd111 + 0 ], 0x10, %r1529;
	// end inline asm
	selp.b32 	%r1670, 16, 0, %p98;
	selp.b32 	%r1531, %r1670, 0, %p74;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r141 + 0 ], [ %rd112 + 0 ], 0x10, %r1531;
	// end inline asm
	selp.b32 	%r1671, 16, 0, %p97;
	selp.b32 	%r1533, %r1671, 0, %p74;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r143 + 0 ], [ %rd113 + 0 ], 0x10, %r1533;
	// end inline asm
	selp.b32 	%r1672, 16, 0, %p96;
	selp.b32 	%r1535, %r1672, 0, %p74;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r145 + 0 ], [ %rd114 + 0 ], 0x10, %r1535;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 183 22
	add.s64 	%rd139, %rd13, %rd164;
	add.s64 	%rd115, %rd139, 262144;
	add.s64 	%rd116, %rd139, 327680;
	add.s64 	%rd117, %rd139, 393216;
	add.s64 	%rd118, %rd139, 458752;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r147 + 0 ], [ %rd115 + 0 ], 0x10, %r1521;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r149 + 0 ], [ %rd116 + 0 ], 0x10, %r1523;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r151 + 0 ], [ %rd117 + 0 ], 0x10, %r1525;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r153 + 0 ], [ %rd118 + 0 ], 0x10, %r1527;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 187 26
	add.s64 	%rd119, %rd139, 262272;
	add.s64 	%rd120, %rd139, 327808;
	add.s64 	%rd121, %rd139, 393344;
	add.s64 	%rd122, %rd139, 458880;
	bar.sync 	0;
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r155 + 0 ], [ %rd119 + 0 ], 0x10, %r1521;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r157 + 0 ], [ %rd120 + 0 ], 0x10, %r1523;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r159 + 0 ], [ %rd121 + 0 ], 0x10, %r1525;
	// end inline asm
	// begin inline asm
	@%p35 cp.async.cg.shared.global [ %r161 + 0 ], [ %rd122 + 0 ], 0x10, %r1527;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 127 22
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 113 21
	add.s64 	%rd164, %rd164, 262144;
	add.s64 	%rd163, %rd163, 64;
	setp.lt.s32 	%p79, %r1772, %r2;
	@%p79 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_4;
$L__BB0_1:
	.loc	1 200 23
	shr.u32 	%r1782, %r3, 4;
	or.b32  	%r1781, %r26, 8;
	or.b32  	%r1780, %r26, 16;
	or.b32  	%r1779, %r26, 24;
	or.b32  	%r1778, %r26, 32;
	or.b32  	%r1777, %r26, 40;
	or.b32  	%r1776, %r26, 48;
	or.b32  	%r1775, %r26, 56;
	shr.u32 	%r1774, %r4, 4;
	shl.b32 	%r1773, %r25, 1;
	mov.f32 	%f1315, 0f00000000;
	mov.f32 	%f1316, %f1315;
	mov.f32 	%f1317, %f1315;
	mov.f32 	%f1318, %f1315;
	mov.f32 	%f1319, %f1315;
	mov.f32 	%f1320, %f1315;
	mov.f32 	%f1321, %f1315;
	mov.f32 	%f1322, %f1315;
	mov.f32 	%f1323, %f1315;
	mov.f32 	%f1324, %f1315;
	mov.f32 	%f1325, %f1315;
	mov.f32 	%f1326, %f1315;
	mov.f32 	%f1327, %f1315;
	mov.f32 	%f1328, %f1315;
	mov.f32 	%f1329, %f1315;
	mov.f32 	%f1330, %f1315;
	mov.f32 	%f1331, %f1315;
	mov.f32 	%f1332, %f1315;
	mov.f32 	%f1333, %f1315;
	mov.f32 	%f1334, %f1315;
	mov.f32 	%f1335, %f1315;
	mov.f32 	%f1336, %f1315;
	mov.f32 	%f1337, %f1315;
	mov.f32 	%f1338, %f1315;
	mov.f32 	%f1339, %f1315;
	mov.f32 	%f1340, %f1315;
	mov.f32 	%f1341, %f1315;
	mov.f32 	%f1342, %f1315;
	mov.f32 	%f1343, %f1315;
	mov.f32 	%f1344, %f1315;
	mov.f32 	%f1345, %f1315;
	mov.f32 	%f1346, %f1315;
	mov.f32 	%f1283, %f1315;
	mov.f32 	%f1284, %f1315;
	mov.f32 	%f1285, %f1315;
	mov.f32 	%f1286, %f1315;
	mov.f32 	%f1287, %f1315;
	mov.f32 	%f1288, %f1315;
	mov.f32 	%f1289, %f1315;
	mov.f32 	%f1290, %f1315;
	mov.f32 	%f1291, %f1315;
	mov.f32 	%f1292, %f1315;
	mov.f32 	%f1293, %f1315;
	mov.f32 	%f1294, %f1315;
	mov.f32 	%f1295, %f1315;
	mov.f32 	%f1296, %f1315;
	mov.f32 	%f1297, %f1315;
	mov.f32 	%f1298, %f1315;
	mov.f32 	%f1299, %f1315;
	mov.f32 	%f1300, %f1315;
	mov.f32 	%f1301, %f1315;
	mov.f32 	%f1302, %f1315;
	mov.f32 	%f1303, %f1315;
	mov.f32 	%f1304, %f1315;
	mov.f32 	%f1305, %f1315;
	mov.f32 	%f1306, %f1315;
	mov.f32 	%f1307, %f1315;
	mov.f32 	%f1308, %f1315;
	mov.f32 	%f1309, %f1315;
	mov.f32 	%f1310, %f1315;
	mov.f32 	%f1311, %f1315;
	mov.f32 	%f1312, %f1315;
	mov.f32 	%f1313, %f1315;
	mov.f32 	%f1314, %f1315;
$L__BB0_4:
	.loc	1 96 27
	shl.b32 	%r1737, %r120, 14;
	.loc	1 96 18
	mul.wide.s32 	%rd156, %r1737, 4;
	add.s64 	%rd157, %rd24, %rd156;
	.loc	1 113 21
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 200 23
	shl.b32 	%r1738, %r3, 2;
	and.b32  	%r1739, %r1738, 60;
	cvt.u64.u32 	%rd158, %r1739;
	shl.b32 	%r1740, %r1782, 7;
	and.b32  	%r1741, %r1740, 896;
	or.b64  	%rd159, %rd3, %rd158;
	mul.wide.u32 	%rd160, %r1741, 4;
	add.s64 	%rd161, %rd157, %rd160;
	shl.b64 	%rd162, %rd159, 2;
	add.s64 	%rd140, %rd161, %rd162;
	add.s64 	%rd141, %rd140, 4096;
	add.s64 	%rd142, %rd140, 8192;
	add.s64 	%rd143, %rd140, 12288;
	add.s64 	%rd144, %rd140, 16384;
	add.s64 	%rd145, %rd140, 20480;
	add.s64 	%rd146, %rd140, 24576;
	add.s64 	%rd147, %rd140, 28672;
	setp.lt.u64 	%p80, %rd159, 128;
	mul.lo.s32 	%r1742, %r27, 68;
	add.s32 	%r1743, %r1742, %r26;
	shl.b32 	%r1744, %r1743, 2;
	add.s32 	%r1746, %r193, %r1744;
	st.shared.v2.f32 	[%r1746], {%f1315, %f1316};
	st.shared.v2.f32 	[%r1746+2176], {%f1317, %f1318};
	add.s32 	%r1747, %r1742, %r1781;
	shl.b32 	%r1748, %r1747, 2;
	add.s32 	%r1749, %r193, %r1748;
	st.shared.v2.f32 	[%r1749], {%f1319, %f1320};
	st.shared.v2.f32 	[%r1749+2176], {%f1321, %f1322};
	add.s32 	%r1750, %r1742, %r1780;
	shl.b32 	%r1751, %r1750, 2;
	add.s32 	%r1752, %r193, %r1751;
	st.shared.v2.f32 	[%r1752], {%f1323, %f1324};
	st.shared.v2.f32 	[%r1752+2176], {%f1325, %f1326};
	add.s32 	%r1753, %r1742, %r1779;
	shl.b32 	%r1754, %r1753, 2;
	add.s32 	%r1755, %r193, %r1754;
	st.shared.v2.f32 	[%r1755], {%f1327, %f1328};
	st.shared.v2.f32 	[%r1755+2176], {%f1329, %f1330};
	add.s32 	%r1756, %r1742, %r1778;
	shl.b32 	%r1757, %r1756, 2;
	add.s32 	%r1758, %r193, %r1757;
	st.shared.v2.f32 	[%r1758], {%f1331, %f1332};
	st.shared.v2.f32 	[%r1758+2176], {%f1333, %f1334};
	add.s32 	%r1759, %r1742, %r1777;
	shl.b32 	%r1760, %r1759, 2;
	add.s32 	%r1761, %r193, %r1760;
	st.shared.v2.f32 	[%r1761], {%f1335, %f1336};
	st.shared.v2.f32 	[%r1761+2176], {%f1337, %f1338};
	add.s32 	%r1762, %r1742, %r1776;
	shl.b32 	%r1763, %r1762, 2;
	add.s32 	%r1764, %r193, %r1763;
	st.shared.v2.f32 	[%r1764], {%f1339, %f1340};
	st.shared.v2.f32 	[%r1764+2176], {%f1341, %f1342};
	add.s32 	%r1765, %r1742, %r1775;
	shl.b32 	%r1766, %r1765, 2;
	add.s32 	%r1767, %r193, %r1766;
	st.shared.v2.f32 	[%r1767], {%f1343, %f1344};
	st.shared.v2.f32 	[%r1767+2176], {%f1345, %f1346};
	bar.sync 	0;
	or.b32  	%r1768, %r1773, %r1774;
	mad.lo.s32 	%r1769, %r1768, 68, %r1739;
	shl.b32 	%r1770, %r1769, 2;
	add.s32 	%r1771, %r193, %r1770;
	ld.shared.v4.u32 	{%r1673, %r1674, %r1675, %r1676}, [%r1771];
	ld.shared.v4.u32 	{%r1677, %r1678, %r1679, %r1680}, [%r1771+2176];
	ld.shared.v4.u32 	{%r1681, %r1682, %r1683, %r1684}, [%r1771+4352];
	ld.shared.v4.u32 	{%r1685, %r1686, %r1687, %r1688}, [%r1771+6528];
	ld.shared.v4.u32 	{%r1689, %r1690, %r1691, %r1692}, [%r1771+8704];
	ld.shared.v4.u32 	{%r1693, %r1694, %r1695, %r1696}, [%r1771+10880];
	ld.shared.v4.u32 	{%r1697, %r1698, %r1699, %r1700}, [%r1771+13056];
	ld.shared.v4.u32 	{%r1701, %r1702, %r1703, %r1704}, [%r1771+15232];
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd140 + 0 ], { %r1673, %r1674, %r1675, %r1676 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd141 + 0 ], { %r1677, %r1678, %r1679, %r1680 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd142 + 0 ], { %r1681, %r1682, %r1683, %r1684 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd143 + 0 ], { %r1685, %r1686, %r1687, %r1688 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd144 + 0 ], { %r1689, %r1690, %r1691, %r1692 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd145 + 0 ], { %r1693, %r1694, %r1695, %r1696 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd146 + 0 ], { %r1697, %r1698, %r1699, %r1700 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd147 + 0 ], { %r1701, %r1702, %r1703, %r1704 };
	// end inline asm
	.loc	1 203 27
	add.s64 	%rd148, %rd140, 32768;
	add.s64 	%rd149, %rd140, 36864;
	add.s64 	%rd150, %rd140, 40960;
	add.s64 	%rd151, %rd140, 45056;
	add.s64 	%rd152, %rd140, 49152;
	add.s64 	%rd153, %rd140, 53248;
	add.s64 	%rd154, %rd140, 57344;
	add.s64 	%rd155, %rd140, 61440;
	bar.sync 	0;
	st.shared.v2.f32 	[%r1746], {%f1283, %f1284};
	st.shared.v2.f32 	[%r1746+2176], {%f1285, %f1286};
	st.shared.v2.f32 	[%r1749], {%f1287, %f1288};
	st.shared.v2.f32 	[%r1749+2176], {%f1289, %f1290};
	st.shared.v2.f32 	[%r1752], {%f1291, %f1292};
	st.shared.v2.f32 	[%r1752+2176], {%f1293, %f1294};
	st.shared.v2.f32 	[%r1755], {%f1295, %f1296};
	st.shared.v2.f32 	[%r1755+2176], {%f1297, %f1298};
	st.shared.v2.f32 	[%r1758], {%f1299, %f1300};
	st.shared.v2.f32 	[%r1758+2176], {%f1301, %f1302};
	st.shared.v2.f32 	[%r1761], {%f1303, %f1304};
	st.shared.v2.f32 	[%r1761+2176], {%f1305, %f1306};
	st.shared.v2.f32 	[%r1764], {%f1307, %f1308};
	st.shared.v2.f32 	[%r1764+2176], {%f1309, %f1310};
	st.shared.v2.f32 	[%r1767], {%f1311, %f1312};
	st.shared.v2.f32 	[%r1767+2176], {%f1313, %f1314};
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1705, %r1706, %r1707, %r1708}, [%r1771];
	ld.shared.v4.u32 	{%r1709, %r1710, %r1711, %r1712}, [%r1771+2176];
	ld.shared.v4.u32 	{%r1713, %r1714, %r1715, %r1716}, [%r1771+4352];
	ld.shared.v4.u32 	{%r1717, %r1718, %r1719, %r1720}, [%r1771+6528];
	ld.shared.v4.u32 	{%r1721, %r1722, %r1723, %r1724}, [%r1771+8704];
	ld.shared.v4.u32 	{%r1725, %r1726, %r1727, %r1728}, [%r1771+10880];
	ld.shared.v4.u32 	{%r1729, %r1730, %r1731, %r1732}, [%r1771+13056];
	ld.shared.v4.u32 	{%r1733, %r1734, %r1735, %r1736}, [%r1771+15232];
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd148 + 0 ], { %r1705, %r1706, %r1707, %r1708 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd149 + 0 ], { %r1709, %r1710, %r1711, %r1712 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd150 + 0 ], { %r1713, %r1714, %r1715, %r1716 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd151 + 0 ], { %r1717, %r1718, %r1719, %r1720 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd152 + 0 ], { %r1721, %r1722, %r1723, %r1724 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd153 + 0 ], { %r1725, %r1726, %r1727, %r1728 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd154 + 0 ], { %r1729, %r1730, %r1731, %r1732 };
	// end inline asm
	// begin inline asm
	@%p80 st.global.v4.b32 [ %rd155 + 0 ], { %r1733, %r1734, %r1735, %r1736 };
	// end inline asm
	.loc	1 198 4
	ret;
$L__tmp3:
$L__func_end0:

}
	.file	1 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\fla\\ops\\common\\chunk_delta_h.py"
	.file	2 "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\triton\\language\\standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 253
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 104
.b8 117
.b8 110
.b8 107
.b8 95
.b8 100
.b8 101
.b8 108
.b8 116
.b8 97
.b8 95
.b8 104
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 68
.b8 58
.b8 92
.b8 85
.b8 115
.b8 101
.b8 114
.b8 115
.b8 92
.b8 76
.b8 111
.b8 117
.b8 105
.b8 115
.b8 92
.b8 80
.b8 121
.b8 99
.b8 104
.b8 97
.b8 114
.b8 109
.b8 80
.b8 114
.b8 111
.b8 106
.b8 101
.b8 99
.b8 116
.b8 115
.b8 92
.b8 77
.b8 97
.b8 115
.b8 116
.b8 101
.b8 114
.b8 95
.b8 116
.b8 104
.b8 101
.b8 115
.b8 105
.b8 115
.b8 92
.b8 66
.b8 97
.b8 98
.b8 105
.b8 108
.b8 111
.b8 110
.b8 103
.b8 95
.b8 66
.b8 101
.b8 110
.b8 99
.b8 104
.b8 109
.b8 97
.b8 114
.b8 107
.b8 92
.b8 46
.b8 118
.b8 101
.b8 110
.b8 118
.b8 92
.b8 76
.b8 105
.b8 98
.b8 92
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 92
.b8 102
.b8 108
.b8 97
.b8 92
.b8 111
.b8 112
.b8 115
.b8 92
.b8 99
.b8 111
.b8 109
.b8 109
.b8 111
.b8 110
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 99
.b8 104
.b8 117
.b8 110
.b8 107
.b8 95
.b8 103
.b8 97
.b8 116
.b8 101
.b8 100
.b8 95
.b8 100
.b8 101
.b8 108
.b8 116
.b8 97
.b8 95
.b8 114
.b8 117
.b8 108
.b8 101
.b8 95
.b8 102
.b8 119
.b8 100
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 95
.b8 104
.b8 95
.b8 98
.b8 108
.b8 111
.b8 99
.b8 107
.b8 100
.b8 105
.b8 109
.b8 54
.b8 52
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 161
.b8 4
.b32 161
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 67
.b8 24
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
