{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-15T14:58:04.708879Z",
     "start_time": "2025-12-15T14:58:03.202743Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# 0. INSTALLATION & SETUP\n",
    "# ==============================================================================\n",
    "# Ensure core libraries for QLoRA and Mamba are installed\n",
    "%pip install torch transformers bitsandbytes peft trl datasets accelerate scipy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (5.0.0.dev0)\n",
      "Requirement already satisfied: bitsandbytes in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (0.48.2)\n",
      "Requirement already satisfied: peft in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: trl in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: datasets in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: scipy in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: filelock in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.0.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: typer-slim in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.0.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.0.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.0.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: anyio in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: psutil in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from typer-slim->transformers) (8.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T15:07:25.945699Z",
     "start_time": "2025-12-15T15:07:23.704921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORTS\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Hugging Face & Data ---\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# --- Mamba Specifics (Optional, but good for explicit typing) ---\n",
    "from transformers import MambaConfig, MambaForCausalLM\n",
    "\n",
    "# --- Efficiency Stack (QLoRA & Fine-Tuning) ---\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# --- Local Project Modules (Thesis Codebase) ---\n",
    "# Ensure your notebook is running from the project root to find 'source'\n",
    "from source.babilong.prompts import DEFAULT_PROMPTS, DEFAULT_TEMPLATE, get_formatted_input\n",
    "from source.babilong.babilong_utils import compare_answers\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. SYSTEM CHECKS\n",
    "# ==============================================================================\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current Device: {torch.cuda.get_device_name(0)}\")"
   ],
   "id": "3843962c81971efa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "Current Device: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup and Config",
   "id": "bd4351a493902b1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T21:51:13.458572Z",
     "start_time": "2025-12-14T21:50:51.482870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 3. MODEL CONFIGURATION & LOADING\n",
    "# ==============================================================================\n",
    "# Constants\n",
    "MODEL_ID = \"state-spaces/mamba-1.4b-hf\"\n",
    "OUTPUT_DIR = \"./babilong_mamba_finetune\"\n",
    "TASK_NAME = \"qa1\"    # Task: Single Supporting Fact\n",
    "SPLIT_LENGTH = \"0k\"  # Starting complexity (Short Context)\n",
    "\n",
    "# --- QLoRA Configuration ---\n",
    "# We use 4-bit Normal Float (NF4) quantization.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_map = {\"\": torch.cuda.current_device()}\n",
    "else:\n",
    "    device_map = \"auto\"\n",
    "\n",
    "print(f\"‚è≥ Loading Mamba Model ({MODEL_ID}) in 4-bit...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# --- Tokenizer Loading ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "print(f\"‚úÖ Model successfully loaded on device: {device_map}\")\n",
    "print(f\"‚úÖ Tokenizer loaded. Vocab size: {len(tokenizer)}\")"
   ],
   "id": "e43fb9ba9511672b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Mamba Model (state-spaces/mamba-1.4b-hf) in 4-bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and install the kernels library using `pip install kernels` or https://github.com/Dao-AILab/causal-conv1d for causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "900b9e5eaec34672bfc1713d72361704"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model successfully loaded on device: {'': 0}\n",
      "‚úÖ Tokenizer loaded. Vocab size: 50277\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T21:51:13.518447Z",
     "start_time": "2025-12-14T21:51:13.463079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 4. LoRA ADAPTER CONFIGURATION\n",
    "# ==============================================================================\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    \n",
    "    # Target Modules Justification:\n",
    "    # We specifically target 'x_proj' and 'dt_proj' (in addition to standard projections)\n",
    "    # because they govern the Mamba Selection Mechanism (SSM parameters B, C, Delta).\n",
    "    # Fine-tuning these layers allows the model to adapt its content filtering logic \n",
    "    # (\"what to remember\") for the specific needle-in-haystack task.\n",
    "    target_modules=[\"in_proj\", \"x_proj\", \"dt_proj\"]\n",
    ")\n",
    "\n",
    "# Prepare model for QLoRA training (casts non-trainable layers to efficient dtypes)\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "id": "a158b8104dfe7fe1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T21:51:13.880027Z",
     "start_time": "2025-12-14T21:51:13.518447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 2. TOKENIZER CONFIGURATION\n",
    "# ==============================================================================\n",
    "print(f\"Loading Tokenizer for {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Pad Token Fix:\n",
    "# Mamba (GPT-NeoX based) lacks a native pad token. We reuse EOS to allow batching.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Padding Side:\n",
    "# We explicitly set right-padding. While LLMs typically left-pad for generation,\n",
    "# the SFTTrainer expects right-padding during the training phase to handle \n",
    "# attention masks correctly.\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer ready. Pad Token ID: {tokenizer.pad_token_id}\")"
   ],
   "id": "a59ab2010b4b6e08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer for state-spaces/mamba-1.4b-hf...\n",
      "Tokenizer ready. Pad Token ID: 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Dataset",
   "id": "3b7649b802146d21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T21:51:24.966755Z",
     "start_time": "2025-12-14T21:51:13.881529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 5. DATASET PREPARATION\n",
    "# ==============================================================================\n",
    "import re\n",
    "\n",
    "print(f\"Loading BABILong {TASK_NAME} ({SPLIT_LENGTH})...\")\n",
    "dataset = load_dataset(\"RMT-team/babilong\", SPLIT_LENGTH, split=TASK_NAME)\n",
    "\n",
    "# --- Configuration ---\n",
    "TRIGGER_PHRASE = \"The most recent location of\"\n",
    "TRIGGER_SUFFIX = f\"\\nAnswer: {TRIGGER_PHRASE}\"\n",
    "\n",
    "# Prepare the prompt configuration\n",
    "prompt_cfg = {\n",
    "    'instruction': DEFAULT_PROMPTS[TASK_NAME]['instruction'],\n",
    "    'examples': DEFAULT_PROMPTS[TASK_NAME]['examples'],\n",
    "    'post_prompt': DEFAULT_PROMPTS[TASK_NAME]['post_prompt'],\n",
    "    \n",
    "    # CRITICAL: We append a specific trigger suffix to the template.\n",
    "    # Observation: Without this, the model frequently failed to answer. Instead of \n",
    "    # generating a location, it would simply continue the sequence pattern \n",
    "    # (e.g., generating a new <context> block or another question).\n",
    "    # Solution: This suffix acts as \"Teacher Forcing,\" breaking the pattern \n",
    "    # and constraining the model to immediately output the answer.\n",
    "    # Note: This hallucination/continuation issue was specific to the 0k (short) \n",
    "    # dataset. It was not observed in the 1k+ contexts, but we apply the fix \n",
    "    # uniformly for consistency.\n",
    "    'template': DEFAULT_TEMPLATE + TRIGGER_SUFFIX, \n",
    "}\n",
    "\n",
    "def format_aligned_training(example):\n",
    "    \"\"\"\n",
    "    Maps data to 'prompt' and 'completion' columns for the SFTTrainer.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Prompt: Ends with the trigger \"...Answer: The most recent location of\"\n",
    "    2. Completion: Reconstructs a full sentence \" Mary is bathroom.\"\n",
    "       This forces the model to identify the subject before predicting the location.\n",
    "    \"\"\"\n",
    "    # 1. Generate Prompt (Inputs + Instruction + Trigger)\n",
    "    prompt_str = get_formatted_input(\n",
    "        context=example['input'], \n",
    "        question=example['question'], \n",
    "        examples=prompt_cfg['examples'],\n",
    "        instruction=prompt_cfg['instruction'], \n",
    "        post_prompt=prompt_cfg['post_prompt'],\n",
    "        template=prompt_cfg['template']\n",
    "    )\n",
    "    \n",
    "    # 2. Generate Completion (Target with Grammar Fix)\n",
    "    # Extract subject (e.g., \"Mary\") from question to form: \" Mary is {target}\"\n",
    "    question_str = example['question']\n",
    "    target_loc = example['target']\n",
    "    \n",
    "    match = re.search(r\"Where is (.*?)\\?\", question_str)\n",
    "    if match:\n",
    "        person_name = match.group(1)\n",
    "        completion_str = f\" {person_name} is {target_loc}\" \n",
    "    else:\n",
    "        completion_str = f\" {target_loc}\" # Fallback\n",
    "\n",
    "    # Add EOS token to signal termination\n",
    "    completion_str = f\"{completion_str}{tokenizer.eos_token}\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt_str,\n",
    "        \"completion\": completion_str\n",
    "    }\n",
    "\n",
    "print(\"‚è≥ Formatting dataset...\")\n",
    "aligned_dataset = dataset.map(format_aligned_training, remove_columns=dataset.column_names)\n",
    "\n",
    "# --- 90/10 Split ---\n",
    "split_dataset = aligned_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"‚úÖ Data Ready: {len(train_dataset)} Training, {len(eval_dataset)} Validation samples.\")"
   ],
   "id": "8048b40bf7106148",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BABILong qa1 (0k)...\n",
      "‚è≥ Formatting dataset...\n",
      "‚úÖ Data Ready: 90 Training, 10 Validation samples.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T21:51:24.986211Z",
     "start_time": "2025-12-14T21:51:24.967754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# DATA INSPECTION CELL\n",
    "# ==============================================================================\n",
    "# Let's inspect one sample to verify the \"Prompt\" vs \"Completion\" split\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üëÄ DATASET SAMPLE INSPECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n--- [PROMPT (Last 500 chars)] ---\")\n",
    "# We only show the end to verify the Instruction and Trigger are present\n",
    "print(f\"...{sample['prompt'][-500:]}\")\n",
    "\n",
    "print(f\"\\n--- [COMPLETION (The Target)] ---\")\n",
    "print(f\"'{sample['completion']}'\") \n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ],
   "id": "486e6d9249021dda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üëÄ DATASET SAMPLE INSPECTION\n",
      "================================================================================\n",
      "\n",
      "--- [PROMPT (Last 500 chars)] ---\n",
      "... balcony.\n",
      "</example>\n",
      "\n",
      "<example>\n",
      "Alan moved to the garage. Charlie went to the beach. Alan went to the shop. Rouse travelled to balcony. Where is Alan?\n",
      "Answer: The most recent location of Alan is shop.\n",
      "</example>\n",
      "\n",
      "Always return your answer in the following format: The most recent location of ‚Äôperson‚Äô is ‚Äôlocation‚Äô. Do not write anything else after that.\n",
      "\n",
      "<context>\n",
      "John travelled to the office. Mary journeyed to the kitchen.\n",
      "</context>\n",
      "\n",
      "Question: Where is Mary? \n",
      "Answer: The most recent location of\n",
      "\n",
      "--- [COMPLETION (The Target)] ---\n",
      "' Mary is kitchen<|endoftext|>'\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Config and start",
   "id": "3d3775ddf30e65a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T05:54:35.154393Z",
     "start_time": "2025-12-14T21:51:24.987210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# 6. TRAINING CONFIGURATION & EXECUTION\n",
    "# ==============================================================================\n",
    "# We define the training strategy with a focus on memory efficiency (QLoRA) \n",
    "# and task adaptation (Loss Masking).\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=10,             \n",
    "    eval_strategy=\"epoch\",           \n",
    "    save_strategy=\"epoch\",           \n",
    "    load_best_model_at_end=True,     \n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # --- Efficiency Stack (CRITICAL for RTX 3080) ---\n",
    "    # 1. bf16: Native Ampere precision (faster/stable than fp16).\n",
    "    # 2. gradient_checkpointing: Trades small compute cost for massive VRAM savings.\n",
    "    # 3. paged_adamw_8bit: Offloads optimizer states to CPU if GPU fills up.\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # --- Loss Masking ---\n",
    "    # completion_only_loss=True: \n",
    "    # Forces the model to ignore the Prompt (Haystack) and only learn the Answer.\n",
    "    completion_only_loss=True,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    packing=False,\n",
    "    \n",
    "    # --- Hyperparameters ---\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    group_by_length=False,     # False prevents sorting issues with Mamba state\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "print(\"Initializing SFTTrainer ...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,      \n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "# Start the training loop\n",
    "print(\"Starting Training...\")\n",
    "trainer.train()\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. SAVE ARTIFACTS\n",
    "# ==============================================================================\n",
    "print(f\"\\nüíæ Saving LoRA Adapters to {OUTPUT_DIR}...\")\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"‚úÖ Training Complete. The adapter is ready for the Evaluation Notebook.\")"
   ],
   "id": "6b3093604c6d52af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SFTTrainer ...\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 8:00:57, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.242433</td>\n",
       "      <td>2.519003</td>\n",
       "      <td>22291.000000</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>2.337514</td>\n",
       "      <td>44582.000000</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.097709</td>\n",
       "      <td>2.263515</td>\n",
       "      <td>66873.000000</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.030557</td>\n",
       "      <td>2.224169</td>\n",
       "      <td>89164.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.031937</td>\n",
       "      <td>2.192666</td>\n",
       "      <td>111455.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.035105</td>\n",
       "      <td>2.176443</td>\n",
       "      <td>133746.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.039166</td>\n",
       "      <td>2.167946</td>\n",
       "      <td>156037.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.039091</td>\n",
       "      <td>2.164132</td>\n",
       "      <td>178328.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>2.160753</td>\n",
       "      <td>200619.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.039029</td>\n",
       "      <td>2.160094</td>\n",
       "      <td>222910.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving LoRA Adapters to ./babilong_mamba_finetune...\n",
      "‚úÖ Training Complete. The adapter is ready for the Evaluation Notebook.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T05:54:35.161624Z",
     "start_time": "2025-12-15T05:54:35.156388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "0,# Check if the model already has a `peft_config` attribute\n",
    "if hasattr(model, \"peft_config\"):\n",
    "    print(\"Warning: The model already has a `peft_config` attribute. Removing it to avoid multiple adapters.\")\n",
    "    del model.peft_config  # Remove the existing `peft_config`\n",
    "else:\n",
    "    print(\"No `peft_config` attribute found in the model. Safe to proceed.\")\n"
   ],
   "id": "d4aa5962593b8c9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The model already has a `peft_config` attribute. Removing it to avoid multiple adapters.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2aefb3d3be4cedd2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
