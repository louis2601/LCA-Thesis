{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7837a407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louis/miniconda3/envs/mamba2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lm_eval import simple_evaluate\n",
    "from lm_eval.utils import make_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a069cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state-spaces/mamba2-1.3b...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Define model and tokenizer IDs\n",
    "model_id = \"state-spaces/mamba2-1.3b\"\n",
    "tokenizer_id = \"EleutherAI/gpt-neox-20b\" # Standard tokenizer for Mamba models\n",
    "\n",
    "# 2. Load the Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# 3. Load the Model using the official mamba-ssm library\n",
    "print(f\"Loading {model_id}...\")\n",
    "model = MambaLMHeadModel.from_pretrained(\n",
    "    model_id, \n",
    "    device=\"cuda\", \n",
    "    dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe2165e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The capital of France is\n",
      "Output: The capital of France is a city of contrasts. It is a city of the past, a city of the present, and a city of the future\n"
     ]
    }
   ],
   "source": [
    "# Quick generation test\n",
    "text = \"The capital of France is\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "print(f\"Input: {text}\")\n",
    "out = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    max_length=30, \n",
    "    temperature=0.7, \n",
    "    top_p=0.9\n",
    ")\n",
    "print(f\"Output: {tokenizer.decode(out[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e9dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Œ Creating MambaHFLM wrapper...\n",
      "âœ… Wrapper created. Max length: 131072\n"
     ]
    }
   ],
   "source": [
    "# First, create the HFLM wrapper\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "class MambaHFLM(HFLM):\n",
    "    \"\"\"Custom wrapper for mamba-ssm models\"\"\"\n",
    "    \n",
    "    def _model_generate(self, context, max_length, stop, **generation_kwargs):\n",
    "        # Remove kwargs that mamba-ssm doesn't support\n",
    "        generation_kwargs.pop('stopping_criteria', None)\n",
    "        generation_kwargs.pop('pad_token_id', None)\n",
    "        generation_kwargs.pop('use_cache', None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            return self.model.generate(\n",
    "                input_ids=context,\n",
    "                max_length=max_length,\n",
    "                temperature=generation_kwargs.get('temperature', 1.0),\n",
    "                top_p=generation_kwargs.get('top_p', 1.0),\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "# Patch mamba-ssm model for HFLM compatibility\n",
    "model.device = next(model.parameters()).device\n",
    "\n",
    "class MambaConfig:\n",
    "    vocab_size = 50277\n",
    "    hidden_size = 2048\n",
    "    num_hidden_layers = 48\n",
    "    tie_embeddings = True\n",
    "\n",
    "model.config = MambaConfig()\n",
    "model.tie_weights = lambda: None\n",
    "\n",
    "# Create the wrapper\n",
    "print(\"ðŸ”Œ Creating MambaHFLM wrapper...\")\n",
    "lm_obj = MambaHFLM(\n",
    "    pretrained=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=1,\n",
    "    max_length=131072,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"âœ… Wrapper created. Max length: {lm_obj.max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7732d659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª TESTING niah_single_1 (pass-key retrieval)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "niah_single_1: Custom kwargs can be passed to `--metadata` in console (as json string) or to the TaskManager.\n",
      "For example --metadata='{\"max_seq_lengths\":[4096, 8192]}'. For details see task Readme.\n",
      "Generating synthetic samples: repeat | 4096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 153.34it/s]\n",
      "Overwriting default num_fewshot of niah_single_1 from None to 0\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 2030.11it/s]\n",
      "Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:48<00:00,  5.43s/it]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Tasks    |Version|Filter|n-shot|Metric|   |Value|   |Stderr|\n",
      "|-------------|------:|------|-----:|-----:|---|----:|---|------|\n",
      "|niah_single_1|      1|none  |     0|  4096|â†‘  |    1|Â±  |   N/A|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try niah_single_1 (pass-key retrieval) which might match the paper\n",
    "print(\"ðŸ§ª TESTING niah_single_1 (pass-key retrieval)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = simple_evaluate(\n",
    "    model=lm_obj,\n",
    "    tasks=[\"niah_single_1\"],\n",
    "    device=\"cuda\",\n",
    "    num_fewshot=0,\n",
    "    limit=20,\n",
    "    metadata={\"max_seq_lengths\": [4096], \"tokenizer\": tokenizer_id}\n",
    ")\n",
    "\n",
    "print(make_table(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f09b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "niah_single_2: Custom kwargs can be passed to `--metadata` in console (as json string) or to the TaskManager.\n",
      "For example --metadata='{\"max_seq_lengths\":[4096, 8192]}'. For details see task Readme.\n",
      "Generating synthetic samples: essay | 1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 913.94it/s]\n",
      "Generating synthetic samples: essay | 2048: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:02<00:00, 229.54it/s]\n",
      "Generating synthetic samples: essay | 4096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 147.73it/s]\n",
      "Generating synthetic samples: essay | 8192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:06<00:00, 78.96it/s]\n",
      "Overwriting default num_fewshot of niah_single_2 from None to 0\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 2339.70it/s]\n",
      "Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [2:50:24<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Tasks    |Version|Filter|n-shot|Metric|   |Value|   |Stderr|\n",
      "|-------------|------:|------|-----:|-----:|---|----:|---|------|\n",
      "|niah_single_2|      1|none  |     0|  1024|   |0.990|Â±  |0.0045|\n",
      "|             |       |none  |     0|  2048|   |0.768|Â±  |0.0189|\n",
      "|             |       |none  |     0|  4096|â†‘  |0.000|Â±  |   N/A|\n",
      "|             |       |none  |     0|  8192|â†‘  |0.000|Â±  |   N/A|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TASK = \"niah_single_2\"\n",
    "LENGTHS = [1024, 2048, 4096, 8192]\n",
    "\n",
    "results = simple_evaluate(\n",
    "    model=lm_obj,\n",
    "    tasks=[TASK],\n",
    "    device=\"cuda\",\n",
    "    num_fewshot=0,\n",
    "    \n",
    "    metadata={\"max_seq_lengths\": LENGTHS, \"tokenizer\": tokenizer_id}\n",
    ")\n",
    "print(make_table(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36bb5a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "niah_single_1: Custom kwargs can be passed to `--metadata` in console (as json string) or to the TaskManager.\n",
      "For example --metadata='{\"max_seq_lengths\":[4096, 8192]}'. For details see task Readme.\n",
      "Generating synthetic samples: repeat | 1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 775.15it/s]\n",
      "Generating synthetic samples: repeat | 2048: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 338.11it/s]\n",
      "Generating synthetic samples: repeat | 4096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 164.75it/s]\n",
      "Generating synthetic samples: repeat | 8192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:05<00:00, 86.59it/s]\n",
      "Overwriting default num_fewshot of niah_single_1 from None to 0\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 2322.76it/s]\n",
      "Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [2:48:42<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Tasks    |Version|Filter|n-shot|Metric|   |Value|   |Stderr|\n",
      "|-------------|------:|------|-----:|-----:|---|----:|---|------|\n",
      "|niah_single_1|      1|none  |     0|  1024|   |    1|Â±  |     0|\n",
      "|             |       |none  |     0|  2048|   |    1|Â±  |     0|\n",
      "|             |       |none  |     0|  4096|â†‘  |    1|Â±  |   N/A|\n",
      "|             |       |none  |     0|  8192|â†‘  |    1|Â±  |   N/A|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TASK = \"niah_single_1\"\n",
    "LENGTHS = [1024, 2048, 4096, 8192]\n",
    "\n",
    "results = simple_evaluate(\n",
    "    model=lm_obj,\n",
    "    tasks=[TASK],\n",
    "    device=\"cuda\",\n",
    "    num_fewshot=0,\n",
    "    metadata={\"max_seq_lengths\": LENGTHS, \"tokenizer\": tokenizer_id}\n",
    ")\n",
    "print(make_table(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f207bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "niah_single_3: Custom kwargs can be passed to `--metadata` in console (as json string) or to the TaskManager.\n",
      "For example --metadata='{\"max_seq_lengths\":[4096, 8192]}'. For details see task Readme.\n",
      "Generating synthetic samples: essay | 1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 801.77it/s]\n",
      "Generating synthetic samples: essay | 2048: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 433.00it/s]\n",
      "Generating synthetic samples: essay | 4096: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:03<00:00, 147.80it/s]\n",
      "Generating synthetic samples: essay | 8192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:06<00:00, 73.65it/s]\n",
      "Overwriting default num_fewshot of niah_single_3 from None to 0\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1983.26it/s]\n",
      "Running generate_until requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [2:48:21<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Tasks    |Version|Filter|n-shot|Metric|   |Value|   |Stderr|\n",
      "|-------------|------:|------|-----:|-----:|---|----:|---|------|\n",
      "|niah_single_3|      1|none  |     0|  1024|   |0.982|Â±  |0.0060|\n",
      "|             |       |none  |     0|  2048|   |0.810|Â±  |0.0176|\n",
      "|             |       |none  |     0|  4096|â†‘  |0.002|Â±  |   N/A|\n",
      "|             |       |none  |     0|  8192|â†‘  |0.000|Â±  |   N/A|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TASK = \"niah_single_3\"\n",
    "LENGTHS = [1024, 2048, 4096, 8192]\n",
    "\n",
    "results = simple_evaluate(\n",
    "    model=lm_obj,\n",
    "    tasks=[TASK],\n",
    "    device=\"cuda\",\n",
    "    num_fewshot=0,\n",
    "    metadata={\"max_seq_lengths\": LENGTHS, \"tokenizer\": tokenizer_id}\n",
    ")\n",
    "print(make_table(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mamba2)",
   "language": "python",
   "name": "mamba2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
