{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7837a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import simple_evaluate\n",
    "from lm_eval.utils import make_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a069cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louis/miniconda3/envs/mamba2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state-spaces/mamba2-1.3b...\n",
      "\n",
      "Output: The theory of state space models is a powerful tool for the analysis of complex systems. The theory is based on the assumption that the system under study is a linear time-invariant system with a state space representation. The state space representation is a set of\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Define model and tokenizer IDs\n",
    "model_id = \"state-spaces/mamba2-1.3b\"\n",
    "tokenizer_id = \"EleutherAI/gpt-neox-20b\" # Standard tokenizer for Mamba models\n",
    "\n",
    "# 2. Load the Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "# Mamba does not use a BOS token, but EOS is usually needed\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. Load the Model using the official mamba-ssm library\n",
    "# Note: 'device=\"cuda\"' is usually required as Mamba kernels are GPU-only\n",
    "print(f\"Loading {model_id}...\")\n",
    "model = MambaLMHeadModel.from_pretrained(\n",
    "    model_id, \n",
    "    device=\"cuda\", \n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 4. Run Inference\n",
    "text = \"The theory of state space models is\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# Generate text (MambaLMHeadModel has a simple generate method)\n",
    "out = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    max_length=50, \n",
    "    temperature=0.9, \n",
    "    top_p=0.95, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "decoded = tokenizer.decode(out[0])\n",
    "print(\"\\nOutput:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T19:16:55.868745Z",
     "start_time": "2026-01-09T19:16:53.493845Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state-spaces/mamba2-1.3b...\n",
      "\n",
      "Output: The theory of state space models is a powerful tool for the analysis of complex systems. The theory is based on the assumption that the system under study is a linear time-invariant system with a state space representation. The state space representation is a set of state variables and a set of state\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Define model and tokenizer IDs\n",
    "model_id = \"state-spaces/mamba2-1.3b\"\n",
    "tokenizer_id = \"EleutherAI/gpt-neox-20b\" # Standard tokenizer for Mamba models\n",
    "\n",
    "# 2. Load the Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "# Mamba does not use a BOS token, but EOS is usually needed\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. Load the Model using the official mamba-ssm library\n",
    "# Note: 'device=\"cuda\"' is usually required as Mamba kernels are GPU-only\n",
    "print(f\"Loading {model_id}...\")\n",
    "model = MambaLMHeadModel.from_pretrained(\n",
    "    model_id, \n",
    "    device=\"cuda\", \n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 4. Run Inference\n",
    "text = \"The theory of state space models is\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# Generate text (MambaLMHeadModel has a simple generate method)\n",
    "out = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    max_length=input_ids.shape[1] + 50,  \n",
    "    temperature=1.0,      \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "decoded = tokenizer.decode(out[0])\n",
    "print(\"\\nOutput:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe2165e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The capital of France is\n",
      "Output: The capital of France is a city of contrasts. It is a city of the past, a city of the present, and a city of the future\n"
     ]
    }
   ],
   "source": [
    "# Quick generation test\n",
    "text = \"The capital of France is\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "print(f\"Input: {text}\")\n",
    "out = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    max_length=30, \n",
    "    temperature=0.7, \n",
    "    top_p=0.9\n",
    ")\n",
    "print(f\"Output: {tokenizer.decode(out[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "335bd8d2a3b6b001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Input length: 618 tokens\n",
      "üéØ Response:  7392. The weather is nice today. The\n"
     ]
    }
   ],
   "source": [
    "# Simple NIAH test\n",
    "needle = \"The secret code is 7392.\"\n",
    "haystack = \"The weather is nice today. \" * 50\n",
    "prompt = f\"{haystack} {needle} {haystack} What is the secret code? The code is\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "print(f\"üìè Input length: {input_ids.shape[1]} tokens\")\n",
    "\n",
    "out = model.generate(input_ids=input_ids, max_length=input_ids.shape[1]+10, temperature=0.1)\n",
    "response = tokenizer.decode(out[0][input_ids.shape[1]:])\n",
    "print(f\"üéØ Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "385b11a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Wrapping Mamba-2 with custom HF-compatible wrapper...\n"
     ]
    }
   ],
   "source": [
    "# Patch mamba-ssm model for HFLM compatibility\n",
    "model.device = next(model.parameters()).device\n",
    "\n",
    "class MambaConfig:\n",
    "    vocab_size = 50277\n",
    "    hidden_size = 2048\n",
    "    num_hidden_layers = 48\n",
    "    tie_embeddings = True\n",
    "\n",
    "model.config = MambaConfig()\n",
    "model.tie_weights = lambda: None\n",
    "\n",
    "# Now wrap with the custom class\n",
    "print(\"üîå Wrapping Mamba-2 with custom HF-compatible wrapper...\")\n",
    "lm_obj = MambaHFLM(\n",
    "    pretrained=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=1,\n",
    "    max_length=131072,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b567be8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Wrapping Mamba-2 with custom HF-compatible wrapper...\n",
      "‚úÖ Harness max_length: 131072\n"
     ]
    }
   ],
   "source": [
    "# Custom wrapper to make mamba-ssm compatible with lm-eval\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "class MambaHFLM(HFLM):\n",
    "    \"\"\"Custom wrapper for mamba-ssm models\"\"\"\n",
    "    \n",
    "    def _model_generate(self, context, max_length, stop, **generation_kwargs):\n",
    "        # Remove kwargs that mamba-ssm doesn't support\n",
    "        generation_kwargs.pop('stopping_criteria', None)\n",
    "        generation_kwargs.pop('pad_token_id', None)\n",
    "        generation_kwargs.pop('use_cache', None)\n",
    "        \n",
    "        # Mamba uses max_length differently - it's total length, not new tokens\n",
    "        with torch.no_grad():\n",
    "            return self.model.generate(\n",
    "                input_ids=context,\n",
    "                max_length=max_length,\n",
    "                temperature=generation_kwargs.get('temperature', 1.0),\n",
    "                top_p=generation_kwargs.get('top_p', 1.0),\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "# Now wrap with the custom class\n",
    "print(\"üîå Wrapping Mamba-2 with custom HF-compatible wrapper...\")\n",
    "lm_obj = MambaHFLM(\n",
    "    pretrained=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=1,\n",
    "    max_length=131072,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"‚úÖ Harness max_length: {lm_obj.max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f09b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "niah_single_2: Custom kwargs can be passed to `--metadata` in console (as json string) or to the TaskManager.\n",
      "For example --metadata='{\"max_seq_lengths\":[4096, 8192]}'. For details see task Readme.\n",
      "Generating synthetic samples: essay | 1024: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 913.94it/s]\n",
      "Generating synthetic samples: essay | 2048: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 229.54it/s]\n",
      "Generating synthetic samples: essay | 4096: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:03<00:00, 147.73it/s]\n",
      "Generating synthetic samples: essay | 8192: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:06<00:00, 78.96it/s]\n",
      "Overwriting default num_fewshot of niah_single_2 from None to 0\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 2339.70it/s]\n",
      "Running generate_until requests:   8%|‚ñà‚ñà‚ñà‚ñå                                         | 161/2000 [14:30<3:28:41,  6.81s/it]"
     ]
    }
   ],
   "source": [
    "TASK = \"niah_single_2\"\n",
    "LENGTHS = [1024, 2048, 4096, 8192]\n",
    "\n",
    "results = simple_evaluate(\n",
    "    model=lm_obj,\n",
    "    tasks=[TASK],\n",
    "    device=\"cuda\",\n",
    "    num_fewshot=0,\n",
    "    metadata={\"max_seq_lengths\": LENGTHS, \"tokenizer\": tokenizer_id}\n",
    ")\n",
    "print(make_table(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"niah_single_1\"\n",
    "LENGTHS = [1024, 2048, 4096, 8192]\n",
    "\n",
    "results = simple_evaluate(\n",
    "    model=lm_obj,\n",
    "    tasks=[TASK],\n",
    "    device=\"cuda\",\n",
    "    num_fewshot=0,\n",
    "    metadata={\"max_seq_lengths\": LENGTHS, \"tokenizer\": tokenizer_id}\n",
    ")\n",
    "print(make_table(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f207bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wonderwords\n",
      "  Downloading wonderwords-3.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: nltk in /home/louis/miniconda3/envs/mamba2/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /home/louis/miniconda3/envs/mamba2/lib/python3.11/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/louis/miniconda3/envs/mamba2/lib/python3.11/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/louis/miniconda3/envs/mamba2/lib/python3.11/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/louis/miniconda3/envs/mamba2/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Downloading wonderwords-3.0.1-py3-none-any.whl (51 kB)\n",
      "Installing collected packages: wonderwords\n",
      "Successfully installed wonderwords-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/louis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TASK = \"niah_single_3\"\n",
    "LENGTHS = [1024, 2048, 4096, 8192]\n",
    "\n",
    "results = simple_evaluate(\n",
    "    model=lm_obj,\n",
    "    tasks=[TASK],\n",
    "    device=\"cuda\",\n",
    "    num_fewshot=0,\n",
    "    metadata={\"max_seq_lengths\": LENGTHS, \"tokenizer\": tokenizer_id}\n",
    ")\n",
    "print(make_table(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mamba2)",
   "language": "python",
   "name": "mamba2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
